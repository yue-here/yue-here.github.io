[{"categories":null,"content":"\rFor associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign. In this article, we build a text-to-image AI that learns Chinese characters in the same way humans do - by understanding what their components mean. It can then invent new characters based on the meaning of an English prompt. ","date":"2024-05-26","objectID":"/glyffuser/:0:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Intro Chinese characters are pretty cool, and there are a lot of them; around 21,000 are represented in unicode alone. Interestingly, they encode meaning in multiple ways. Some are simple pictures of things - 山 (shān) means, as you might be able to guess, ‘mountain’. Most characters are compounds, however: 好 (hǎo), meaning ‘good’, is constructed from the pictograms for 女 (nǚ), ‘woman’ and 子 (zǐ), child. Read into that what you will. Compounds often contain a semantic and a phonetic component; these subcomponents are known as radicals. For example, the names of most metallic chemical elements such as lithium, 锂 (lǐ) are composed of a semantic radical (⻐, the condensed, simplified form of 金 (jīn) meaning ‘gold/metal’) and a phonetic component 里 (lǐ, of unrelated meaning) which approximates the ’li’ sound of lithium. I’ve been interested for a while in how Chinese characters might be explored using machine learning. Can we teach a model to understand how they are structured from pictures and definitions alone? Can it then invent new characters? (Spoiler: yes) Let’s talk about a couple of ways of engaging with this, starting with relatively simple ML models and moving towards state-of-the-art technologies. ","date":"2024-05-26","objectID":"/glyffuser/:1:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Dataset First, we need a dataset consisting of images of as many known Chinese characters as possible. The unicode standard is a convenient method of indexing a large proportion of known Chinese characters. Fonts provide a specific way of representing characters, in the form of distinct glyphs. Fonts have different levels of completeness, so choosing one that contained as many characters as possible for a large and diverse dataset was important. The main resources used for generating the dataset: The unicode CJK Unified Ideographs block (4E00–9FFF) of 20,992 Chinese characters Google fonts (Noto Sans and others) English definitions from the unihan database (kDefinition.txt) - this becomes useful later Let’s make the dataset by saving the glyphs as pictures in the square 2n×2n format favoured by ML convention. We want to keep detail while minimizing size - I found that 64×64 and 128×128 worked well. Since we don’t need colour, we can get away with a single channel - otherwise we’d use a 3 channel RGB image of size 3×128×128. ","date":"2024-05-26","objectID":"/glyffuser/:2:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Visualise the character set in 2D (Jupyter notebook) First let’s map the space with a dimensional reduction. UMAP works well here. Distinct clusters emerge - the largest cluster on the left represents the “lefty-righty” characters (to use the technical term), while the second largest cluster on the right represents “uppy-downy” characters. Tighter clusters/subclusters tend to correspond to characters that share the same radical - the small streak at the very top right is an almost literal leper colony of characters with the 疒 radical, indicating sickness. (Hover over/tap points to see the characters) ","date":"2024-05-26","objectID":"/glyffuser/:3:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Variational autoencoder (Jupyter notebook) Let’s try using an older generative ML technology, the variational autoencoder (VAE), to model the underlying distribution of Chinese characters. The dimensionality of the original input boils down to a vector of length 64×64=4096, i.e. one value for each pixel. The VAE architecture works by using convolutional layers to reduce the dimensionality of the input over several steps (the ’encoder’), then using a reverse convolution to increase the dimensionality back to the original (the ‘decoder’). The network is trained on its ability to output a match to the original. The key intuition is that not all of the pixels in the images contain valuable information, so they can somehow be compressed to a smaller (lower-dimensional) vector. This is known as the ’latent space’. By passing our training images through a bottleneck the size of the latent space and then training the model to reconstruct the full original image, we create a ‘decoder’ that can translate any vector in the latent space to a full sized image. In this case, we go from 64×64=4096 to a latent space of size 256. By passing images from our original dataset though the VAE again, we can see that the VAE has learnt a good latent representation of our glyphs: Aside: Training the VAE has an extra subtlety compared to a vanilla autoencoder which is only graded on reconstruction. Our VAE loss function consists of not just reconstruction loss, but also the Kullback–Leibler divergence, which is a measure of distance between two probability distributions. In this case, we want to minimize the distance between the learned distribution of Chinese glyphs and the normal distribution N(0,I). This pushes the latent distribution from ‘whatever it wants to be’ towards a normal distribution, which makes sampling and traversing the latent space easier. We can compare the latent space learnt by the VAE by passing the dataset through the VAE encoder only, generating a dataset of vectors in the latent space. We can then apply the same UMAP visualisation as before. While the lefty-righty and uppy-downy clusters from before still exist, you can see that the points have all been pushed closer to a (2D) normal distribution. Intuitively, adjacent points in this distribution are more likely to be similar - it’s ‘smoother’. We can interact with the latent space via the encoder and decoder components of our trained VAE. For example, we can perform bilinear interpolation between 4 characters. We obtain the latent vector for each one by using the VAE encoder as before, then interpolate the intermediate vectors and decode them to images with VAE decoder. This allows us to do some interesting things like morph smoothly between characters by interpolating between vectors in latent space: Your browser does not support the video tag.\rFor some types of data such as faces or organic molecules, VAEs can be used to generate convincing new samples simply by moving around in the latent space. However, this is a dead end for us - the intermediate steps are clearly not valid Chinese characters, and there is no clear way of accessing the underlying structure of the distribution do so. This gives us a not-so-elegant segue to the generative modality du jour: diffusion models. ","date":"2024-05-26","objectID":"/glyffuser/:4:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Unconditional diffusion model Initially models designed to remove noise from images, it was found that when applied to pure noise, denoising diffusion models could generate entirely new images of types they were trained on. Even more excitingly, when trained with text conditioning, they could then generate images related to a provided text prompt. This led the proliferation of text-to-image models starting around late 2022 like Dall-E, Midjourney, Imagen, and Stable Diffusion. These models tend to be trained on a huge number of text-image pairs harvested en masse from the internet. The training cost of these models is generally on the order of 6 figures (USD) or more. I was curious if the same type of architecture could be used in a simplified form, with a smaller dataset, to achieve an interesting result like generating convincing Chinese characters. Of course, I didn’t have 6 figures (USD) to spend on this - everything was done on my home rig with a used 3090 bought for ~$700 (the best VRAM Gb/$ value in 2023/4!). The first step was to implement an unconditional model trained on our existing dataset, to see if convincing generations could be achieved at all. Unconditional simply means that the model is trained to generate similar things to the data it sees, without any other guidance. Those playing the home game can follow along with the associated notebook here Most image diffusion models use some variant of the U-net architecture, initially developed for segmenting biomedical images. This network turns out to be a good choice for diffusion models, where we need to train a model that can predict noise that can then be subtracted to generate recognizable images. We also need a noise scheduler that controls the mathematics of how noise is added during the training process. To save time in implementing the whole model from scratch, we can use Huggingface’s handy diffusers library with the UNet2DModel for the Unet and the DDPMScheduler as the noise scheduler. This makes the core of the code relatively straightforward: from diffusers import UNet2DModel, DDPMScheduler # Define the UNet model model = UNet2DModel( sample_size=128, # the target image resolution in_channels=1, # the number of input channels out_channels=1, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(128, 128, 256, 256, 512, 512), # the number of output channels for each UNet block # Define the UNet architecture down_block_types=( \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\", ), up_block_types=( \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", ), ) # Initialize the noise scheduler noise_scheduler = DDPMScheduler(num_train_timesteps=1000) As the process is relatively expensive, the large models mentioned above generally train on some kind of lower dimensional space, coupled with a method to later obtain high resolution images. For example, Google’s Imagen uses a sequence of conditioned superresolution networks, while Stable Diffusion is built on the idea of latent diffusion, where the final images are decoded from a lower-dimensional latent space using a VAE. This step is omitted here as we have enough compute to train the model in the relatively small native size (128×128) of the data. The remainder of the code is mostly plumbing to get the data to the model. I did run into a couple of traps here though: The scheduler for running inference on a diffusion model need not be the same as the one used for training, and many good schedulers have been developed by the community that can give excellent results in 20-50 steps rather than the 100s that might be needed with the standard DDPMScheduler(). For our purposes, “DPM++ 2M” or DPMSolverMultistepScheduler() in the Diffusers library worked very well. Dataloader shuffling is essential with such small datasets. I ran into a bug that took weeks to diagnose: sampling every ","date":"2024-05-26","objectID":"/glyffuser/:5:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Conditional diffusion model Now that we have confirmed a diffusion model can generate convincing Chinese characters, let’s train a text-to-image model conditioned on the English definitions of each character. If the model correctly learns how the English definition relates to the Chinese character, it should be able to ‘understand’ the rules for how characters are constructed and generate characters that ‘give the right vibe’ for a given english prompt. Note\rThe concept of ‘conditioning’ may seem mysterious but here it boils down to working out a way to represent the English text as a vector, then adding that vector to another vector representing the image during the training process.\rIn a previous blog post, I discussed finetuning Stable Diffusion. However that seemed like the wrong approach here - the pretraining of SD wouldn’t do much for us since the types of images we want to generate are unlikely to be well represented in their training set. So which framework to use? In a misguided attempt to save effort, I first tried Assembly AI’s minimagen implementation, as an ostensibly simple conditional diffusion framework. It rapidly (but not rapidly enough) became clear that even after extensive debugging this was not fully functional demo code. I moved on to lucidrains’ much cleaner imagen implementation and trained some variants in unconditional mode, matching architectures as best as I could with my previous model (within the constraints of the framework), but I couldn’t never replicate the same quality - I suspect this was due to some differences in the layer architecture I couldn’t identify. “You could not live with your own failure, where did that bring you? Back to me.” In the end I decided I had to implement the model myself, something I’d been studiously avoiding. While huggingface’s UNet2DConditionModel() provides a framework for this, I was unable to find good documentation and so ended up having to scour the codebase directly. My observations below on what is needed, in case you dear reader want to take up this foolhardy task. Follow along with the notebook here. The UNet2DConditionModel() is able to be conditioned on either discrete classes (dog, car, house, etc.), or embeddings (vectors from a latent space). For text-to-image, we are using embeddings. I decided to take a page out of Google’s book and use LLM embeddings directly for conditioning, as Imagen does. What Google found with Imagen was that conditioning on text embeddings from an LLM such as their T5 model worked just as well or even better than the CLIP model trained on image-caption pairs that Dall-E uses. This is very handy for us, since pretrained CLIP-type embeddings would likely be inappropriate for our use case reasons discussed above. In practice, we can call down existing T5 methods from Huggingface’s transformers library and pass it some text; it will give us back a fixed-length vector embedding, imbued with LLM magic. Here’s the core of the conditional UNet model: from diffusers import UNet2DConditionModel # Define the conditional UNet model model = UNet2DConditionModel( sample_size=128, in_channels=1, out_channels=1, layers_per_block=2, block_out_channels=(128, 256, 512, 512), addition_embed_type=\"text\", # Make it conditional cross_attention_dim=512, encoder_hid_dim=512, # the hidden dimension of the encoder encoder_hid_dim_type=\"text_proj\", down_block_types=( \"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\" ), up_block_types=( \"UpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\" ), ) We specify the nature of the conditioning vector being passed to the model via some extra arguments. We also need to introduce some extra plumbing. First, we now need text captions for each training image, which we obtained from the unihan definitions database (way back at the top of this article). We then need a collator function that helps the dataloader return LLM embeddings from the text caption, as well as an attention mask for the ","date":"2024-05-26","objectID":"/glyffuser/:6:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Learning the structure of Chinese characters Now that we’ve trained the conditional model, how can we know what the model has learnt about how Chinese characters are constructed? One way is to probe it with different prompts and look at how the sampling steps progress. In the intro I mentioned that most characters contain semantic or phonetic radicals, and the UMAP shows the distribution of major character types. If we obtain a list of most common Chinese radicals from, say, an 18th century Chinese dictionary we can probe the model with prompts corresponding to the English meaning of each radical and see what is activated. Below we take the first sampling step (NB: in the animation in the previous section, we show training steps but each frame is sampled for 50 steps. Don’t get these confused!) for English text prompts corresponding to the most frequently appearing 36 Chinese radicals. (The radicals are not used in the prompt but shown in parentheses for reference): Very interesting! We see in most cases the corresponding radical is directly activated in the part of the character it usually resides, even though the rest of the character is very noisy. We can infer that the english prompt is strongly tied semantically, i.e. in meaning, to characters that contain this radical. The 虫 radical means “bug” and is quite strongly activated by the prompt ‘insect’ as it is used in bug-related characters (fly: 蝇, butterfly: 蝴蝶, earthworm: 蚯蚓). However, the 忄(heart) radical is not activated by “heart” (its literal meaning) as its semantic meaning is linked to the historical belief that thoughts and emotions arise from the physical heart (memory: 忆, afraid: 怕, fast: 快). Can we activate this radical by changing our prompt? The picture shows sequential sampling (i.e. denoising) steps on the same prompt. While at step 1 we see the model faking toward 忄, it rapidly switches to having a 心 at the bottom. If you can read Chinese, you’ll already know why this is: 忄 is actually an alternative form of 心, which is standalone character for heart and can also be used as a bottom radical. Characters with this radical are also linked to thoughts and feelings, e.g. 念 (to miss) and 想 (to think). Notably, we can’t access the phonetic component in this model as we haven’t included any information on Chinese pronunciation in the training process. Can we, however, generate new characters using semantic components? Let’s try: We can! We have both the components for ‘sickness’ and ’thoughts/feelings’ in this character. The overall structure of this character likely draws from characters such as 痣 (zhì, a mole/mark on the skin) in the training set, but in 痣 the 志, also pronounced zhì, is a phonetic component and has no semantic significance. Of course, this is a cherry-picked example. What happens if we try to activate two radicals that prefer to live in the same place? If we provoke a conflict by prompting “a sickness of fire”, for example, we get this: You can see that the model initially tries to place 火 on the left side of the square. By step 3, we can see the sickness radical 疒 also trying to form. As the sampling converges, however, neither concept is strong enough to dominate so the model resorts to enforcing some local structure out of the noise, with the overall character looking not at all convincing. Now that we understand better how the model works, let’s take another look at previous grid and take it all the way up to 50 sampling steps. You may wonder why we never generate the base character, always the radical. The nature of our training set means that there are many more semantically-related training samples containing the radical compared to the one original character, so it makes sense that model would prefer to place the radical in the appropriate position. Sample weighting by corpus frequency would be one way of getting around this! We also see the the rest of the character is filled with what appears to be random stuff. I suspect this is again","date":"2024-05-26","objectID":"/glyffuser/:7:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Classifier-free guidance (CFG) for the glyffuser Classifier-free guidance is an elegant and powerful technique that is now used in basically all production conditional diffusion models to improve adherence to prompts. (For an excellent treatment, see here) Essentially, this method allows the strength of any given prompt to be varied without needing to perform any additional training. Moreover, the strength of the prompt can be increased far above that for standard conditional training. Can we use this method get better generations from the glyffuser? To implement this method, we simply add random dropout of the text conditioning tokens during training (10-20% has been found to work well). This effectively trains an unconditional model at the same time. During sampling steps, we then perform the noise prediction twice, once normally and once with a blank conditioning tensor. We then combine them as follows: noise_prediction = noise_prediction_unconditional + guidance_scale * (noise_prediction - noise_prediction_unconditional) Note\rAt guidance_scale = 0, the model acts as an unconditional model while at guidance_scale = 1, the model acts as the standard conditional model\rGenerally, increasing guidance_scale in text-to-image models decreases variety while increasing adherence to the prompt. Let’s try probing the model by varying the number of sampling steps and guidance scale for the prompt “bird” corresponding to a very common radical (鳥/鸟): Note\rUnusually, the “bird” radical can occur on either the left (“鸵”, ostrich) or right (“鸡”, chicken) sides of characters.\rI’ve also include interactive explorers for the prompts ‘fire’ and ‘hair’ in the appendices. I highly recommend taking a look, they are very funny! Compared to the base conditional model, we see that as we increase the guidance scale, the ‘bird’ radical becomes increasingly activated from the very first sampling step. Interestingly, while the traditional form of the bird character “鳥” dominates (it is more prevalent in the training set), the simplified form “鸟” also makes a single appearance (10 steps, scale=50), making it a ’transition state’ during the denoising process. The explorer below shows CFG scales of 0 to 100 for different random seeds - higher CFG scales do indeed reduce sample variety. Compared to general-purpose text-to-image models however, we can tolerate higher CFG scales as they tend to give more convincing characters. If you follow any individual character, you’ll see that it tends to start with one ‘bird’ radical, then as CFG scale increases, at some point the other side will also collapse to a ‘bird’ radical. Pause\rCFG scale 1\rLooking at the a picture of varying CFG scale generations for prompts for each common radical below, we see that for scale values of around 5-15, the generated characters are much more consistently convincing than the those made by the base conditional or unconditional models. (I think that anyone who grew up with Chinese or has studied it for a long time gains a strong instinctive sense of whether a character could be real, even if they don’t recognise it.) ","date":"2024-05-26","objectID":"/glyffuser/:8:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Outro So can we teach a model to understand how Chinese characters and then invent new ones? It looks the answer is a resounding yes! We find that the final conditioned diffusion model (the Glyffuser™) has a strong conception of how the components of a Chinese character relate to its meaning, in much the same way a human would guess at the meaning of an unknown character. Moreover, by strengthening the adherence to known concepts using classifier-free guidance, we can generate very convincing characters that follow the rules of Chinese glyph construction. ","date":"2024-05-26","objectID":"/glyffuser/:9:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Test the models here If you want to try out the conditional glyffuser, the easiest way is to get the demo repo with git clone https://github.com/yue-here/glyffuser, make a python environment with requirements.txt (you may need to set it up for your GPU and jupyter) then run the glyffuser inference.ipynb notebook, which will summon the trained model from my huggingface repo. Failing that, I’ve made this applet that will run the inference, but please be patient as it’s quite slow (around a minute per step). ","date":"2024-05-26","objectID":"/glyffuser/:10:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Appendices ","date":"2024-05-26","objectID":"/glyffuser/:11:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Appendix 1: other writing styles Here’s a training video from a version of the glyffuser trained on the ancient Chinese writing known as seal script: Your browser does not support the video tag.\r","date":"2024-05-26","objectID":"/glyffuser/:11:1","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Appendix 2: phonetic radicals To include the phonetic radicals, we need a representation. The standard romanization is known as pinyin - for example, the phrase 人工智能 (artificial intelligence) would be “rén gōng zhì néng”. The diacritics on the vowels are the tones of which there are 4 in standard mandarin. These can also be represented numerically, e.g. “ren2 gong1 zhi4 neng2”. Simply adding the pinyin to the english definition prior to training is unlikely to work as we are using a frozen tokenizer and text model which does not recognize the pinyin syllables. Instead, we can create separate embeddings of the same length as the image vector for the pinyin and add them directly, the same as the text conditioning is added. Specifically, based on how Chinese syllables are structured, I added 3 trainable embeddings for the initial, final, and tone (for example, ren2 would become ‘r’, ’en’ and ‘2’). To train the model, I added extra inputs for the pinyin and tone. After training the model in the same way as before, I tried to ‘summon’ the phonetic components. However, the results only changed slightly with different pinyin prompts. I suspect this is because of the excessive homophony in Chinese. For example, 羊 (yang2, ‘goat/sheep’) is a common phonetic radical. But for this exact syllable, wiktionary gives 47 different characters. Have a look at characters containing the top two phonetic radicals for this pronunciation, 羊 and 昜: 羊佯垟徉样洋烊珜眻蛘𨦡羏劷 昜崵揚暘楊湯敭瑒煬禓瘍諹輰鍚鐊陽霷颺鰑鸉 It’s likely the same situation as the ‘clashing radicals’ case where we can’t activate the ‘fire’ and ‘sickness’ radicals at same time - when neither phonetic radical dominates the distribution, we end up sampling garbage. ","date":"2024-05-26","objectID":"/glyffuser/:11:2","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Appendix 3: CFG variations for “fire” The Chinese character for fire “火” has a particularly varied set of possible locations. These are showcased in the characters “炎” and “焱”. Another form is the bottom radical “灬”, a kind of deconstructed version of “火”. As such, greater variety is possible and this shows: Pause\rCFG scale 1\r","date":"2024-05-26","objectID":"/glyffuser/:11:3","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"Appendix 4: CFG variations for “hair” I’m mostly including this because the characters look very funny. Pause\rCFG scale 1\r","date":"2024-05-26","objectID":"/glyffuser/:11:4","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyffuser/"},{"categories":null,"content":"\rFor associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign. In this article, we build a text-to-image AI that learns Chinese characters in the same way humans do - by understanding what their components mean. It can then invent new characters based on the meaning of an English prompt. ","date":"2024-05-26","objectID":"/glyphexplorer/:0:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Intro Chinese characters are pretty cool, and there are a lot of them; around 21,000 are represented in unicode alone. Interestingly, they encode meaning in multiple ways. Some are simple pictures of things - 山 (shān) means, as you might be able to guess, ‘mountain’. Most characters are compounds, however: 好 (hǎo), meaning ‘good’, is constructed from the pictograms for 女 (nǚ), ‘woman’ and 子 (zǐ), child. Read into that what you will. Compounds often contain a semantic and a phonetic component; these subcomponents are known as radicals. For example, the names of most metallic chemical elements such as lithium, 锂 (lǐ) are composed of a semantic radical (⻐, the condensed, simplified form of 金 (jīn) meaning ‘gold/metal’) and a phonetic component 里 (lǐ, of unrelated meaning) which approximates the ’li’ sound of lithium. I’ve been interested for a while in how Chinese characters might be explored using machine learning. Can we teach a model to understand how they are structured from pictures and definitions alone? Can it then invent new characters? (Spoiler: yes) Let’s talk about a couple of ways of engaging with this, starting with relatively simple ML models and moving towards state-of-the-art technologies. ","date":"2024-05-26","objectID":"/glyphexplorer/:1:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Dataset First, we need a dataset consisting of images of as many known Chinese characters as possible. The unicode standard is a convenient method of indexing a large proportion of known Chinese characters. Fonts provide a specific way of representing characters, in the form of distinct glyphs. Fonts have different levels of completeness, so choosing one that contained as many characters as possible for a large and diverse dataset was important. The main resources used for generating the dataset: The unicode CJK Unified Ideographs block (4E00–9FFF) of 20,992 Chinese characters Google fonts (Noto Sans and others) English definitions from the unihan database (kDefinition.txt) - this becomes useful later Let’s make the dataset by saving the glyphs as pictures in the square 2n×2n format favoured by ML convention. We want to keep detail while minimizing size - I found that 64×64 and 128×128 worked well. Since we don’t need colour, we can get away with a single channel - otherwise we’d use a 3 channel RGB image of size 3×128×128. ","date":"2024-05-26","objectID":"/glyphexplorer/:2:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Visualise the character set in 2D (Jupyter notebook) First let’s map the space with a dimensional reduction. UMAP works well here. Distinct clusters emerge - the largest cluster on the left represents the “lefty-righty” characters (to use the technical term), while the second largest cluster on the right represents “uppy-downy” characters. Tighter clusters/subclusters tend to correspond to characters that share the same radical - the small streak at the very top right is an almost literal leper colony of characters with the 疒 radical, indicating sickness. (Hover over/tap points to see the characters) ","date":"2024-05-26","objectID":"/glyphexplorer/:3:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Variational autoencoder (Jupyter notebook) Let’s try using an older generative ML technology, the variational autoencoder (VAE), to model the underlying distribution of Chinese characters. The dimensionality of the original input boils down to a vector of length 64×64=4096, i.e. one value for each pixel. The VAE architecture works by using convolutional layers to reduce the dimensionality of the input over several steps (the ’encoder’), then using a reverse convolution to increase the dimensionality back to the original (the ‘decoder’). The network is trained on its ability to output a match to the original. The key intuition is that not all of the pixels in the images contain valuable information, so they can somehow be compressed to a smaller (lower-dimensional) vector. This is known as the ’latent space’. By passing our training images through a bottleneck the size of the latent space and then training the model to reconstruct the full original image, we create a ‘decoder’ that can translate any vector in the latent space to a full sized image. In this case, we go from 64×64=4096 to a latent space of size 256. By passing images from our original dataset though the VAE again, we can see that the VAE has learnt a good latent representation of our glyphs: Aside: Training the VAE has an extra subtlety compared to a vanilla autoencoder which is only graded on reconstruction. Our VAE loss function consists of not just reconstruction loss, but also the Kullback–Leibler divergence, which is a measure of distance between two probability distributions. In this case, we want to minimize the distance between the learned distribution of Chinese glyphs and the normal distribution N(0,I). This pushes the latent distribution from ‘whatever it wants to be’ towards a normal distribution, which makes sampling and traversing the latent space easier. We can compare the latent space learnt by the VAE by passing the dataset through the VAE encoder only, generating a dataset of vectors in the latent space. We can then apply the same UMAP visualisation as before. While the lefty-righty and uppy-downy clusters from before still exist, you can see that the points have all been pushed closer to a (2D) normal distribution. Intuitively, adjacent points in this distribution are more likely to be similar - it’s ‘smoother’. We can interact with the latent space via the encoder and decoder components of our trained VAE. For example, we can perform bilinear interpolation between 4 characters. We obtain the latent vector for each one by using the VAE encoder as before, then interpolate the intermediate vectors and decode them to images with VAE decoder. This allows us to do some interesting things like morph smoothly between characters by interpolating between vectors in latent space: Your browser does not support the video tag.\rFor some types of data such as faces or organic molecules, VAEs can be used to generate convincing new samples simply by moving around in the latent space. However, this is a dead end for us - the intermediate steps are clearly not valid Chinese characters, and there is no clear way of accessing the underlying structure of the distribution do so. This gives us a not-so-elegant segue to the generative modality du jour: diffusion models. ","date":"2024-05-26","objectID":"/glyphexplorer/:4:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Unconditional diffusion model Initially models designed to remove noise from images, it was found that when applied to pure noise, denoising diffusion models could generate entirely new images of types they were trained on. Even more excitingly, when trained with text conditioning, they could then generate images related to a provided text prompt. This led the proliferation of text-to-image models starting around late 2022 like Dall-E, Midjourney, Imagen, and Stable Diffusion. These models tend to be trained on a huge number of text-image pairs harvested en masse from the internet. The training cost of these models is generally on the order of 6 figures (USD) or more. I was curious if the same type of architecture could be used in a simplified form, with a smaller dataset, to achieve an interesting result like generating convincing Chinese characters. Of course, I didn’t have 6 figures (USD) to spend on this - everything was done on my home rig with a used 3090 bought for ~$700 (the best VRAM Gb/$ value in 2023/4!). The first step was to implement an unconditional model trained on our existing dataset, to see if convincing generations could be achieved at all. Unconditional simply means that the model is trained to generate similar things to the data it sees, without any other guidance. Those playing the home game can follow along with the associated notebook here Most image diffusion models use some variant of the U-net architecture, initially developed for segmenting biomedical images. This network turns out to be a good choice for diffusion models, where we need to train a model that can predict noise that can then be subtracted to generate recognizable images. We also need a noise scheduler that controls the mathematics of how noise is added during the training process. To save time in implementing the whole model from scratch, we can use Huggingface’s handy diffusers library with the UNet2DModel for the Unet and the DDPMScheduler as the noise scheduler. This makes the core of the code relatively straightforward: from diffusers import UNet2DModel, DDPMScheduler # Define the UNet model model = UNet2DModel( sample_size=128, # the target image resolution in_channels=1, # the number of input channels out_channels=1, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(128, 128, 256, 256, 512, 512), # the number of output channels for each UNet block # Define the UNet architecture down_block_types=( \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\", ), up_block_types=( \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", ), ) # Initialize the noise scheduler noise_scheduler = DDPMScheduler(num_train_timesteps=1000) As the process is relatively expensive, the large models mentioned above generally train on some kind of lower dimensional space, coupled with a method to later obtain high resolution images. For example, Google’s Imagen uses a sequence of conditioned superresolution networks, while Stable Diffusion is built on the idea of latent diffusion, where the final images are decoded from a lower-dimensional latent space using a VAE. This step is omitted here as we have enough compute to train the model in the relatively small native size (128×128) of the data. The remainder of the code is mostly plumbing to get the data to the model. I did run into a couple of traps here though: The scheduler for running inference on a diffusion model need not be the same as the one used for training, and many good schedulers have been developed by the community that can give excellent results in 20-50 steps rather than the 100s that might be needed with the standard DDPMScheduler(). For our purposes, “DPM++ 2M” or DPMSolverMultistepScheduler() in the Diffusers library worked very well. Dataloader shuffling is essential with such small datasets. I ran into a bug that took weeks to diagnose: sampling every ","date":"2024-05-26","objectID":"/glyphexplorer/:5:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Conditional diffusion model Now that we have confirmed a diffusion model can generate convincing Chinese characters, let’s train a text-to-image model conditioned on the English definitions of each character. If the model correctly learns how the English definition relates to the Chinese character, it should be able to ‘understand’ the rules for how characters are constructed and generate characters that ‘give the right vibe’ for a given english prompt. Note\rThe concept of ‘conditioning’ may seem mysterious but here it boils down to working out a way to represent the English text as a vector, then adding that vector to another vector representing the image during the training process.\rIn a previous blog post, I discussed finetuning Stable Diffusion. However that seemed like the wrong approach here - the pretraining of SD wouldn’t do much for us since the types of images we want to generate are unlikely to be well represented in their training set. So which framework to use? In a misguided attempt to save effort, I first tried Assembly AI’s minimagen implementation, as an ostensibly simple conditional diffusion framework. It rapidly (but not rapidly enough) became clear that even after extensive debugging this was not fully functional demo code. I moved on to lucidrains’ much cleaner imagen implementation and trained some variants in unconditional mode, matching architectures as best as I could with my previous model (within the constraints of the framework), but I couldn’t never replicate the same quality - I suspect this was due to some differences in the layer architecture I couldn’t identify. “You could not live with your own failure, where did that bring you? Back to me.” In the end I decided I had to implement the model myself, something I’d been studiously avoiding. While huggingface’s UNet2DConditionModel() provides a framework for this, I was unable to find good documentation and so ended up having to scour the codebase directly. My observations below on what is needed, in case you dear reader want to take up this foolhardy task. Follow along with the notebook here. The UNet2DConditionModel() is able to be conditioned on either discrete classes (dog, car, house, etc.), or embeddings (vectors from a latent space). For text-to-image, we are using embeddings. I decided to take a page out of Google’s book and use LLM embeddings directly for conditioning, as Imagen does. What Google found with Imagen was that conditioning on text embeddings from an LLM such as their T5 model worked just as well or even better than the CLIP model trained on image-caption pairs that Dall-E uses. This is very handy for us, since pretrained CLIP-type embeddings would likely be inappropriate for our use case reasons discussed above. In practice, we can call down existing T5 methods from Huggingface’s transformers library and pass it some text; it will give us back a fixed-length vector embedding, imbued with LLM magic. Here’s the core of the conditional UNet model: from diffusers import UNet2DConditionModel # Define the conditional UNet model model = UNet2DConditionModel( sample_size=128, in_channels=1, out_channels=1, layers_per_block=2, block_out_channels=(128, 256, 512, 512), addition_embed_type=\"text\", # Make it conditional cross_attention_dim=512, encoder_hid_dim=512, # the hidden dimension of the encoder encoder_hid_dim_type=\"text_proj\", down_block_types=( \"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\" ), up_block_types=( \"UpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\" ), ) We specify the nature of the conditioning vector being passed to the model via some extra arguments. We also need to introduce some extra plumbing. First, we now need text captions for each training image, which we obtained from the unihan definitions database (way back at the top of this article). We then need a collator function that helps the dataloader return LLM embeddings from the text caption, as well as an attention mask for the ","date":"2024-05-26","objectID":"/glyphexplorer/:6:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Learning the structure of Chinese characters Now that we’ve trained the conditional model, how can we know what the model has learnt about how Chinese characters are constructed? One way is to probe it with different prompts and look at how the sampling steps progress. In the intro I mentioned that most characters contain semantic or phonetic radicals, and the UMAP shows the distribution of major character types. If we obtain a list of most common Chinese radicals from, say, an 18th century Chinese dictionary we can probe the model with prompts corresponding to the English meaning of each radical and see what is activated. Below we take the first sampling step (NB: in the animation in the previous section, we show training steps but each frame is sampled for 50 steps. Don’t get these confused!) for English text prompts corresponding to the most frequently appearing 36 Chinese radicals. (The radicals are not used in the prompt but shown in parentheses for reference): Very interesting! We see in most cases the corresponding radical is directly activated in the part of the character it usually resides, even though the rest of the character is very noisy. We can infer that the english prompt is strongly tied semantically, i.e. in meaning, to characters that contain this radical. The 虫 radical means “bug” and is quite strongly activated by the prompt ‘insect’ as it is used in bug-related characters (fly: 蝇, butterfly: 蝴蝶, earthworm: 蚯蚓). However, the 忄(heart) radical is not activated by “heart” (its literal meaning) as its semantic meaning is linked to the historical belief that thoughts and emotions arise from the physical heart (memory: 忆, afraid: 怕, fast: 快). Can we activate this radical by changing our prompt? The picture shows sequential sampling (i.e. denoising) steps on the same prompt. While at step 1 we see the model faking toward 忄, it rapidly switches to having a 心 at the bottom. If you can read Chinese, you’ll already know why this is: 忄 is actually an alternative form of 心, which is standalone character for heart and can also be used as a bottom radical. Characters with this radical are also linked to thoughts and feelings, e.g. 念 (to miss) and 想 (to think). Notably, we can’t access the phonetic component in this model as we haven’t included any information on Chinese pronunciation in the training process. Can we, however, generate new characters using semantic components? Let’s try: We can! We have both the components for ‘sickness’ and ’thoughts/feelings’ in this character. The overall structure of this character likely draws from characters such as 痣 (zhì, a mole/mark on the skin) in the training set, but in 痣 the 志, also pronounced zhì, is a phonetic component and has no semantic significance. Of course, this is a cherry-picked example. What happens if we try to activate two radicals that prefer to live in the same place? If we provoke a conflict by prompting “a sickness of fire”, for example, we get this: You can see that the model initially tries to place 火 on the left side of the square. By step 3, we can see the sickness radical 疒 also trying to form. As the sampling converges, however, neither concept is strong enough to dominate so the model resorts to enforcing some local structure out of the noise, with the overall character looking not at all convincing. Now that we understand better how the model works, let’s take another look at previous grid and take it all the way up to 50 sampling steps. You may wonder why we never generate the base character, always the radical. The nature of our training set means that there are many more semantically-related training samples containing the radical compared to the one original character, so it makes sense that model would prefer to place the radical in the appropriate position. Sample weighting by corpus frequency would be one way of getting around this! We also see the the rest of the character is filled with what appears to be random stuff. I suspect this is again","date":"2024-05-26","objectID":"/glyphexplorer/:7:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Classifier-free guidance (CFG) for the glyffuser Classifier-free guidance is an elegant and powerful technique that is now used in basically all production conditional diffusion models to improve adherence to prompts. (For an excellent treatment, see here) Essentially, this method allows the strength of any given prompt to be varied without needing to perform any additional training. Moreover, the strength of the prompt can be increased far above that for standard conditional training. Can we use this method get better generations from the glyffuser? To implement this method, we simply add random dropout of the text conditioning tokens during training (10-20% has been found to work well). This effectively trains an unconditional model at the same time. During sampling steps, we then perform the noise prediction twice, once normally and once with a blank conditioning tensor. We then combine them as follows: noise_prediction = noise_prediction_unconditional + guidance_scale * (noise_prediction - noise_prediction_unconditional) Note\rAt guidance_scale = 0, the model acts as an unconditional model while at guidance_scale = 1, the model acts as the standard conditional model\rGenerally, increasing guidance_scale in text-to-image models decreases variety while increasing adherence to the prompt. Let’s try probing the model by varying the number of sampling steps and guidance scale for the prompt “bird” corresponding to a very common radical (鳥/鸟): Note\rUnusually, the “bird” radical can occur on either the left (“鸵”, ostrich) or right (“鸡”, chicken) sides of characters.\rI’ve also include interactive explorers for the prompts ‘fire’ and ‘hair’ in the appendices. I highly recommend taking a look, they are very funny! Compared to the base conditional model, we see that as we increase the guidance scale, the ‘bird’ radical becomes increasingly activated from the very first sampling step. Interestingly, while the traditional form of the bird character “鳥” dominates (it is more prevalent in the training set), the simplified form “鸟” also makes a single appearance (10 steps, scale=50), making it a ’transition state’ during the denoising process. The explorer below shows CFG scales of 0 to 100 for different random seeds - higher CFG scales do indeed reduce sample variety. Compared to general-purpose text-to-image models however, we can tolerate higher CFG scales as they tend to give more convincing characters. If you follow any individual character, you’ll see that it tends to start with one ‘bird’ radical, then as CFG scale increases, at some point the other side will also collapse to a ‘bird’ radical. Pause\rCFG scale 1\rLooking at the a picture of varying CFG scale generations for prompts for each common radical below, we see that for scale values of around 5-15, the generated characters are much more consistently convincing than the those made by the base conditional or unconditional models. (I think that anyone who grew up with Chinese or has studied it for a long time gains a strong instinctive sense of whether a character could be real, even if they don’t recognise it.) ","date":"2024-05-26","objectID":"/glyphexplorer/:8:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Outro So can we teach a model to understand how Chinese characters and then invent new ones? It looks the answer is a resounding yes! We find that the final conditioned diffusion model (the Glyffuser™) has a strong conception of how the components of a Chinese character relate to its meaning, in much the same way a human would guess at the meaning of an unknown character. Moreover, by strengthening the adherence to known concepts using classifier-free guidance, we can generate very convincing characters that follow the rules of Chinese glyph construction. ","date":"2024-05-26","objectID":"/glyphexplorer/:9:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Test the models here If you want to try out the conditional glyffuser, the easiest way is to get the demo repo with git clone https://github.com/yue-here/glyffuser, make a python environment with requirements.txt (you may need to set it up for your GPU and jupyter) then run the glyffuser inference.ipynb notebook, which will summon the trained model from my huggingface repo. Failing that, I’ve made this applet that will run the inference, but please be patient as it’s quite slow (around a minute per step). ","date":"2024-05-26","objectID":"/glyphexplorer/:10:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Appendices ","date":"2024-05-26","objectID":"/glyphexplorer/:11:0","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Appendix 1: other writing styles Here’s a training video from a version of the glyffuser trained on the ancient Chinese writing known as seal script: Your browser does not support the video tag.\r","date":"2024-05-26","objectID":"/glyphexplorer/:11:1","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Appendix 2: phonetic radicals To include the phonetic radicals, we need a representation. The standard romanization is known as pinyin - for example, the phrase 人工智能 (artificial intelligence) would be “rén gōng zhì néng”. The diacritics on the vowels are the tones of which there are 4 in standard mandarin. These can also be represented numerically, e.g. “ren2 gong1 zhi4 neng2”. Simply adding the pinyin to the english definition prior to training is unlikely to work as we are using a frozen tokenizer and text model which does not recognize the pinyin syllables. Instead, we can create separate embeddings of the same length as the image vector for the pinyin and add them directly, the same as the text conditioning is added. Specifically, based on how Chinese syllables are structured, I added 3 trainable embeddings for the initial, final, and tone (for example, ren2 would become ‘r’, ’en’ and ‘2’). To train the model, I added extra inputs for the pinyin and tone. After training the model in the same way as before, I tried to ‘summon’ the phonetic components. However, the results only changed slightly with different pinyin prompts. I suspect this is because of the excessive homophony in Chinese. For example, 羊 (yang2, ‘goat/sheep’) is a common phonetic radical. But for this exact syllable, wiktionary gives 47 different characters. Have a look at characters containing the top two phonetic radicals for this pronunciation, 羊 and 昜: 羊佯垟徉样洋烊珜眻蛘𨦡羏劷 昜崵揚暘楊湯敭瑒煬禓瘍諹輰鍚鐊陽霷颺鰑鸉 It’s likely the same situation as the ‘clashing radicals’ case where we can’t activate the ‘fire’ and ‘sickness’ radicals at same time - when neither phonetic radical dominates the distribution, we end up sampling garbage. ","date":"2024-05-26","objectID":"/glyphexplorer/:11:2","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Appendix 3: CFG variations for “fire” The Chinese character for fire “火” has a particularly varied set of possible locations. These are showcased in the characters “炎” and “焱”. Another form is the bottom radical “灬”, a kind of deconstructed version of “火”. As such, greater variety is possible and this shows: Pause\rCFG scale 1\r","date":"2024-05-26","objectID":"/glyphexplorer/:11:3","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"Appendix 4: CFG variations for “hair” I’m mostly including this because the characters look very funny. Pause\rCFG scale 1\r","date":"2024-05-26","objectID":"/glyphexplorer/:11:4","tags":null,"title":"Teaching an AI to invent new Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"For associated code, please see the Jupyter notebook in the github repository While machine learning has been very successful in image-processing applications, there’s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I’ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn’t rely on specific feature tracking. A short clip of unstabilized footage ","date":"2024-02-14","objectID":"/opticalflow/:0:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Background I had a vague conception that tracking a similarity metric between two frames would furnish the shift between then, perhaps using least-squares peak fitting in a similar way to the fitting of X-ray diffraction data (once you have a hammer, everything looks like a nail!) by performing gradient descent with small shifts to the two frames. To my great relief I didn’t have to write this code as further investigation revealed OpenCV contains a feature called optical flow which essentially performs this calculation, outputting vectors for the movement of every pixel between two frames. This could form the basis of stabilisation code. ","date":"2024-02-14","objectID":"/opticalflow/:1:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Execution: optical flow interpretation My exploratory calculations showed that the optical flow vectors did indeed provide a clear picture of shift direction and magnitude. Optical flow analysis of a frame transition with a small shift downward You can see in the maps above the algorithm detects pixels that can be mapped to a clear shift (furniture etc.) while uniform areas like wallpaper aren’t picked up. The regions of uniform grey in the “flow magnitude” plot indicate a consistent shift, and the sharp peak in the histogram of shifts confirms this - the camera view has tilted by around 16 pixels. Likewise for the “flow angle plot”, the peak in the histogram at ~90° indicates a shift downwards. The next step was to work out how to use these results. It was clear that this was already a relatively expensive calculation and it would have to be performed on ~18,000 pairs of frames for my test timelapse dataset. (Aside: luckily, OpenCV has easily accessible GPU acceleration with OpenCL. Simply replacing standard matrices with UMat objects speeds up the calculations by an order of magnitude. I looked into using the CUDA implementation which would likely be even faster, but it seemed more trouble than it was worth to install.) While peak fitting the histograms would yield the best result in principle, I opted for an economical approach to save compute: filter out magnitudes that were the first entry in the array (i.e. the top of the decay curve, not a peak), only include angles within 5 of 90/270 degrees (i.e. vertical shifts), then take the max values and round the angles to 90 or 270. This part was done as a single script outputting a CSV file of the 4 relevant parameters extracted from histograms for each frame - vector magnitude, vector angle, and the frequencies of each . (In principle the full optical flow vector set could be saved but file would be huge. The 17K frame dataset took around 90 min on my system which was an acceptable tradeoff for repeats.) ","date":"2024-02-14","objectID":"/opticalflow/:2:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Execution: stabilisation This part was simple in principle but somewhat difficult in practice. The idea was very simple - by cropping the top and bottom of each frame by the maximum total shift across the dataset, the relative shift of each frame could be controlled by varying the shifts above and below. The execution: Calculate the relative shift from the starting position at for each frame of the dataset by summing the up and down shifts. The overall window size across the set can be extrapolated by the distance between the maximum and minimum shift. Then for the starting frames, crop down to the window size. Afterwards, perform the first crop plus the relative shift calculated previously to shift the visible window to a consistent location. It’s difficult to explain clearly but the code and demonstration make it clearer. The unstabilized footage from above\rThe optical flow stabilized footage\r","date":"2024-02-14","objectID":"/opticalflow/:3:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Generative text-to-image models have recent become very popular. Having a bunch of fully captioned images left over from the This JACS Does Not Exist project, I’ve trained a Stable Diffusion checkpoint on that dataset (~60K JACS table-of-contents images with matched paper titles). It seems to work pretty well. Here are some examples of prompts and the resulting images generated: “Development of a Highly Efficient and Selective Catalytic Enantioselective Hydrogenation for Organic Synthesis” “Lead-free Cs2AgBiBr6 Perovskite Solar Cells with High Efficiency and Stability” “A Triazine-Based Covalent Organic Framework for High-Efficiency CO2 Capture” “The Design and Synthesis of a New Family of Small Molecule Inhibitors Targeting the BCL-2 Protein” ","date":"2023-02-15","objectID":"/chemicaldiffusion/:0:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"Running the model The fun of generative models is in running it yourself of course. I’ve uploaded a tuned checkpoint that can be freely downloaded. If you’re not familiar with the process, here’s a quick guide: Install a Stable Diffusion UI. I’ve been using this one, which has good installation instructions and works on both windows with NVIDIA/AMD GPUs and apple silicon. Download the trained chemical diffusion checkpoint hosted here on hugging face - you just need to put the .ckpt file (~2.5GB) in the \\stable-diffusion-webui\\models\\Stable-diffusion folder Run the UI and have fun! ","date":"2023-02-15","objectID":"/chemicaldiffusion/:1:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"Notes Taking a page from the larger Stable Diffusion community, negative prompts can clean up the generated images - I’ve used ‘out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts’. Different samplers can have a big effect as well. Here’s a grid showing the same prompt with several different samplers: “The Discovery of a Highly Efficient and Stable Iridium Catalyst for the Oxygen Reduction Reaction in Fuel Cells” ","date":"2023-02-15","objectID":"/chemicaldiffusion/:2:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"Training As I’m VRAM-poor (running on 8GB 3060Ti), recent optimizations have made it possible to finetune SD on my home system where a few months ago twice as much memory was needed. I used the training UI this repo which made the process very easy. YMMV - I ended up being able to train with batch sizes of 4, but frequently encountered the dreaded CUDA out of memory error. ","date":"2023-02-15","objectID":"/chemicaldiffusion/:3:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"For the language-to-language components of this JACS does not exist, I chose Google’s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model. I had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE. As abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5’s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here’s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer. tokenizer = AutoTokenizer.from_pretrained('t5-base') abs_lengths = df['Abstract'].map(lambda x: len(tokenizer(x)['input_ids'])) abs_lengths.hist(bins = 100) It seems that the vast majority of abstracts are within the 512 token limit, so I didn’t do any further preprocessing. Note - unlike the GPT-2 tokenizer discussed in a previous post, the T5 tokenizer has separate tokens for padding and end-of-sequence, so we don’t need change anything. To fine-tune the base T5 model, I used a standard Huggingface sequence-to-sequence trainer structure with a pytorch dataset, similar to the one I used for the vision encoder-decoder model discussed in a previous post. For my previous text generators toc2title a simple model.generate() call was sufficient while title2abstract worked well after adding no_repeat_ngram_size=2. However, for abstract2title, I wanted to generate multiple distinct but convincing title suggestions from an abstract - essentially a summarization task. After finetuning, the simplest generation methods (greedy and beam search) resulted in very similar suggestions each time. Instead, I used top-K and top-p sampling to generate more distinct and surprising text. This is easily implemented with the top_k and top_p arguments to the generate() method: generated_ids = model.generate( input_ids, max_length=128, num_return_sequences=3, do_sample=True, top_k=5, top_p=0.95, early_stopping=True) If we take the example of my own first paper, you can see that beam search generates very similar results while even with a very limited amount of text, the sampling methods give much more diversity. Beam search Top-K + top-p sampling 1. Negative Thermal Expansion in a Metal–Organic Framework 1. Negative Thermal Expansion in a Metal–Organic Framework 2. Negative Thermal Expansion in a Metal–Organic Framework Lattice 2. Dynamics of Metal–Organic Frameworks 3. Negative Thermal Expansion in Metal–Organic Frameworks 3. Effect of Metal–Organic Framework Contraction on Negative Thermal Expansion (Abstract: The action behind contraction: The metal–organic framework [Cu 3 (btc) 2] displays negative thermal expansion (NTE) over a broad temperature range. This property arises from two coincident mechanisms, each of which are unique for NTE systems: the concerted transverse vibration of triangular organic linkers, and the local dynamic distortion of dinuclear metal centers within the framework lattice.) The real title? Negative Thermal Expansion in the Metal-Organic Framework Material Cu3(1,3,5-benzenetricarboxylate)2 ","date":"2022-08-25","objectID":"/t5/:0:0","tags":null,"title":"Training T5 models and generating text","uri":"/t5/"},{"categories":null,"content":"Many people enjoyed title2abstract from the this JACS does not exist project so I inverted the training parameters for a quick follow up. Presenting abstract2title: You can also test the title2abstract and toc2title apps in my previous post. ","date":"2022-08-22","objectID":"/abstract2title/:0:0","tags":null,"title":"Abstract2title","uri":"/abstract2title/"},{"categories":null,"content":"Broadly: materials science, applications of machine learning, autonomous experimentation Projects I’ve worked on previously: A new method for intelligent exploration of design spaces beyond Bayesian optimization. With Bayesian algorithm execution (BAX), any design space exploration question can easily be optimized, e.g.: “find synthesis conditions for nanoparticles with dispersity \u003c5% and sizes of {10, 20, 30} ± 0.5nm”. Invented a way to make covalent organic frameworks (‘COFs’) in water, in minutes, using ultrasound. Usually they are made in toxic organic solvents over 3 days in an oven at 120 °C. Created a method to 3D print pure metal-organic framework (‘MOF’) monoliths without binder by extruding a colloidal gel of MOF nanoparticles. Discovered a new family of hypophosphite-based hybrid perovskites Used in situ X-ray diffraction to understand the formation processes of MOFs. Reported the first full characterisation of negative thermal expansion in MOFs. See my Google scholar for a full list of papers. ","date":"2022-08-05","objectID":"/research/:0:0","tags":null,"title":"Research","uri":"/research/"},{"categories":null,"content":" Research Associate, SSRL, SLAC, Stanford. 2022- Research Lead, Cooper group, Materials Innovation Factory, University of Liverpool. 2019-2021 Postdoc, Cheetham \u0026 Wang groups, National University of Singapore. 2017-2019 Postdoc, Cheetham group, University of Cambridge. 2016-2017 Postdoc, O’Hare group, University of Oxford. 2014-2016 PhD (Chemistry), Kepert group, University of Sydney. 2008-2013 BSc (Adv) Hons I, University of Sydney. 2004-2007 ","date":"2022-08-01","objectID":"/bio/:0:0","tags":null,"title":"Bio","uri":"/bio/"},{"categories":null,"content":" An imaginary abstract generated at thisJACSdoesnotexist.com In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee) In general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train: A generative adversarial network (StyleGAN3) - a model which learns to generate images similar to those in its training set. A finetuned version of GPT-2 that generates chemistry-paper-title-like lists of words. A vision encoder-decoder model (test it out here) that converts images to the appropriate text - here, the above GPT-2 model acts as the text generator. A T5 sequence-to-sequence model (test it out here) that generates a text sequence from a prompt. For those who are interested in the inner workings or may want to do something similar, I’ve included a writeup of the project below. Examples of the code are available on my GitHub. As you’ll see, a lot of my choices were fairly abitrary, arising more-or-less from the availability heuristic. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:0:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Training the GAN ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Getting the dataset ready My initial idea was simply to see how well a GAN could generate ToC images, and so I needed a whole bunch of ToC images. I used the popular python package beautifulsoup with a headless browser and some simple loops to scrape the JACS website for the ToC images, titles and abstracts. The full dataset from around 60,000 papers could be collected overnight. Note: this was relatively easy with JACS (and ACS journals in general) as the website organisation follows a logical structure. YMMV for other publishers’ platforms. Generally, ML training inputs for image models should be the same size, often with powers of 2 as values - StyleGAN3 uses square images of size 128, 256, 512, and 1024 pixels. Luckily, JACS ToC images are fixed to 500 pixels in width and maximum height. I padded the figures to 512x512 using Irfanview’s batch processing tool, then resized those images to generate smaller datasets at 128 and 256 px as well. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:1","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Running StyleGAN3 I chose StyleGAN3 as I had heard of it before and 3 was the latest version number. At the time of writing it appears to be one of the best \u0026 most mature GANs for image generation. The implementation is fairly user-friendly and includes various data preparation and training scripts; the documentation is excellent and deserves commendation. The first step was to use the dataset_tool.py script to prepare a dataset in a form the model could use - something like this: python dataset_tool.py --source=\"C:\\...\\512px\" --dest=\"C:\\...\\512px.zip\" After this, it’s straightforward to train the model using the following script: python train.py --outdir=\"C:\\...\\training_runs\" --cfg=stylegan3-t --data=\"C:\\...\\512px.zip\" --gpus=1 --batch=32 --gamma=8 --batch-gpu=8 --snap=10 --cbase=16384 The key parameters that need to be set are batch size and gamma, which is the R1 regularization parameter. ‘cbase=16384’ speeds up training by sacrificing network capacity, but at low resolutions I didn’t find it to be a problem. Having only a humble RTX 3060 Ti (the best I could get during the GPU apocalypse of 2020-2021), the meagre 8 GB of VRAM was a limiting factor for batch sizes. I also found useful the –resume flag, which allows you to resume training from a previous network snapshot in case of crashes, although I found I also had to edit the resume_kimgs variable in the training script directly to get it to work properly. The tensorbard output is also very good, and can be run like so: tensorboard --logdir \"C:\\...\\\u003ctraining run directory\u003e\" I considered training on cloud compute but seemed to be more expensive than training locally, and also required learning a new thing. In retrospect this might have been a poor life choice. My wife was still annoyed that our power bill doubled during the month I was training though! Also that the computer was essentially a fan heater running in the bedroom during the height of summer. After bluescreening a couple of times I removed the computer side panel which dropped temps by a couple of degrees. Fakes generated from a model trained on 128 px images I ran a first test with a 128 px as proof of concept, which was convincing enough that I scaled up to 256 px. In principle resolutions of up to 1024 px are available, but the raw data resolution mean that going above 512 was meaningless, and compute requirements made that unfeasible on my home system. After training continuously for a few weeks, the 256 px model started to converge. In this case I used a common evaluation metric for GANs called the Fréchet Inception Distance (FID), a measure of similarity between image distributions. Below is a video of images generated from the model as it trains - each successive frame, the models has trained on an additional 40,000 images. Overall, the model saw images from the training set around 5 million times. GAN training progress video ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:2","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Generating fake ToC images With the GAN model trained, it was time to generate the fake ToC images. This is done very simply with this StyleGAN script: python gen_images.py --outdir=\"C:\\...\\output\" --trunc=0.8 --seeds=0-50000 --network=\"C:\\...\\network-snapshot-....pkl\" The key parameters here are ψ (“trunc”), which controls the tradeoff between image quality and weirdness (higher values are weirder). Some examples below: ψ = 0.2 ψ = 0.8 ψ = 1.1 ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:3","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Assembling a ToC to title model With generated ToC images in hand, I wondered if it was possible to generate relevant paper titles. Having labels in the form of titles for all the ToCs in the training set, the challenge was finding an appropriate model. After trying a couple of things, I settled on using 🤗 Hugging Face’s transformers library, mostly due to its good documentation and straightforward code structuring. I used the 🤗 vision encoder decoder model as the basis for the toc-to-title model. This model can easily be warm-started with a pretrained vision transformer plus a pretrained language transformer model. This is very convenient as big tech companies have already spent millions of dollars and vast amounts of compute training models which then can be easily fine-tuned or even just used out-of-the-box. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Fine-tuning GPT-2 to generate paper titles I chose GPT-2 as the base for the language model. Once considered too dangerous to fully release, GPT-2 is now the most popular model hosted on Hugging Face. Much more powerful (and presumably commensurately dangerous) models such as the 20 billion parameter GPT-NeoX are now available (cf. the mere 1.5B parameters of GPT-2), however for the purposes of this project the relative computational tractability of tuning GPT-2 makes it an excellent choice. GPT-2 is based on what is known as a decoder-only model (at this point, what I’ll dubb “Godwin’s law but for AI” comes into effect - as the length of this ML article increases, the probability that I’ll direct you to the original transformers paper, “Attention Is All You Need” , approaches 1). These models are good for text generation, but the base model is designed to generate text of an arbitrary length, so I had to make a couple of tweaks to get the model to generate paper-title-style sentences. Language models such a GPT-2 first tokenize their inputs - that is, they break human-readable language into a set of tokens that may have some relationship to the underlying structure of the language, such as words, subwords, or in the case of GPT-2 byte-pair encoding. Additionally, tokens for special occurences such as beginning/end of sequence, or padding, can be used. As GPT-2 only uses end-of-sequence tokens by default, I added a padding token “\u003c|pad|\u003e” to distinguish between the padding required for setting all training inputs to the same size, and the actual end of a title. As a rank amateur, this took quite a long time to work out so I share it here in the hope that it may be helpful to someone. I may do a full writeup in a separate post. Sample code below: from transformers import GPT2Tokenizer text_processor = GPT2Tokenizer.from_pretrained(\"gpt2\", pad_token=\"\u003c|pad|\u003e\") I then appended end-of-sequence tokens to each of the training outputs. Finally, it’s imporant to resize the token embeddings of the model so that the extra token is accounted for: from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # To account for extra padding token model.resize_token_embeddings(len(text_processor)) Having done that, the model can then be trained and produces convincing results. Here’s a sample of real titles compared to GPT-2-JACS completions using the first 4 words of the real title as a prompt: Real GPT-2-JACS completion Origin of Dark-Channel X-ray Fluorescence from Transition-Metal Ions in Water Origin of Dark-Channel X-ray Absorption Fine Structure of (NH3)4Ca(OH)5 in Supercritical Carbon Dioxide Comprehensive Thermochemistry of W–H Bonding in the Metal Hydrides CpW(CO)2(IMes)H, [CpW(CO)2(IMes)H]•+, and [CpW(CO)2(IMes)(H)2]+. Influence of an N-Heterocyclic Carbene Ligand on Metal Hydride Bond Energies Comprehensive Thermochemistry of W–H and H–H Bonds in the Lanthanide Phosphate (Ln5Me4) System Fragmentation Energetics of Clusters Relevant to Atmospheric New Particle Formation Fragmentation Energetics of Clusters Based on Cluster Modification: Assignment of the Concentration-Dependent Rate Constant Transient Photoconductivity of Acceptor-Substituted Poly(3-butylthiophene) Transient Photoconductivity of Acceptor-Substituted Layered Zirconium Oxides Palladium-Catalyzed Aerobic Oxidative Cyclization of N-Aryl Imines: Indole Synthesis from Anilines and Ketones Palladium-Catalyzed Aerobic Oxidative Cyclization of Unactivated Alkenes Mild Aerobic Oxidative Palladium (II) Catalyzed C−H Bond Functionalization: Regioselective and Switchable C−H Alkenylation and Annulation of Pyrroles Mild Aerobic Oxidative Palladium(II)-Catalyzed Arylation of Indoles: Access to Chiral Olefins A Pentacoordinate Boron-Containing π-Electron System with Cl–B–Cl Three-Center Four-Electron Bonds A Pentacoordinate Boron-Containing π-Electron System for High-Performance Polymer Solar Cells Ferroelectric Alkylamide-Substituted Helicene","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:1","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Choosing a vision model Vision transformer models seem to have superceded CNNs as cutting edge vision models. Initially (and perhaps still) I thought that fine-tuning a vision transformer such as ViT or Swin would be the way to go. I tried pre-training both of these with masked image modelling (helpful repo here), which uses a partially masked version of original images as training data and the unmasked version as the ground truth. However in the end the vanilla BEiT proved to work better out of the box. A reminder to not reinvent wheels. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:2","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Putting it all together Once the vision and language models are selected, its a simple matter of combining them then training the model via the standard transformers api. (Full writeup in subsequent post) from transformers import BeitConfig, GPT2Config # Load pretrained components config_encoder = BeitConfig.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\") config_decoder = GPT2Config.from_pretrained(\"local model folder/\") # set decoder config to causal lm config_decoder.is_decoder = True config_decoder.add_cross_attention = True config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder) # Initializing the model model = VisionEncoderDecoderModel(config=config) ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:3","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Training a title-to-abstract model Section to be completed. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:3:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Test the models here I’ve generated huggingface spaces with gradio apps - they are embedded below if you want to test the models. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:4:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Image to title generator ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:4:1","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Title to abstract generator ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:4:2","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"I enjoy taking care of plants and making time-lapse videos of their growth - lots of examples on my youtube channel. I sometimes design and 3D print things. I used to be an avid archer and martial artist. ","date":"0001-01-01","objectID":"/personal/:0:0","tags":null,"title":"Personal","uri":"/personal/"}]