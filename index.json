[{"categories":null,"content":"For associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign. Intro Chinese characters are pretty cool, and there’s a lot of them; around 21,000 are represented in unicode. Interestingly, they encode meaning in multiple ways. Some are simple pictures of things - 山 (shān) means, as you might be able to guess, ‘mountain’. Characters can also be compounded: 好 (hǎo) meaning ‘good’, is constructed from the pictograms for 女 (nǚ), ‘woman’ and 子 (zǐ), child. Read into that what you will. Compounds often contain a semantic and a phonetic component. For example, the names of most chemical elements such as the metallic element lithium, 锂 (lǐ) are composed of a semantic radical (⻐, the condensed form of 金 meaning ‘metal’) and a phonetic component 里 (lǐ) which approximates the ’li’ in lithium. I’ve been interested for a while in how Chinese characters might be explored using machine learning. Can we teach a model to understand how they are structured from pictures and definitions alone? Let’s talk about a couple of ways of engaging with this, starting with relatively simple ML models and moving towards state-of-the-art technologies. Dataset First we need a dataset containing specific images of as many known Chinese characters as possible. In modern computers, the unicode standard is a convenient method of accessing a large portion of known Chinese characters. Fonts provide specific way of representing characters, in the form of distinct glyphs. Fonts have different levels of completeness, so choosing one that contained as many characters as possible for a large and diverse dataset was important. The main resources used for generating the dataset: The unicode CJK Unified Ideographs block (4E00–9FFF) of 20,992 Chinese characters Google fonts (Noto Sans and others) English definitions from the unihan database (kDefinition.txt) - this becomes useful later Jun Da’s Chinese character frequency list Let’s make the dataset by saving the glyphs as pictures of the square 2n×2n format favoured by convention. I found that 64x64 and 128x128 work well. Visualise the character set in 2D (Jupyter notebook) First let’s try to map the space with a dimensional reduction. UMAP works well here. (hover/tap the points to show each character in the plot.) Distinct clusters emerge - the largest cluster on the left represents the “lefty-righty” characters (to use the technical term), while the second largest cluster on the right represents “uppy-downy” characters. Tighter clusters/subclusters tend to correspond to characters that share the same radical. (Hover over/tap points to see the characters) Variational autoencoder Now, we use a variational autoencoder (VAE) to model the underlying distribution of Chinese characters. This architecture works by using convolutional layers to reduce the dimensionality of the input over several steps, then using a reverse convolution to increase the dimensionality back to the original. The network is trained on its ability to output a match to the original. The key idea is that the low-dimensional bottleneck is still able to describe the distribution being modeled, and the VAE decoder translates from that low dimensional latent space back to, in our case, images. In this case, we go from 64×64=4096 to a single vector of length 256 which represents the latent space. As you can, the VAE learns a fairly good latent representation of our glyphs: Training the VAE has an extra subtlety compared to a vanilla autoencoder which is only graded on reconstruction. Our VAE loss function consists of not just reconstruction loss, but also the Kullback–Leibler divergence, which is a measure of distance between two probability distributions. In this case, we want to minimize the distance between the learned distribution of Chinese glyphs and the normal distribution N(0,I). This pushes the latent distribution ","date":"2024-05-26","objectID":"/glyphexplorer/:0:0","tags":null,"title":"Teaching generative AI the structure of Chinese characters","uri":"/glyphexplorer/"},{"categories":null,"content":"For associated code, please see the Jupyter notebook in the github repository While machine learning has been very successful in image-processing applications, there’s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I’ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn’t rely on specific feature tracking. A short clip of unstabilized footage ","date":"2024-02-14","objectID":"/opticalflow/:0:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Background I had a vague conception that tracking a similarity metric between two frames would furnish the shift between then, perhaps using least-squares peak fitting in a similar way to the fitting of X-ray diffraction data (once you have a hammer, everything looks like a nail!) by performing gradient descent with small shifts to the two frames. To my great relief I didn’t have to write this code as further investigation revealed OpenCV contains a feature called optical flow which essentially performs this calculation, outputting vectors for the movement of every pixel between two frames. This could form the basis of stabilisation code. ","date":"2024-02-14","objectID":"/opticalflow/:1:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Execution: optical flow interpretation My exploratory calculations showed that the optical flow vectors did indeed provide a clear picture of shift direction and magnitude. Optical flow analysis of a frame transition with a small shift downward You can see in the maps above the algorithm detects pixels that can be mapped to a clear shift (furniture etc.) while uniform areas like wallpaper aren’t picked up. The regions of uniform grey in the “flow magnitude” plot indicate a consistent shift, and the sharp peak in the histogram of shifts confirms this - the camera view has tilted by around 16 pixels. Likewise for the “flow angle plot”, the peak in the histogram at ~90° indicates a shift downwards. The next step was to work out how to use these results. It was clear that this was already a relatively expensive calculation and it would have to be performed on ~18,000 pairs of frames for my test timelapse dataset. (Aside: luckily, OpenCV has easily accessible GPU acceleration with OpenCL. Simply replacing standard matrices with UMat objects speeds up the calculations by an order of magnitude. I looked into using the CUDA implementation which would likely be even faster, but it seemed more trouble than it was worth to install.) While peak fitting the histograms would yield the best result in principle, I opted for an economical approach to save compute: filter out magnitudes that were the first entry in the array (i.e. the top of the decay curve, not a peak), only include angles within 5 of 90/270 degrees (i.e. vertical shifts), then take the max values and round the angles to 90 or 270. This part was done as a single script outputting a CSV file of the 4 relevant parameters extracted from histograms for each frame - vector magnitude, vector angle, and the frequencies of each . (In principle the full optical flow vector set could be saved but file would be huge. The 17K frame dataset took around 90 min on my system which was an acceptable tradeoff for repeats.) ","date":"2024-02-14","objectID":"/opticalflow/:2:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Execution: stabilisation This part was simple in principle but somewhat difficult in practice. The idea was very simple - by cropping the top and bottom of each frame by the maximum total shift across the dataset, the relative shift of each frame could be controlled by varying the shifts above and below. The execution: Calculate the relative shift from the starting position at for each frame of the dataset by summing the up and down shifts. The overall window size across the set can be extrapolated by the distance between the maximum and minimum shift. Then for the starting frames, crop down to the window size. Afterwards, perform the first crop plus the relative shift calculated previously to shift the visible window to a consistent location. It’s difficult to explain clearly but the code and demonstration make it clearer. The unstabilized footage from above\rThe optical flow stabilized footage\r","date":"2024-02-14","objectID":"/opticalflow/:3:0","tags":null,"title":"Optical flow timelapse stabiliser","uri":"/opticalflow/"},{"categories":null,"content":"Generative text-to-image models have recent become very popular. Having a bunch of fully captioned images left over from the This JACS Does Not Exist project, I’ve trained a Stable Diffusion checkpoint on that dataset (~60K JACS table-of-contents images with matched paper titles). It seems to work pretty well. Here are some examples of prompts and the resulting images generated: “Development of a Highly Efficient and Selective Catalytic Enantioselective Hydrogenation for Organic Synthesis” “Lead-free Cs2AgBiBr6 Perovskite Solar Cells with High Efficiency and Stability” “A Triazine-Based Covalent Organic Framework for High-Efficiency CO2 Capture” “The Design and Synthesis of a New Family of Small Molecule Inhibitors Targeting the BCL-2 Protein” ","date":"2023-02-15","objectID":"/chemicaldiffusion/:0:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"Running the model The fun of generative models is in running it yourself of course. I’ve uploaded a tuned checkpoint that can be freely downloaded. If you’re not familiar with the process, here’s a quick guide: Install a Stable Diffusion UI. I’ve been using this one, which has good installation instructions and works on both windows with NVIDIA/AMD GPUs and apple silicon. Download the trained chemical diffusion checkpoint hosted here on hugging face - you just need to put the .ckpt file (~2.5GB) in the \\stable-diffusion-webui\\models\\Stable-diffusion folder Run the UI and have fun! ","date":"2023-02-15","objectID":"/chemicaldiffusion/:1:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"Notes Taking a page from the larger Stable Diffusion community, negative prompts can clean up the generated images - I’ve used ‘out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts’. Different samplers can have a big effect as well. Here’s a grid showing the same prompt with several different samplers: “The Discovery of a Highly Efficient and Stable Iridium Catalyst for the Oxygen Reduction Reaction in Fuel Cells” ","date":"2023-02-15","objectID":"/chemicaldiffusion/:2:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"Training As I’m VRAM-poor (running on 8GB 3060Ti), recent optimizations have made it possible to finetune SD on my home system where a few months ago twice as much memory was needed. I used the training UI this repo which made the process very easy. YMMV - I ended up being able to train with batch sizes of 4, but frequently encountered the dreaded CUDA out of memory error. ","date":"2023-02-15","objectID":"/chemicaldiffusion/:3:0","tags":null,"title":"Chemical Diffusion","uri":"/chemicaldiffusion/"},{"categories":null,"content":"For the language-to-language components of this JACS does not exist, I chose Google’s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model. I had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE. As abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5’s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here’s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer. tokenizer = AutoTokenizer.from_pretrained('t5-base') abs_lengths = df['Abstract'].map(lambda x: len(tokenizer(x)['input_ids'])) abs_lengths.hist(bins = 100) It seems that the vast majority of abstracts are within the 512 token limit, so I didn’t do any further preprocessing. Note - unlike the GPT-2 tokenizer discussed in a previous post, the T5 tokenizer has separate tokens for padding and end-of-sequence, so we don’t need change anything. To fine-tune the base T5 model, I used a standard Huggingface sequence-to-sequence trainer structure with a pytorch dataset, similar to the one I used for the vision encoder-decoder model discussed in a previous post. For my previous text generators toc2title a simple model.generate() call was sufficient while title2abstract worked well after adding no_repeat_ngram_size=2. However, for abstract2title, I wanted to generate multiple distinct but convincing title suggestions from an abstract - essentially a summarization task. After finetuning, the simplest generation methods (greedy and beam search) resulted in very similar suggestions each time. Instead, I used top-K and top-p sampling to generate more distinct and surprising text. This is easily implemented with the top_k and top_p arguments to the generate() method: generated_ids = model.generate( input_ids, max_length=128, num_return_sequences=3, do_sample=True, top_k=5, top_p=0.95, early_stopping=True) If we take the example of my own first paper, you can see that beam search generates very similar results while even with a very limited amount of text, the sampling methods give much more diversity. Beam search Top-K + top-p sampling 1. Negative Thermal Expansion in a Metal–Organic Framework 1. Negative Thermal Expansion in a Metal–Organic Framework 2. Negative Thermal Expansion in a Metal–Organic Framework Lattice 2. Dynamics of Metal–Organic Frameworks 3. Negative Thermal Expansion in Metal–Organic Frameworks 3. Effect of Metal–Organic Framework Contraction on Negative Thermal Expansion (Abstract: The action behind contraction: The metal–organic framework [Cu 3 (btc) 2] displays negative thermal expansion (NTE) over a broad temperature range. This property arises from two coincident mechanisms, each of which are unique for NTE systems: the concerted transverse vibration of triangular organic linkers, and the local dynamic distortion of dinuclear metal centers within the framework lattice.) The real title? Negative Thermal Expansion in the Metal-Organic Framework Material Cu3(1,3,5-benzenetricarboxylate)2 ","date":"2022-08-25","objectID":"/t5/:0:0","tags":null,"title":"Training T5 models and generating text","uri":"/t5/"},{"categories":null,"content":"Many people enjoyed title2abstract from the this JACS does not exist project so I inverted the training parameters for a quick follow up. Presenting abstract2title: You can also test the title2abstract and toc2title apps in my previous post. ","date":"2022-08-22","objectID":"/abstract2title/:0:0","tags":null,"title":"Abstract2title","uri":"/abstract2title/"},{"categories":null,"content":"Broadly: materials science, applications of machine learning, autonomous experimentation Projects I’ve worked on previously: Invented a way to make covalent organic frameworks (‘COFs’) in water, in minutes, using ultrasound. Usually they are made in toxic organic solvents over 3 days in an oven at 120 °C. Created a method to 3D print pure metal-organic framework (‘MOF’) monoliths without binder by extruding a colloidal gel of MOF nanoparticles. Discovered a new family of hypophosphite-based hybrid perovskites Used in situ X-ray diffraction to understand the formation processes of MOFs. Reported the first full characterisation of negative thermal expansion in MOFs. See my Google scholar for a full list of papers. ","date":"2022-08-05","objectID":"/research/:0:0","tags":null,"title":"Research","uri":"/research/"},{"categories":null,"content":" Research Associate, SSRL, SLAC, Stanford. 2022- Research Lead, Cooper group, Materials Innovation Factory, University of Liverpool. 2019-2021 Postdoc, Cheetham \u0026 Wang groups, National University of Singapore. 2017-2019 Postdoc, Cheetham group, University of Cambridge. 2016-2017 Postdoc, O’Hare group, University of Oxford. 2014-2016 PhD (Chemistry), Kepert group, University of Sydney. 2008-2013 BSc (Adv) Hons I, University of Sydney. 2004-2007 ","date":"2022-08-01","objectID":"/bio/:0:0","tags":null,"title":"Bio","uri":"/bio/"},{"categories":null,"content":" An imaginary abstract generated at thisJACSdoesnotexist.com In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee) In general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train: A generative adversarial network (StyleGAN3) - a model which learns to generate images similar to those in its training set. A finetuned version of GPT-2 that generates chemistry-paper-title-like lists of words. A vision encoder-decoder model (test it out here) that converts images to the appropriate text - here, the above GPT-2 model acts as the text generator. A T5 sequence-to-sequence model (test it out here) that generates a text sequence from a prompt. For those who are interested in the inner workings or may want to do something similar, I’ve included a writeup of the project below. Examples of the code are available on my GitHub. As you’ll see, a lot of my choices were fairly abitrary, arising more-or-less from the availability heuristic. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:0:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Training the GAN ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Getting the dataset ready My initial idea was simply to see how well a GAN could generate ToC images, and so I needed a whole bunch of ToC images. I used the popular python package beautifulsoup with a headless browser and some simple loops to scrape the JACS website for the ToC images, titles and abstracts. The full dataset from around 60,000 papers could be collected overnight. Note: this was relatively easy with JACS (and ACS journals in general) as the website organisation follows a logical structure. YMMV for other publishers’ platforms. Generally, ML training inputs for image models should be the same size, often with powers of 2 as values - StyleGAN3 uses square images of size 128, 256, 512, and 1024 pixels. Luckily, JACS ToC images are fixed to 500 pixels in width and maximum height. I padded the figures to 512x512 using Irfanview’s batch processing tool, then resized those images to generate smaller datasets at 128 and 256 px as well. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:1","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Running StyleGAN3 I chose StyleGAN3 as I had heard of it before and 3 was the latest version number. At the time of writing it appears to be one of the best \u0026 most mature GANs for image generation. The implementation is fairly user-friendly and includes various data preparation and training scripts; the documentation is excellent and deserves commendation. The first step was to use the dataset_tool.py script to prepare a dataset in a form the model could use - something like this: python dataset_tool.py --source=\"C:\\...\\512px\" --dest=\"C:\\...\\512px.zip\" After this, it’s straightforward to train the model using the following script: python train.py --outdir=\"C:\\...\\training_runs\" --cfg=stylegan3-t --data=\"C:\\...\\512px.zip\" --gpus=1 --batch=32 --gamma=8 --batch-gpu=8 --snap=10 --cbase=16384 The key parameters that need to be set are batch size and gamma, which is the R1 regularization parameter. ‘cbase=16384’ speeds up training by sacrificing network capacity, but at low resolutions I didn’t find it to be a problem. Having only a humble RTX 3060 Ti (the best I could get during the GPU apocalypse of 2020-2021), the meagre 8 GB of VRAM was a limiting factor for batch sizes. I also found useful the –resume flag, which allows you to resume training from a previous network snapshot in case of crashes, although I found I also had to edit the resume_kimgs variable in the training script directly to get it to work properly. The tensorbard output is also very good, and can be run like so: tensorboard --logdir \"C:\\...\\\u003ctraining run directory\u003e\" I considered training on cloud compute but seemed to be more expensive than training locally, and also required learning a new thing. In retrospect this might have been a poor life choice. My wife was still annoyed that our power bill doubled during the month I was training though! Also that the computer was essentially a fan heater running in the bedroom during the height of summer. After bluescreening a couple of times I removed the computer side panel which dropped temps by a couple of degrees. Fakes generated from a model trained on 128 px images I ran a first test with a 128 px as proof of concept, which was convincing enough that I scaled up to 256 px. In principle resolutions of up to 1024 px are available, but the raw data resolution mean that going above 512 was meaningless, and compute requirements made that unfeasible on my home system. After training continuously for a few weeks, the 256 px model started to converge. In this case I used a common evaluation metric for GANs called the Fréchet Inception Distance (FID), a measure of similarity between image distributions. Below is a video of images generated from the model as it trains - each successive frame, the models has trained on an additional 40,000 images. Overall, the model saw images from the training set around 5 million times. GAN training progress video ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:2","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Generating fake ToC images With the GAN model trained, it was time to generate the fake ToC images. This is done very simply with this StyleGAN script: python gen_images.py --outdir=\"C:\\...\\output\" --trunc=0.8 --seeds=0-50000 --network=\"C:\\...\\network-snapshot-....pkl\" The key parameters here are ψ (“trunc”), which controls the tradeoff between image quality and weirdness (higher values are weirder). Some examples below: ψ = 0.2 ψ = 0.8 ψ = 1.1 ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:1:3","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Assembling a ToC to title model With generated ToC images in hand, I wondered if it was possible to generate relevant paper titles. Having labels in the form of titles for all the ToCs in the training set, the challenge was finding an appropriate model. After trying a couple of things, I settled on using 🤗 Hugging Face’s transformers library, mostly due to its good documentation and straightforward code structuring. I used the 🤗 vision encoder decoder model as the basis for the toc-to-title model. This model can easily be warm-started with a pretrained vision transformer plus a pretrained language transformer model. This is very convenient as big tech companies have already spent millions of dollars and vast amounts of compute training models which then can be easily fine-tuned or even just used out-of-the-box. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Fine-tuning GPT-2 to generate paper titles I chose GPT-2 as the base for the language model. Once considered too dangerous to fully release, GPT-2 is now the most popular model hosted on Hugging Face. Much more powerful (and presumably commensurately dangerous) models such as the 20 billion parameter GPT-NeoX are now available (cf. the mere 1.5B parameters of GPT-2), however for the purposes of this project the relative computational tractability of tuning GPT-2 makes it an excellent choice. GPT-2 is based on what is known as a decoder-only model (at this point, what I’ll dubb “Godwin’s law but for AI” comes into effect - as the length of this ML article increases, the probability that I’ll direct you to the original transformers paper, “Attention Is All You Need” , approaches 1). These models are good for text generation, but the base model is designed to generate text of an arbitrary length, so I had to make a couple of tweaks to get the model to generate paper-title-style sentences. Language models such a GPT-2 first tokenize their inputs - that is, they break human-readable language into a set of tokens that may have some relationship to the underlying structure of the language, such as words, subwords, or in the case of GPT-2 byte-pair encoding. Additionally, tokens for special occurences such as beginning/end of sequence, or padding, can be used. As GPT-2 only uses end-of-sequence tokens by default, I added a padding token “\u003c|pad|\u003e” to distinguish between the padding required for setting all training inputs to the same size, and the actual end of a title. As a rank amateur, this took quite a long time to work out so I share it here in the hope that it may be helpful to someone. I may do a full writeup in a separate post. Sample code below: from transformers import GPT2Tokenizer text_processor = GPT2Tokenizer.from_pretrained(\"gpt2\", pad_token=\"\u003c|pad|\u003e\") I then appended end-of-sequence tokens to each of the training outputs. Finally, it’s imporant to resize the token embeddings of the model so that the extra token is accounted for: from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # To account for extra padding token model.resize_token_embeddings(len(text_processor)) Having done that, the model can then be trained and produces convincing results. Here’s a sample of real titles compared to GPT-2-JACS completions using the first 4 words of the real title as a prompt: Real GPT-2-JACS completion Origin of Dark-Channel X-ray Fluorescence from Transition-Metal Ions in Water Origin of Dark-Channel X-ray Absorption Fine Structure of (NH3)4Ca(OH)5 in Supercritical Carbon Dioxide Comprehensive Thermochemistry of W–H Bonding in the Metal Hydrides CpW(CO)2(IMes)H, [CpW(CO)2(IMes)H]•+, and [CpW(CO)2(IMes)(H)2]+. Influence of an N-Heterocyclic Carbene Ligand on Metal Hydride Bond Energies Comprehensive Thermochemistry of W–H and H–H Bonds in the Lanthanide Phosphate (Ln5Me4) System Fragmentation Energetics of Clusters Relevant to Atmospheric New Particle Formation Fragmentation Energetics of Clusters Based on Cluster Modification: Assignment of the Concentration-Dependent Rate Constant Transient Photoconductivity of Acceptor-Substituted Poly(3-butylthiophene) Transient Photoconductivity of Acceptor-Substituted Layered Zirconium Oxides Palladium-Catalyzed Aerobic Oxidative Cyclization of N-Aryl Imines: Indole Synthesis from Anilines and Ketones Palladium-Catalyzed Aerobic Oxidative Cyclization of Unactivated Alkenes Mild Aerobic Oxidative Palladium (II) Catalyzed C−H Bond Functionalization: Regioselective and Switchable C−H Alkenylation and Annulation of Pyrroles Mild Aerobic Oxidative Palladium(II)-Catalyzed Arylation of Indoles: Access to Chiral Olefins A Pentacoordinate Boron-Containing π-Electron System with Cl–B–Cl Three-Center Four-Electron Bonds A Pentacoordinate Boron-Containing π-Electron System for High-Performance Polymer Solar Cells Ferroelectric Alkylamide-Substituted Helicene","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:1","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Choosing a vision model Vision transformer models seem to have superceded CNNs as cutting edge vision models. Initially (and perhaps still) I thought that fine-tuning a vision transformer such as ViT or Swin would be the way to go. I tried pre-training both of these with masked image modelling (helpful repo here), which uses a partially masked version of original images as training data and the unmasked version as the ground truth. However in the end the vanilla BEiT proved to work better out of the box. A reminder to not reinvent wheels. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:2","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Putting it all together Once the vision and language models are selected, its a simple matter of combining them then training the model via the standard transformers api. (Full writeup in subsequent post) from transformers import BeitConfig, GPT2Config # Load pretrained components config_encoder = BeitConfig.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\") config_decoder = GPT2Config.from_pretrained(\"local model folder/\") # set decoder config to causal lm config_decoder.is_decoder = True config_decoder.add_cross_attention = True config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder) # Initializing the model model = VisionEncoderDecoderModel(config=config) ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:2:3","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Training a title-to-abstract model Section to be completed. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:3:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Test the models here I’ve generated huggingface spaces with gradio apps - they are embedded below if you want to test the models. ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:4:0","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Image to title generator ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:4:1","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"Title to abstract generator ","date":"2022-08-01","objectID":"/thisjacsdoesnotexist/:4:2","tags":null,"title":"This JACS does not exist","uri":"/thisjacsdoesnotexist/"},{"categories":null,"content":"I enjoy taking care of plants and making time-lapse videos of their growth - lots of examples on my youtube channel. I sometimes design and 3D print things. I used to be an avid archer and martial artist. ","date":"0001-01-01","objectID":"/personal/:0:0","tags":null,"title":"Personal","uri":"/personal/"}]