<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation | Yue Wu</title>
<meta name=keywords content><meta name=description content="


In this post, I give an end-to-end example of using human feedback to align a diffusion model using direct preference optimization (DPO). Working code is provided so you can follow along."><meta name=author content="Yue Wu"><link rel=canonical href=http://localhost:1313/posts/diffusion-dpo/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/diffusion-dpo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css integrity=sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js integrity=sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:url" content="http://localhost:1313/posts/diffusion-dpo/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation"><meta property="og:description" content="In this post, I give an end-to-end example of using human feedback to align a diffusion model using direct preference optimization (DPO). Working code is provided so you can follow along."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-05-29T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation"><meta name=twitter:description content="


In this post, I give an end-to-end example of using human feedback to align a diffusion model using direct preference optimization (DPO). Working code is provided so you can follow along."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation","item":"http://localhost:1313/posts/diffusion-dpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation","name":"Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation","description":"\rIn this post, I give an end-to-end example of using human feedback to align a diffusion model using direct preference optimization (DPO). Working code is provided so you can follow along.\n","keywords":[],"articleBody":"\rIn this post, I give an end-to-end example of using human feedback to align a diffusion model using direct preference optimization (DPO). Working code is provided so you can follow along.\nIntro This post builds on my previous glyffuser project, where I build a diffusion model to generate novel Chinese-like glyphs. One problem was that many glyphs still disobey some rules of character construction and so look wrong to anyone who has some experience with Chinese characters. I’ve highlighted some examples in the picture below.\nExamples of glyffuser training data (left) and generated data (right); parts that ’look wrong’ are highlighted in red.\nWe’ll gather preference data from a human “expert” (me), then use DPO to align the model to only produce highly plausible glyphs. A preview of the results below—I think the improvement should be clear, even to the untrained eye! Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)\nHere’s what we’ll do:\nObtain a pretrained model Get human feedback data Implement the diffusion-DPO loss Create a training script Train the model and assess the results Background Using human feedback to align generative models is the key post-training ‘secret sauce’ that turned text completion models into useful chatbots. OpenAI used reinforcement learning with human feedback (RLHF) to create InstructGPT from the base GPT-3 model, paving the way for ChatGPT and the subsequent deluge of LLM-based chatbots.\nA drawback to RLHF is that it is somewhat complicated to implement: a separate “reward model” needs to be trained on human feedback data, and then used to align the base model by assessing its outputs and adjusting its weights accordingly.\nDirect preference optimization (DPO) is a streamlined reformulation of RLHF that has become a widely used alternative for human-feedback alignment in models such as Llama 3. Through mathematical wizardry, it dispenses with the need to train a reward model, instead allowing the model to be directly tuned on paired human preference data. Interestingly, DPO can also be used to align diffusion models.\nGetting human feedback data To use DPO, we need human feedback data in the form of pairs of samples where a human assesses one sample to be the winner and the other to be the loser. For the glyffuser, our samples are simply pairs of 128×128 pixel images generated by the model. The winning criterion was simply: “which sample is a more plausible Chinese glyph?”\nIn practical terms, I generated a 10,000 image set using the unconditional glyffuser model (you can test the repo here) and (since this is 2025), vibe coded a simple GUI for saving preference data. (Test it here)\nA simple UI for generating a human preference dataset. For this pair, those familiar with Chinese characters would almost certainly prefer the example on the right.\nThe key design thinking here was to use keyboard shortcuts to improve ergonomics. I included a skip function as well, to drop one sample where it wasn’t clear which was better. Using the GUI, the process was fairly painless, taking me around 2 hours to label 10,000 pairs (while listening to an audiobook!).\nThe result was a json file with around 2,500 pairs (around 5,000 samples being dropped) structured in this way:\n1 2 3 4 5 6 [{ \"better\": \"image1.jpg\", \"worse\": \"image2.jpg\", \"better_path\": \"C:/path/to/image1.jpg\", \"worse_path\": \"C:/path/to/image2.jpg\" }] Implementing the diffusion-DPO loss While the derivation of the diffusion-DPO loss is nontrivial, the implementation is fairly straightforward, as described in the pseudocode in section S9 of the diffusion-DPO paper:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def loss(model, ref_model, x_w, x_l, c, beta): \"\"\" # This is an example psuedo-code snippet for calculating the Diffusion-DPO loss # on a single image pair with corresponding caption model: Diffusion model that accepts prompt conditioning c and time conditioning t ref_model: Frozen initialization of model x_w: Preferred Image (latents in this work) x_l: Non-Preferred Image (latents in this work) c: Conditioning (text in this work) beta: Regularization Parameter returns: DPO loss value \"\"\" timestep = torch.randint(0, 1000) noise = torch.randn_like(x_w) noisy_x_w = add_noise(x_w, noise, t) noisy_x_l = add_noise(x_l, noise, t) model_w_pred = model(noisy_x_w, c, t) model_l_pred = model(noisy_x_l, c, t) ref_w_pred = ref_model(noisy_x_w, c, t) ref_l_pred = ref_model(noisy_x_l, c, t) model_w_err = (model_w_pred - noise).norm().pow(2) model_l_err = (model_l_pred - noise).norm().pow(2) ref_w_err = (ref_w_pred - noise).norm().pow(2) ref_l_err = (ref_l_pred - noise).norm().pow(2) w_diff = model_w_err - ref_w_err l_diff = model_l_err - ref_l_err inside_term = -1 * beta * (w_diff - l_diff) loss = -1 * log(sigmoid(inside_term)) return loss We can almost paste directly into python—see the [notebook] for the exact details. (Note: in the unconditional case, we can simply ignore c)\nBelow I’ll discuss what’s happening in this loss function; if you’re mainly interested in the implementation, feel free to skip to the next section.\nTo interpret the DPO loss, we need to recall the standard diffusion training loop for the noise-prediction model (commonly a U-net), the core of a diffusion model. If you aren’t familiar with this, I’ve written a diffusion explainer that goes through this topic clearly with minimal assumed knowledge.\nTo summarize, in the diffusion training loop we:\nDefine a noising trajectory via timesteps, where t = 0 is uncorrupted data and t = 1000 (or another large number) is pure (pixel-wise) Gaussian noise Take a random training example, and noise it to a random timestep t. This is efficient as Gaussian noise for any t can obtained in closed form. Pass the noised example through the noise-prediction model to obtain predicted noise. Update the weights of the noise-prediction model by minimizing the mean squared error loss between the predicted noise and actual noise, conditioned on the timestep. Repeat. Over many samples and timesteps, the model learns to predict noise for the training data distribution at any given point on the denoising trajectory. Differences in the diffusion-DPO loss:\nWe take a diffusion noise-prediction model pre-trained using the method above. We load one copy as a frozen ref_model and another as a trainable model. Instead of a single sample as before, each training sample is a preference pair (x_w, x_l). We noise both samples with identical noise to obtain noisy_x_w and noisy_x_l. We then pass the noised pair through both models to obtain 4 noise predictions: model_w_pred, model_l_pred, ref_w_pred and ref_l_pred. From these, we obtain 4 mean squared errors between predicted and actual noise: model_w_err, model_l_err, ref_w_err, and ref_l_err. We construct the diffusion-DPO loss by combining these errors. The overall aim of the diffusion-DPO loss is to push model towards generating samples more similar to x_w, and less similar to x_l. To do this, we define w_diff and l_diff as the differences between the model error and the reference error; intuitively, we want to to minimize w_diff while maximizing l_diff. These values are combined in inside_term with a scaling term beta, then passed through a log-sigmoid to generate a log-likelihood loss. The figure belows visualizes this: to minimize the loss, we want to maximize inside_term, which means reducing w_diff and increasing l_diff.\nHow the DPO loss varies with inside_term\nTo paraphrase Linkin Park’s “Numb”:\nI’m becoming this, all I want to do\nIs be more like x_w and be less like x_l\nCreate a training script We need to create a new DPO training script. We can base it on the previous glyffuser training script, with a couple of key differences:\nThe dataset class needs to load our preference pair data, rather than single images as before The training loss is now the diffusion-DPO loss discussed above. Train the model and assess the results We can start with the key training hyperparameters from the diffusion-DPO paper:\nA learning rate of 2000 (β = 2.048·10⁻⁸) is used with 25% linear warmup. The inverse scaling is motivated by the norm of the DPO objective gradient being proportional to β (the divergence penalty parameter) [33]. For both SD1.5 and SDXL, we find β ∈ [2000, 5000] to offer good performance (Supp. S5). We present main SD1.5 results with β = 2000 and SDXL results with β = 5000.\nS5 offers a little more information (recall that β is the coefficient for inside_term):\nFor β far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater, and the KL-divergence penalty greatly restricts any appreciable adaptation.\nSince our data distribution, preference dataset size and model complexity are all quite different from the paper, I started with runs around the same scale as the base model training—100 epochs of the 2,500 pair dataset (an overnight run on my 24GB 3090). An initial run at β = 5000 shows very little change:\nSamples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a ‘spot the difference’ game.\nLower β values give better results. The interactive figure below shows output of a training run at β = 1500 over 200 epochs.\nPlay\rTraining epoch: 0\rWhile there currently don’t exist any scoring models that can assess ‘glyph plausability’, the subjective quality change is very clear to me:\n12 or more of the 16 glyphs from reference model (epoch 0) have problems By 100 epochs, only around half of the glyphs have clear problems and the problems are less pronounced At 200 epochs, glyph construction is good but we lose a lot of detail, likely due to overfitting to the small preference set. Subjectively, I often rated simpler glyphs higher when generating the preference set as complex glyphs tended to contain more errors. At 200 epochs, training metrics were still improving (see figure below) and importantly the preference gap l_diff - w_diff was still increasing. In this case, we can’t rely on metrics and have to make a judgement on when to stop training. Here, I think that 130-150 epochs seems to strike a good balance of creativity and plausibility.\nTraining metrics for DPO alignment at β = 1500.\n","wordCount":"1680","inLanguage":"en","datePublished":"2025-05-29T00:00:00Z","dateModified":"2025-05-29T00:00:00Z","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/diffusion-dpo/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/index.html title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/bio/ title=Bio><span>Bio</span></a></li><li><a href=http://localhost:1313/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/research/ title=Research><span>Research</span></a></li><li><a href=http://localhost:1313/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Using human feedback to align a diffusion model with direct preference optimization (DPO): a step-by-step implementation
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2025-05-29 00:00:00 +0000 UTC'>May 29, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1680 words&nbsp;·&nbsp;Yue Wu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#background>Background</a></li><li><a href=#getting-human-feedback-data>Getting human feedback data</a></li><li><a href=#implementing-the-diffusion-dpo-loss>Implementing the diffusion-DPO loss</a></li><li><a href=#create-a-training-script>Create a training script</a></li><li><a href=#train-the-model-and-assess-the-results>Train the model and assess the results</a></li></ul></nav></div></details></div><div class=post-content><script>document.addEventListener("DOMContentLoaded",e=>{function t(e,t,n,s,o){const i=document.getElementById(e),d=document.getElementById(t),u=document.getElementById(n),c=document.getElementById(s);i.min=0,i.max=20,i.step=1,i.value=0;let a=!1,l;function r(e){const t=e===0?0:e*10-1,n=String(t).padStart(4,"0");u.textContent=`Training epoch: ${t}`,d.src=`/${o}_slider/epoch_${n}.png`}i.addEventListener("input",e=>{r(+e.target.value)}),c.addEventListener("click",()=>{a=!a,c.textContent=a?"Pause":"Play",a?h():clearInterval(l)});function h(){const e=+i.max;l=setInterval(()=>{let t=+i.value;t=(t+1)%(e+1),i.value=t,r(t)},200)}r(0)}t("parameterSlider1","outputImage1","sliderValue1","autoplayButton1","DPO")})</script><style>html,body{margin:0;padding:0;height:100%;overflow-x:hidden;box-sizing:border-box}*,*::before,*::after{box-sizing:inherit}.slider-container{max-width:80%;width:100%;margin:20px auto;display:flex;align-items:center;flex-wrap:nowrap;overflow:hidden}.parameterSlider{flex:1;margin-left:10px;min-width:0}.outputImage{display:block;margin:20px auto;max-width:100%}.sliderValue{min-width:100px;text-align:right;white-space:nowrap}.autoplayButton{min-width:60px;margin-right:10px;flex-shrink:0;outline:1px solid #000;outline-offset:-2px;background:#bbb}</style><p>In this post, I give an end-to-end example of using human feedback to align a diffusion model using <a href=https://arxiv.org/abs/2305.18290>direct preference optimization</a> (DPO). Working code is provided so you can follow along.</p><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>This post builds on my previous <a href=/posts/glyffuser/>glyffuser</a> project, where I build a diffusion model to generate novel Chinese-like glyphs. One problem was that many glyphs still disobey some rules of character construction and so look wrong to anyone who has some experience with Chinese characters. I&rsquo;ve highlighted some examples in the picture below.</p><figure class=align-center><img loading=lazy src=/real_fake_data_highlighted.png#center alt="Examples of glyffuser training data (left) and generated data (right); parts that &rsquo;look wrong&rsquo; are highlighted in red." width=80%><figcaption><p>Examples of glyffuser training data (left) and generated data (right); parts that &rsquo;look wrong&rsquo; are highlighted in red.</p></figcaption></figure><p>We&rsquo;ll gather preference data from a human &ldquo;expert&rdquo; (me), then use DPO to align the model to only produce highly plausible glyphs. A preview of the results below—I think the improvement should be clear, even to the untrained eye!<figure class=align-center><img loading=lazy src=/ref_vs_dpo.png#center alt="Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)" width=100%><figcaption><p>Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)</p></figcaption></figure></p><p>Here&rsquo;s what we&rsquo;ll do:</p><ol><li>Obtain a pretrained model</li><li>Get human feedback data</li><li>Implement the diffusion-DPO loss</li><li>Create a training script</li><li>Train the model and assess the results</li></ol><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>Using human feedback to align generative models is the key post-training &lsquo;secret sauce&rsquo; that turned text completion models into useful chatbots. OpenAI used reinforcement learning with human feedback (RLHF) to create <a href=https://openai.com/index/instruction-following/>InstructGPT</a> from the base GPT-3 model, paving the way for ChatGPT and the subsequent deluge of LLM-based chatbots.</p><p>A drawback to RLHF is that it is somewhat complicated to implement: a separate &ldquo;reward model&rdquo; needs to be trained on human feedback data, and then used to align the base model by assessing its outputs and adjusting its weights accordingly.</p><p><a href=https://arxiv.org/abs/2305.18290>Direct preference optimization</a> (DPO) is a streamlined reformulation of RLHF that has become a widely used alternative for human-feedback alignment in models such as Llama 3. Through mathematical wizardry, it dispenses with the need to train a reward model, instead allowing the model to be directly tuned on paired human preference data. Interestingly, DPO can also be used to <a href=https://arxiv.org/pdf/2311.12908>align diffusion models</a>.</p><h2 id=getting-human-feedback-data>Getting human feedback data<a hidden class=anchor aria-hidden=true href=#getting-human-feedback-data>#</a></h2><p>To use DPO, we need human feedback data in the form of pairs of samples where a human assesses one sample to be the winner and the other to be the loser. For the glyffuser, our samples are simply pairs of 128×128 pixel images generated by the model. The winning criterion was simply: &ldquo;which sample is a more plausible Chinese glyph?&rdquo;</p><p>In practical terms, I generated a 10,000 image set using the unconditional glyffuser model (you can test the repo <a href=https://github.com/yue-here/glyffuser>here</a>) and (since this is 2025), vibe coded a simple GUI for saving preference data. (Test it <a href=https://github.com/yue-here/glyph-chooser>here</a>)</p><figure class=align-center><img loading=lazy src=/glyph_chooser.png#center alt="A simple UI for generating a human preference dataset. For this pair, those familiar with Chinese characters would almost certainly prefer the example on the right." width=60%><figcaption><p>A simple UI for generating a human preference dataset. For this pair, those familiar with Chinese characters would almost certainly prefer the example on the right.</p></figcaption></figure><p>The key design thinking here was to use keyboard shortcuts to improve ergonomics. I included a skip function as well, to drop one sample where it wasn&rsquo;t clear which was better. Using the GUI, the process was fairly painless, taking me around 2 hours to label 10,000 pairs (while listening to an audiobook!).</p><p>The result was a json file with around 2,500 pairs (around 5,000 samples being dropped) structured in this way:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;better&#34;</span><span class=p>:</span> <span class=s2>&#34;image1.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;worse&#34;</span><span class=p>:</span> <span class=s2>&#34;image2.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;better_path&#34;</span><span class=p>:</span> <span class=s2>&#34;C:/path/to/image1.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;worse_path&#34;</span><span class=p>:</span> <span class=s2>&#34;C:/path/to/image2.jpg&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=implementing-the-diffusion-dpo-loss>Implementing the diffusion-DPO loss<a hidden class=anchor aria-hidden=true href=#implementing-the-diffusion-dpo-loss>#</a></h2><p>While the derivation of the diffusion-DPO loss is nontrivial, the implementation is fairly straightforward, as described in the pseudocode in section S9 of the <a href=https://arxiv.org/pdf/2311.12908>diffusion-DPO paper</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>loss</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>ref_model</span><span class=p>,</span> <span class=n>x_w</span><span class=p>,</span> <span class=n>x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>beta</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    # This is an example psuedo-code snippet for calculating the Diffusion-DPO loss
</span></span></span><span class=line><span class=cl><span class=s2>    # on a single image pair with corresponding caption
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    model: Diffusion model that accepts prompt conditioning c and time conditioning t
</span></span></span><span class=line><span class=cl><span class=s2>    ref_model: Frozen initialization of model
</span></span></span><span class=line><span class=cl><span class=s2>    x_w: Preferred Image (latents in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    x_l: Non-Preferred Image (latents in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    c: Conditioning (text in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    beta: Regularization Parameter
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    returns: DPO loss value
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>timestep</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>x_w</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noisy_x_w</span> <span class=o>=</span> <span class=n>add_noise</span><span class=p>(</span><span class=n>x_w</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noisy_x_l</span> <span class=o>=</span> <span class=n>add_noise</span><span class=p>(</span><span class=n>x_l</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_w_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>noisy_x_w</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_l_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>noisy_x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ref_w_pred</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>noisy_x_w</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_l_pred</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>noisy_x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_w_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_w_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_l_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_l_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ref_w_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>ref_w_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_l_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>ref_l_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>w_diff</span> <span class=o>=</span> <span class=n>model_w_err</span> <span class=o>-</span> <span class=n>ref_w_err</span>
</span></span><span class=line><span class=cl>    <span class=n>l_diff</span> <span class=o>=</span> <span class=n>model_l_err</span> <span class=o>-</span> <span class=n>ref_l_err</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>inside_term</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>*</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>w_diff</span> <span class=o>-</span> <span class=n>l_diff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>*</span> <span class=n>log</span><span class=p>(</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>inside_term</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></td></tr></table></div></div><p>We can almost paste directly into python—see the [notebook] for the exact details.
(Note: in the unconditional case, we can simply ignore <code>c</code>)</p><p>Below I&rsquo;ll discuss what&rsquo;s happening in this loss function; if you&rsquo;re mainly interested in the implementation, feel free to skip to the next section.</p><p>To interpret the DPO loss, we need to recall the standard diffusion training loop for the noise-prediction model (commonly a U-net), the core of a diffusion model. If you aren&rsquo;t familiar with this, I&rsquo;ve written a <a href=/posts/diffusion/>diffusion explainer</a> that goes through this topic clearly with minimal assumed knowledge.</p><p>To summarize, in the diffusion training loop we:</p><ul><li>Define a noising trajectory <em>via</em> timesteps, where <code>t = 0</code> is uncorrupted data and <code>t = 1000</code> (or another large number) is pure (pixel-wise) Gaussian noise</li><li>Take a random training example, and noise it to a random timestep <code>t</code>. This is efficient as Gaussian noise for any <code>t</code> can obtained in closed form.</li><li>Pass the noised example through the noise-prediction model to obtain predicted noise.</li><li>Update the weights of the noise-prediction model by minimizing the mean squared error loss between the predicted noise and actual noise, conditioned on the timestep.</li><li>Repeat. Over many samples and timesteps, the model learns to predict noise for the training data distribution at any given point on the denoising trajectory.</li></ul><p>Differences in the diffusion-DPO loss:</p><ul><li>We take a diffusion noise-prediction model pre-trained using the method above.</li><li>We load one copy as a frozen <code>ref_model</code> and another as a trainable <code>model</code>.</li><li>Instead of a single sample as before, each training sample is a preference pair (<code>x_w</code>, <code>x_l</code>).</li><li>We noise both samples with identical noise to obtain <code>noisy_x_w</code> and <code>noisy_x_l</code>.</li><li>We then pass the noised pair through both models to obtain 4 noise predictions: <code>model_w_pred</code>, <code>model_l_pred</code>, <code>ref_w_pred</code> and <code>ref_l_pred</code>.</li><li>From these, we obtain 4 mean squared errors between predicted and actual noise: <code>model_w_err</code>, <code>model_l_err</code>, <code>ref_w_err</code>, and <code>ref_l_err</code>.</li><li>We construct the diffusion-DPO loss by combining these errors.</li></ul><p>The overall aim of the diffusion-DPO loss is to push <code>model</code> towards generating samples more similar to <code>x_w</code>, and less similar to <code>x_l</code>. To do this, we define <code>w_diff</code> and <code>l_diff</code> as the differences between the model error and the reference error; intuitively, we want to to minimize <code>w_diff</code> while maximizing <code>l_diff</code>. These values are combined in <code>inside_term</code> with a scaling term <code>beta</code>, then passed through a log-sigmoid to generate a log-likelihood loss. The figure belows visualizes this: to minimize the loss, we want to maximize <code>inside_term</code>, which means reducing <code>w_diff</code> and increasing <code>l_diff</code>.</p><figure class=align-center><img loading=lazy src=/dpo_loss.png#center alt="How the DPO loss varies with inside_term" width=60%><figcaption><p>How the DPO loss varies with <code>inside_term</code></p></figcaption></figure><p>To paraphrase Linkin Park&rsquo;s &ldquo;Numb&rdquo;:<br></p><blockquote><p>I&rsquo;m becoming this, all I want to do<br>Is be more like <code>x_w</code> and be less like <code>x_l</code><br></p></blockquote><h2 id=create-a-training-script>Create a training script<a hidden class=anchor aria-hidden=true href=#create-a-training-script>#</a></h2><p>We need to create a new DPO training script. We can base it on the previous glyffuser training script, with a couple of key differences:</p><ul><li>The dataset class needs to load our preference pair data, rather than single images as before</li><li>The training loss is now the diffusion-DPO loss discussed above.</li></ul><h2 id=train-the-model-and-assess-the-results>Train the model and assess the results<a hidden class=anchor aria-hidden=true href=#train-the-model-and-assess-the-results>#</a></h2><p>We can start with the key training hyperparameters from the <a href=https://arxiv.org/pdf/2311.12908>diffusion-DPO paper</a>:</p><blockquote><p>A learning rate of 2000 (β = 2.048·10⁻⁸) is used with 25% linear warmup. The inverse scaling is motivated by the norm of the DPO objective gradient being proportional to β (the divergence penalty parameter) [33]. For both SD1.5 and SDXL, we find β ∈ [2000, 5000] to offer good performance (Supp. S5). We present main SD1.5 results with β = 2000 and SDXL results with β = 5000.</p></blockquote><p>S5 offers a little more information (recall that β is the coefficient for <code>inside_term</code>):</p><blockquote><p>For β far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater, and the KL-divergence penalty greatly restricts any appreciable adaptation.</p></blockquote><p>Since our data distribution, preference dataset size and model complexity are all quite different from the paper, I started with runs around the same scale as the base model training—100 epochs of the 2,500 pair dataset (an overnight run on my 24GB 3090). An initial run at β = 5000 shows very little change:</p><figure class=align-center><img loading=lazy src=/ref_vs_dpo_b5000.png#center alt="Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a &lsquo;spot the difference&rsquo; game." width=100%><figcaption><p>Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a &lsquo;spot the difference&rsquo; game.</p></figcaption></figure><p>Lower β values give better results. The interactive figure below shows output of a training run at β = 1500 over 200 epochs.</p><div class="slider-container unique-slider-container1"><button id=autoplayButton1 class=autoplayButton>Play</button>
<span id=sliderValue1 class=sliderValue>Training epoch: 0</span>
<input type=range id=parameterSlider1 class=parameterSlider min=0 max=20 step=1 value=0></div><center><img id=outputImage1 class=outputImage src=/DPO_slider/epoch_0000.png alt="DPO Model Output"></center><p>While there currently don&rsquo;t exist any scoring models that can assess &lsquo;glyph plausability&rsquo;, the subjective quality change is very clear to me:</p><ul><li>12 or more of the 16 glyphs from reference model (epoch 0) have problems</li><li>By 100 epochs, only around half of the glyphs have clear problems and the problems are less pronounced</li><li>At 200 epochs, glyph construction is good but we lose a lot of detail, likely due to overfitting to the small preference set. Subjectively, I often rated simpler glyphs higher when generating the preference set as complex glyphs tended to contain more errors.</li></ul><p>At 200 epochs, training metrics were still improving (see figure below) and importantly the preference gap <code>l_diff - w_diff</code> was still increasing. In this case, we can&rsquo;t rely on metrics and have to make a judgement on when to stop training. Here, I think that 130-150 epochs seems to strike a good balance of creativity and plausibility.</p><figure class=align-center><img loading=lazy src=/dpo_training_metrics.png#center alt="Training metrics for DPO alignment at β = 1500." width=100%><figcaption><p>Training metrics for DPO alignment at β = 1500.</p></figcaption></figure></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>Yue Wu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>