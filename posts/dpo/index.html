<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using human feedback to align a diffusion model | Yue Wu</title>
<meta name=keywords content><meta name=description content="


In this post, I give an end-to-end example of using human feedback data to align a diffusion model with direct preference optimization (DPO). If you&rsquo;d like to test it out or follow along with the code, have a look at the notebook in this repo."><meta name=author content="Yue Wu"><link rel=canonical href=https://yue-here.com/posts/dpo/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://yue-here.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yue-here.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yue-here.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yue-here.com/apple-touch-icon.png><link rel=mask-icon href=https://yue-here.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yue-here.com/posts/dpo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css integrity=sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js integrity=sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:url" content="https://yue-here.com/posts/dpo/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="Using human feedback to align a diffusion model"><meta property="og:description" content="In this post, I give an end-to-end example of using human feedback data to align a diffusion model with direct preference optimization (DPO). If you’d like to test it out or follow along with the code, have a look at the notebook in this repo."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-02T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using human feedback to align a diffusion model"><meta name=twitter:description content="


In this post, I give an end-to-end example of using human feedback data to align a diffusion model with direct preference optimization (DPO). If you&rsquo;d like to test it out or follow along with the code, have a look at the notebook in this repo."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yue-here.com/posts/"},{"@type":"ListItem","position":2,"name":"Using human feedback to align a diffusion model","item":"https://yue-here.com/posts/dpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using human feedback to align a diffusion model","name":"Using human feedback to align a diffusion model","description":"\rIn this post, I give an end-to-end example of using human feedback data to align a diffusion model with direct preference optimization (DPO). If you\u0026rsquo;d like to test it out or follow along with the code, have a look at the notebook in this repo.\n","keywords":[],"articleBody":"\rIn this post, I give an end-to-end example of using human feedback data to align a diffusion model with direct preference optimization (DPO). If you’d like to test it out or follow along with the code, have a look at the notebook in this repo.\nIntro This post builds on my previous glyffuser project, a diffusion model trained to generate novel Chinese-like glyphs. I encountered a common problem in generative AI: low consistency of good generations. While some samples are convincing, others look wrong to anyone who has some experience with Chinese characters. I’ve highlighted some examples in the figure below:\nExamples of glyffuser training data (left) and generated data (right); parts that ’look wrong’ are highlighted in red.\nThe problem of ‘glyph plausibility’ is difficult to specify computationally, but easy for a human to assess as Chinese characters tend to obey certain stroke order and structural rules. Thus, it presents an good opportunity for post-training model alignment. I’ll go through how to gather preference data from a human “expert” (me), then use DPO to align the model to only produce highly plausible glyphs. A preview of the results below—I think the improvement should be clear, even to the untrained eye! Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)\nBackground Using human feedback to align generative models is the key post-training ‘secret sauce’ that turned text completion models into useful chatbots. OpenAI developed the general-purpose reinforcement learning with human feedback (RLHF) method and used it to create InstructGPT from the base GPT-3 model, paving the way for ChatGPT and the subsequent deluge of LLM-based chatbots. RLHF can also be used to align diffusion models.\nA drawback to RLHF is that it is somewhat complicated to implement: a separate “reward model” needs to be trained on human feedback data, and then used to align the base model by assessing its outputs and adjusting its weights accordingly.\nDirect preference optimization (DPO) is a streamlined reformulation of RLHF that has become a widely used alternative for human-feedback alignment in models such as Llama 3. It dispenses with the need to train a reward model, instead allowing the model to be directly tuned on paired human preference data. Like RLHF, DPO can also be used to align diffusion models.\nGetting human feedback data To use DPO, we need human feedback data in the form of preference pairs: a pair of samples ranked by a human on some criterion. Here, our samples are pairs of 128×128 pixel images generated by the model. The criterion is simply “which sample is a more plausible Chinese glyph?”\nIn practical terms, I generated a 10,000 image set using the unconditional glyffuser model (repo, model weights) and (since this is 2025) vibe coded a simple GUI for saving preference data (repo):\nA simple GUI for generating a human preference dataset. Here, those familiar with Chinese characters would almost certainly prefer the example on the right.\nThe key design choice here was to use keyboard shortcuts to improve ergonomics. I included a skip function as well, to drop one sample where it wasn’t clear which was better. Using the GUI, the process was fairly painless, taking me around 2 hours to label 10,000 pairs (while listening to an audiobook!)\nThe result was a json file with around 2,500 pairs (around 5,000 samples being dropped) structured in this way:\n1 2 3 4 [{ \"better\": \"image1.jpg\", \"worse\": \"image2.jpg\", }] Implementing the diffusion-DPO loss While the derivation of the diffusion-DPO loss is nontrivial, the implementation is fairly straightforward, as described in the pseudocode in section S9 of the diffusion-DPO paper:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def loss(model, ref_model, x_w, x_l, c, beta): \"\"\" # This is an example psuedo-code snippet for calculating the Diffusion-DPO loss # on a single image pair with corresponding caption model: Diffusion model that accepts prompt conditioning c and time conditioning t ref_model: Frozen initialization of model x_w: Preferred Image (latents in this work) x_l: Non-Preferred Image (latents in this work) c: Conditioning (text in this work) beta: Regularization Parameter returns: DPO loss value \"\"\" timestep = torch.randint(0, 1000) noise = torch.randn_like(x_w) noisy_x_w = add_noise(x_w, noise, t) noisy_x_l = add_noise(x_l, noise, t) model_w_pred = model(noisy_x_w, c, t) model_l_pred = model(noisy_x_l, c, t) ref_w_pred = ref_model(noisy_x_w, c, t) ref_l_pred = ref_model(noisy_x_l, c, t) model_w_err = (model_w_pred - noise).norm().pow(2) model_l_err = (model_l_pred - noise).norm().pow(2) ref_w_err = (ref_w_pred - noise).norm().pow(2) ref_l_err = (ref_l_pred - noise).norm().pow(2) w_diff = model_w_err - ref_w_err l_diff = model_l_err - ref_l_err inside_term = -1 * beta * (w_diff - l_diff) loss = -1 * log(sigmoid(inside_term)) return loss We can almost paste this code directly into python—see the notebook section “DPO Loss Function” for the exact details. (Note: since we’re not using text conditioning, we can simply ignore c.)\nBelow I’ll discuss what’s happening in this loss function; if you’re mainly interested in the implementation, feel free to skip to the next section.\nTo interpret the DPO loss, we need to recall the standard diffusion training loop for the noise-prediction model (commonly a U-net), the core of a diffusion model. If you aren’t familiar with this, I’ve written a diffusion explainer that goes through this topic clearly with minimal assumed knowledge.\nTo summarize, in the diffusion training loop we:\nDefine a noising trajectory via timesteps, where t = 0 is uncorrupted data and t = 1000 is pure (pixel-wise) Gaussian noise. Take a random training example, and noise it to a random timestep t. This is efficient as Gaussian noise for any t can obtained in closed form. Pass the noised example through the noise-prediction model to obtain predicted noise. Update the weights of the noise-prediction model by minimizing the mean squared error (MSE) loss between the predicted noise and actual noise, conditioned on the timestep. Repeat. Over many samples and timesteps, the model learns to predict noise for the training data distribution at any given point on the denoising trajectory. Differences in the diffusion-DPO loss:\nWe start with a noise-prediction model pre-trained using the method above. We load one copy as a frozen ref_model and another as a trainable model. Instead of a single sample as before, each training sample is a preference pair (x_w, x_l). We noise both samples with identical noise to obtain noisy_x_w and noisy_x_l. We then pass the noised pair through both models to obtain 4 noise predictions: model_w_pred, model_l_pred, ref_w_pred and ref_l_pred. From these, we obtain 4 MSEs between predicted and actual noise: model_w_err, model_l_err, ref_w_err, and ref_l_err. We construct the diffusion-DPO loss by combining these errors. The overall aim of the diffusion-DPO loss is to push model to generate samples more similar to x_w, and less similar to x_l. To do this, we define w_diff and l_diff as the differences between the model error and the reference error. Intuitively, we want to to minimize w_diff while maximizing l_diff. These values are combined in inside_term with a scaling term beta, then passed through a log-sigmoid to generate a log-likelihood loss. The figure belows visualizes this: to minimize the loss, we want to maximize inside_term, which means reducing w_diff and increasing l_diff.\nHow the DPO loss varies with inside_term\nTo paraphrase Linkin Park’s “Numb”:\nI’m becoming this, all I want to do\nIs be more like x_w and be less like x_l\nCreating the training script We can base the diffusion-DPO training script the standard diffusion training script for the glyffuser with relatively small changes:\nSingle image data were previously loaded with a standard pytorch Dataset. Now we create a custom PreferenceDataset that reads the human feedback JSON created before, and returns an image preference pair. The training loss was previously a single MSE; we substitute in the diffusion-DPO loss discussed above. Train the model and assess the results We can start with the key training hyperparameters from the diffusion-DPO paper:\nA learning rate of 2000⁄β 2.048·10⁻⁸ is used with 25% linear warmup. The inverse scaling is motivated by the norm of the DPO objective gradient being proportional to β (the divergence penalty parameter) [33]. For both SD1.5 and SDXL, we find β ∈ [2000, 5000] to offer good performance (Supp. S5). We present main SD1.5 results with β = 2000 and SDXL results with β = 5000.\nS5 offers a little more information (recall that β is the coefficient for inside_term):\nFor β far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater, and the KL-divergence penalty greatly restricts any appreciable adaptation.\nSince our data distribution, preference dataset size and model complexity are all quite different from the paper, I started with runs around the same scale as the base model training—100 epochs of the 2,500 pair dataset (an overnight run on my RTX3090 GPU). An initial run at β = 5000 shows very little change:\nSamples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a ‘spot the difference’ game.\nFurther testing shows lower β values give better results. The interactive figure below shows output of a training run at β = 1500 over 200 epochs.\nPlay\rTraining epoch: 0\rWhile there currently don’t exist any scoring models that can assess ‘glyph plausibility’, the subjective quality change is very clear to me:\n12 or more of the 16 glyphs from the reference model (epoch 0) have problems By 100 epochs, only around half of the glyphs have clear problems and the problems are less pronounced At 200 epochs, glyph construction is good but we lose a lot of glyph complexity, likely due to overfitting to the small preference set. (I often favoured simpler glyphs when generating the preference set, as complex glyphs tended to contain more errors.) At 200 epochs, training metrics were still improving (see figure below) and importantly the preference gap l_diff - w_diff was still increasing. Looking at progression above, however, we can’t rely on metrics and have to make a judgement on when to stop training. Here, I think that 130-150 epochs seems to strike a good balance of creativity and plausibility.\nTraining metrics for DPO alignment at β = 1500 over 200 epochs.\nFurther remarks I hope that if you’re interested using human feedback to align your models but weren’t sure where to start, this has given you some ideas!\nI was surprised at how effective the model alignment was with a relatively small dataset. With the increasing interest in diffusion models (and other generative models) in the sciences, this might be an interesting avenue of research. Preference sets might be generated automatically from literature using agentic methods. A brief search reveals only a few papers in this area (example).\nDPO alignment also worked for the text-conditioned glyffuser model, but I’ve left that out of this post as I think it complicates things without changing the basic idea.\n","wordCount":"1840","inLanguage":"en","datePublished":"2025-06-02T00:00:00Z","dateModified":"2025-06-02T00:00:00Z","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yue-here.com/posts/dpo/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"https://yue-here.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yue-here.com/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yue-here.com/index.html title=Home><span>Home</span></a></li><li><a href=https://yue-here.com/bio/ title=Bio><span>Bio</span></a></li><li><a href=https://yue-here.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yue-here.com/research/ title=Research><span>Research</span></a></li><li><a href=https://yue-here.com/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Using human feedback to align a diffusion model</h1><div class=post-meta><span title='2025-06-02 00:00:00 +0000 UTC'>June 2, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1840 words&nbsp;·&nbsp;Yue Wu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#background>Background</a></li><li><a href=#getting-human-feedback-data>Getting human feedback data</a></li><li><a href=#implementing-the-diffusion-dpo-loss>Implementing the diffusion-DPO loss</a></li><li><a href=#creating-the-training-script>Creating the training script</a></li><li><a href=#train-the-model-and-assess-the-results>Train the model and assess the results</a></li><li><a href=#further-remarks>Further remarks</a></li></ul></nav></div></details></div><div class=post-content><script>document.addEventListener("DOMContentLoaded",e=>{function t(e,t,n,s,o){const i=document.getElementById(e),d=document.getElementById(t),u=document.getElementById(n),c=document.getElementById(s);i.min=0,i.max=20,i.step=1,i.value=0;let a=!1,l;function r(e){const t=e===0?0:e*10-1,n=String(t).padStart(4,"0");u.textContent=`Training epoch: ${t}`,d.src=`/${o}_slider/epoch_${n}.png`}i.addEventListener("input",e=>{r(+e.target.value)}),c.addEventListener("click",()=>{a=!a,c.textContent=a?"Pause":"Play",a?h():clearInterval(l)});function h(){const e=+i.max;l=setInterval(()=>{let t=+i.value;t=(t+1)%(e+1),i.value=t,r(t)},200)}r(0)}t("parameterSlider1","outputImage1","sliderValue1","autoplayButton1","DPO")})</script><style>html,body{margin:0;padding:0;height:100%;overflow-x:hidden;box-sizing:border-box}*,*::before,*::after{box-sizing:inherit}.slider-container{max-width:80%;width:100%;margin:20px auto;display:flex;align-items:center;flex-wrap:nowrap;overflow:hidden}.parameterSlider{flex:1;margin-left:10px;min-width:0}.outputImage{display:block;margin:20px auto;max-width:100%}.sliderValue{min-width:100px;text-align:right;white-space:nowrap}.autoplayButton{min-width:60px;margin-right:10px;flex-shrink:0;outline:1px solid #000;outline-offset:-2px;background:#bbb}</style><p>In this post, I give an end-to-end example of using human feedback data to align a diffusion model with <a href=https://arxiv.org/abs/2305.18290>direct preference optimization</a> (DPO). If you&rsquo;d like to test it out or follow along with the code, have a look at the notebook in <a href=https://github.com/yue-here/glyffuser-dpo>this repo</a>.</p><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>This post builds on my previous <a href=/posts/glyffuser/>glyffuser</a> project, a diffusion model trained to generate novel Chinese-like glyphs. I encountered a common problem in generative AI: low consistency of good generations. While some samples are convincing, others look wrong to anyone who has some experience with Chinese characters. I&rsquo;ve highlighted some examples in the figure below:</p><figure class=align-center><img loading=lazy src=/real_fake_data_highlighted.png#center alt="Examples of glyffuser training data (left) and generated data (right); parts that &rsquo;look wrong&rsquo; are highlighted in red." width=80%><figcaption><p>Examples of glyffuser training data (left) and generated data (right); parts that &rsquo;look wrong&rsquo; are highlighted in red.</p></figcaption></figure><p>The problem of &lsquo;glyph plausibility&rsquo; is difficult to specify computationally, but easy for a human to assess as Chinese characters tend to obey certain stroke order and structural rules. Thus, it presents an good opportunity for post-training model alignment. I&rsquo;ll go through how to gather preference data from a human &ldquo;expert&rdquo; (me), then use DPO to align the model to only produce highly plausible glyphs. A preview of the results below—I think the improvement should be clear, even to the untrained eye!<figure class=align-center><img loading=lazy src=/ref_vs_dpo.png#center alt="Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)" width=100%><figcaption><p>Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)</p></figcaption></figure></p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>Using human feedback to align generative models is the key post-training &lsquo;secret sauce&rsquo; that turned text completion models into useful chatbots. OpenAI developed the general-purpose <a href=https://arxiv.org/pdf/1706.03741>reinforcement learning with human feedback (RLHF)</a> method and used it to create <a href=https://openai.com/index/instruction-following/>InstructGPT</a> from the base GPT-3 model, paving the way for ChatGPT and the subsequent deluge of LLM-based chatbots. RLHF can also be used to <a href=https://arxiv.org/pdf/2305.13301>align diffusion models</a>.</p><p>A drawback to RLHF is that it is somewhat complicated to implement: a separate &ldquo;reward model&rdquo; needs to be trained on human feedback data, and then used to align the base model by assessing its outputs and adjusting its weights accordingly.</p><p><a href=https://arxiv.org/abs/2305.18290>Direct preference optimization (DPO)</a> is a streamlined reformulation of RLHF that has become a widely used alternative for human-feedback alignment in models such as Llama 3. It dispenses with the need to train a reward model, instead allowing the model to be directly tuned on paired human preference data. Like RLHF, DPO can also be used to <a href=https://arxiv.org/pdf/2311.12908>align diffusion models</a>.</p><h2 id=getting-human-feedback-data>Getting human feedback data<a hidden class=anchor aria-hidden=true href=#getting-human-feedback-data>#</a></h2><p>To use DPO, we need human feedback data in the form of preference pairs: a pair of samples ranked by a human on some criterion. Here, our samples are pairs of 128×128 pixel images generated by the model. The criterion is simply &ldquo;which sample is a more plausible Chinese glyph?&rdquo;</p><p>In practical terms, I generated a 10,000 image set using the unconditional glyffuser model (<a href=https://github.com/yue-here/glyffuser>repo</a>, <a href=https://huggingface.co/yuewu/glyffuser-unconditional>model weights</a>) and (since this is 2025) vibe coded a simple GUI for saving preference data (<a href=https://github.com/yue-here/glyph-chooser>repo</a>):</p><figure class=align-center><img loading=lazy src=/glyph_chooser.png#center alt="A simple GUI for generating a human preference dataset. Here, those familiar with Chinese characters would almost certainly prefer the example on the right." width=60%><figcaption><p>A simple GUI for generating a human preference dataset. Here, those familiar with Chinese characters would almost certainly prefer the example on the right.</p></figcaption></figure><p>The key design choice here was to use keyboard shortcuts to improve ergonomics. I included a skip function as well, to drop one sample where it wasn&rsquo;t clear which was better. Using the GUI, the process was fairly painless, taking me around 2 hours to label 10,000 pairs (while listening to an audiobook!)</p><p>The result was a json file with around 2,500 pairs (around 5,000 samples being dropped) structured in this way:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;better&#34;</span><span class=p>:</span> <span class=s2>&#34;image1.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;worse&#34;</span><span class=p>:</span> <span class=s2>&#34;image2.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=implementing-the-diffusion-dpo-loss>Implementing the diffusion-DPO loss<a hidden class=anchor aria-hidden=true href=#implementing-the-diffusion-dpo-loss>#</a></h2><p>While the derivation of the diffusion-DPO loss is nontrivial, the implementation is fairly straightforward, as described in the pseudocode in section S9 of the <a href=https://arxiv.org/pdf/2311.12908>diffusion-DPO paper</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>loss</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>ref_model</span><span class=p>,</span> <span class=n>x_w</span><span class=p>,</span> <span class=n>x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>beta</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    # This is an example psuedo-code snippet for calculating the Diffusion-DPO loss
</span></span></span><span class=line><span class=cl><span class=s2>    # on a single image pair with corresponding caption
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    model: Diffusion model that accepts prompt conditioning c and time conditioning t
</span></span></span><span class=line><span class=cl><span class=s2>    ref_model: Frozen initialization of model
</span></span></span><span class=line><span class=cl><span class=s2>    x_w: Preferred Image (latents in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    x_l: Non-Preferred Image (latents in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    c: Conditioning (text in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    beta: Regularization Parameter
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    returns: DPO loss value
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>timestep</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>x_w</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noisy_x_w</span> <span class=o>=</span> <span class=n>add_noise</span><span class=p>(</span><span class=n>x_w</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noisy_x_l</span> <span class=o>=</span> <span class=n>add_noise</span><span class=p>(</span><span class=n>x_l</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_w_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>noisy_x_w</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_l_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>noisy_x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ref_w_pred</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>noisy_x_w</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_l_pred</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>noisy_x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_w_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_w_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_l_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_l_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ref_w_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>ref_w_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_l_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>ref_l_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>w_diff</span> <span class=o>=</span> <span class=n>model_w_err</span> <span class=o>-</span> <span class=n>ref_w_err</span>
</span></span><span class=line><span class=cl>    <span class=n>l_diff</span> <span class=o>=</span> <span class=n>model_l_err</span> <span class=o>-</span> <span class=n>ref_l_err</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>inside_term</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>*</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>w_diff</span> <span class=o>-</span> <span class=n>l_diff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>*</span> <span class=n>log</span><span class=p>(</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>inside_term</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></td></tr></table></div></div><p>We can almost paste this code directly into python—see the <a href=https://github.com/yue-here/glyffuser-dpo/blob/main/glyffuser_dpo.ipynb>notebook</a> section &ldquo;DPO Loss Function&rdquo; for the exact details.
(Note: since we&rsquo;re not using text conditioning, we can simply ignore <code>c</code>.)</p><p>Below I&rsquo;ll discuss what&rsquo;s happening in this loss function; if you&rsquo;re mainly interested in the implementation, feel free to skip to the next section.</p><p>To interpret the DPO loss, we need to recall the standard diffusion training loop for the noise-prediction model (commonly a U-net), the core of a diffusion model. If you aren&rsquo;t familiar with this, I&rsquo;ve written a <a href=/posts/diffusion/>diffusion explainer</a> that goes through this topic clearly with minimal assumed knowledge.</p><p>To summarize, in the diffusion training loop we:</p><ul><li>Define a noising trajectory <em>via</em> timesteps, where <code>t = 0</code> is uncorrupted data and <code>t = 1000</code> is pure (pixel-wise) Gaussian noise.</li><li>Take a random training example, and noise it to a random timestep <code>t</code>. This is efficient as Gaussian noise for any <code>t</code> can obtained in closed form.</li><li>Pass the noised example through the noise-prediction model to obtain predicted noise.</li><li>Update the weights of the noise-prediction model by minimizing the mean squared error (MSE) loss between the predicted noise and actual noise, conditioned on the timestep.</li><li>Repeat. Over many samples and timesteps, the model learns to predict noise for the training data distribution at any given point on the denoising trajectory.</li></ul><p>Differences in the diffusion-DPO loss:</p><ul><li>We start with a noise-prediction model pre-trained using the method above.</li><li>We load one copy as a frozen <code>ref_model</code> and another as a trainable <code>model</code>.</li><li>Instead of a single sample as before, each training sample is a preference pair (<code>x_w</code>, <code>x_l</code>).</li><li>We noise both samples with identical noise to obtain <code>noisy_x_w</code> and <code>noisy_x_l</code>.</li><li>We then pass the noised pair through both models to obtain 4 noise predictions: <code>model_w_pred</code>, <code>model_l_pred</code>, <code>ref_w_pred</code> and <code>ref_l_pred</code>.</li><li>From these, we obtain 4 MSEs between predicted and actual noise: <code>model_w_err</code>, <code>model_l_err</code>, <code>ref_w_err</code>, and <code>ref_l_err</code>.</li><li>We construct the diffusion-DPO loss by combining these errors.</li></ul><p>The overall aim of the diffusion-DPO loss is to push <code>model</code> to generate samples more similar to <code>x_w</code>, and less similar to <code>x_l</code>. To do this, we define <code>w_diff</code> and <code>l_diff</code> as the differences between the model error and the reference error. Intuitively, we want to to minimize <code>w_diff</code> while maximizing <code>l_diff</code>. These values are combined in <code>inside_term</code> with a scaling term <code>beta</code>, then passed through a log-sigmoid to generate a log-likelihood loss. The figure belows visualizes this: to minimize the loss, we want to maximize <code>inside_term</code>, which means reducing <code>w_diff</code> and increasing <code>l_diff</code>.</p><figure class=align-center><img loading=lazy src=/dpo_loss.png#center alt="How the DPO loss varies with inside_term" width=60%><figcaption><p>How the DPO loss varies with <code>inside_term</code></p></figcaption></figure><p>To paraphrase Linkin Park&rsquo;s &ldquo;Numb&rdquo;:<br></p><blockquote><p>I&rsquo;m becoming this, all I want to do<br>Is be more like <code>x_w</code> and be less like <code>x_l</code><br></p></blockquote><h2 id=creating-the-training-script>Creating the training script<a hidden class=anchor aria-hidden=true href=#creating-the-training-script>#</a></h2><p>We can base the diffusion-DPO training script the standard diffusion training script for the glyffuser with relatively small changes:</p><ul><li>Single image data were previously loaded with a standard pytorch <code>Dataset</code>. Now we create a custom <code>PreferenceDataset</code> that reads the human feedback JSON created before, and returns an image preference pair.</li><li>The training loss was previously a single MSE; we substitute in the diffusion-DPO loss discussed above.</li></ul><h2 id=train-the-model-and-assess-the-results>Train the model and assess the results<a hidden class=anchor aria-hidden=true href=#train-the-model-and-assess-the-results>#</a></h2><p>We can start with the key training hyperparameters from the <a href=https://arxiv.org/pdf/2311.12908>diffusion-DPO paper</a>:</p><blockquote><p>A learning rate of <sup>2000</sup>⁄<sub>β</sub> 2.048·10⁻⁸ is used with 25% linear warmup. The inverse scaling is motivated by the norm of the DPO objective gradient being proportional to β (the divergence penalty parameter) [33]. For both SD1.5 and SDXL, we find β ∈ [2000, 5000] to offer good performance (Supp. S5). We present main SD1.5 results with β = 2000 and SDXL results with β = 5000.</p></blockquote><p>S5 offers a little more information (recall that β is the coefficient for <code>inside_term</code>):</p><blockquote><p>For β far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater, and the KL-divergence penalty greatly restricts any appreciable adaptation.</p></blockquote><p>Since our data distribution, preference dataset size and model complexity are all quite different from the paper, I started with runs around the same scale as the base model training—100 epochs of the 2,500 pair dataset (an overnight run on my RTX3090 GPU). An initial run at β = 5000 shows very little change:</p><figure class=align-center><img loading=lazy src=/ref_vs_dpo_b5000.png#center alt="Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a &lsquo;spot the difference&rsquo; game." width=100%><figcaption><p>Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a &lsquo;spot the difference&rsquo; game.</p></figcaption></figure><p>Further testing shows lower β values give better results. The interactive figure below shows output of a training run at β = 1500 over 200 epochs.</p><div class="slider-container unique-slider-container1"><button id=autoplayButton1 class=autoplayButton>Play</button>
<span id=sliderValue1 class=sliderValue>Training epoch: 0</span>
<input type=range id=parameterSlider1 class=parameterSlider min=0 max=20 step=1 value=0></div><center><img id=outputImage1 class=outputImage src=/DPO_slider/epoch_0000.png alt="DPO Model Output"></center><p>While there currently don&rsquo;t exist any scoring models that can assess &lsquo;glyph plausibility&rsquo;, the subjective quality change is very clear to me:</p><ul><li>12 or more of the 16 glyphs from the reference model (epoch 0) have problems</li><li>By 100 epochs, only around half of the glyphs have clear problems and the problems are less pronounced</li><li>At 200 epochs, glyph construction is good but we lose a lot of glyph complexity, likely due to overfitting to the small preference set. (I often favoured simpler glyphs when generating the preference set, as complex glyphs tended to contain more errors.)</li></ul><p>At 200 epochs, training metrics were still improving (see figure below) and importantly the preference gap <code>l_diff - w_diff</code> was still increasing. Looking at progression above, however, we can&rsquo;t rely on metrics and have to make a judgement on when to stop training. Here, I think that 130-150 epochs seems to strike a good balance of creativity and plausibility.</p><figure class=align-center><img loading=lazy src=/dpo_training_metrics.png#center alt="Training metrics for DPO alignment at β = 1500 over 200 epochs." width=100%><figcaption><p>Training metrics for DPO alignment at β = 1500 over 200 epochs.</p></figcaption></figure><h2 id=further-remarks>Further remarks<a hidden class=anchor aria-hidden=true href=#further-remarks>#</a></h2><p>I hope that if you&rsquo;re interested using human feedback to align your models but weren&rsquo;t sure where to start, this has given you some ideas!</p><p>I was surprised at how effective the model alignment was with a relatively small dataset. With the increasing interest in diffusion models (and other generative models) in the sciences, this might be an interesting avenue of research. Preference sets might be generated automatically from literature using agentic methods. A brief search reveals only a few papers in this area (<a href=https://arxiv.org/pdf/2310.12304>example</a>).</p><p>DPO alignment also worked for the text-conditioned glyffuser model, but I&rsquo;ve left that out of this post as I think it complicates things without changing the basic idea.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://yue-here.com/posts/diffusion/><span class=title>Next »</span><br><span>A visual guide to how diffusion models work</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yue-here.com/>Yue Wu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>