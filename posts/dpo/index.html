<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Using human feedback to align a diffusion model with DPO | Yue Wu</title>
<meta name=keywords content><meta name=description content="


This article is aimed at those who want to do post-training alignment of generative models, but aren&rsquo;t sure where to start. I give a end-to-end example of using human feedback data to align a generative model with direct preference optimization (DPO). I cover these steps:"><meta name=author content="Yue Wu"><link rel=canonical href=https://yue-here.com/posts/dpo/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://yue-here.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yue-here.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yue-here.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yue-here.com/apple-touch-icon.png><link rel=mask-icon href=https://yue-here.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yue-here.com/posts/dpo/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css integrity=sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js integrity=sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:url" content="https://yue-here.com/posts/dpo/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="Using human feedback to align a diffusion model with DPO"><meta property="og:description" content="This article is aimed at those who want to do post-training alignment of generative models, but aren’t sure where to start. I give a end-to-end example of using human feedback data to align a generative model with direct preference optimization (DPO). I cover these steps:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-02T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-02T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Using human feedback to align a diffusion model with DPO"><meta name=twitter:description content="


This article is aimed at those who want to do post-training alignment of generative models, but aren&rsquo;t sure where to start. I give a end-to-end example of using human feedback data to align a generative model with direct preference optimization (DPO). I cover these steps:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yue-here.com/posts/"},{"@type":"ListItem","position":2,"name":"Using human feedback to align a diffusion model with DPO","item":"https://yue-here.com/posts/dpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Using human feedback to align a diffusion model with DPO","name":"Using human feedback to align a diffusion model with DPO","description":"\rThis article is aimed at those who want to do post-training alignment of generative models, but aren\u0026rsquo;t sure where to start. I give a end-to-end example of using human feedback data to align a generative model with direct preference optimization (DPO). I cover these steps:\n","keywords":[],"articleBody":"\rThis article is aimed at those who want to do post-training alignment of generative models, but aren’t sure where to start. I give a end-to-end example of using human feedback data to align a generative model with direct preference optimization (DPO). I cover these steps:\nCreating a preference data set from the output of a previously trained model Implementing the DPO loss in pytorch DPO-training the model and assessing the outcome Approaching model alignment can initially seem daunting as there a lot of moving parts, but the implementation is surprisingly straightforward. If you’d like to test it out or follow along with the code, have a look at the notebook in this repo.\nIntro This post builds on my previous glyffuser project, a diffusion model trained to generate novel Chinese-like glyphs. I encountered a common problem in generative AI: low consistency of good generations. While some samples are convincing, others look wrong to anyone who has some experience with Chinese characters. I’ve highlighted some examples in the figure below:\nExamples of glyffuser training data (left) and generated data (right); parts that ’look wrong’ are highlighted in red.\nThe problem of ‘glyph plausibility’ is highly suited to post-training model alignment: it is difficult to specify as a training objective directly, but easy for a human to assess as Chinese characters tend to obey certain stroke order and structural rules.\nI’ll go through how to gather preference data from a human “expert” (me), then use DPO to align the model to only produce highly plausible glyphs. A preview of the results below—I think the improvement should be clear, even to the untrained eye! Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)\nBackground Broadly speaking, generative models trained on large amounts of data learn underlying patterns in that data, but don’t necessarily behave in a useful way. For example, large language models will simply use the most probable patterns they have learnt to complete a given text sequence. OpenAI’s InstructGPT article gives this example output from GPT-3:\n1 2 Prompt: Explain the moon landing to a 6 year old in a few sentences. Completion: Explain the theory of gravity to a 6 year old. Using human feedback to align generative models is the key post-training ‘secret sauce’ that turned text completion models into useful chatbots. OpenAI developed the general-purpose reinforcement learning with human feedback (RLHF) method and used it to create InstructGPT from the base GPT-3 model, paving the way for ChatGPT and the subsequent deluge of LLM-based chatbots.\nIn a typical training sequence, models are first pre-trained on huge datasets. Then, a small amount of high-quality example data are used to warm-start the model alignment with supervised finetuning (SFT). This data might be explicit examples of what response would be expected from a model given a prompt, and is very expensive to obtain as it is human-generated. A larger amount of preference data can then be generated by asking humans to rank machine-generated examples, which is significantly cheaper but cannot be used for SFT directly. Reinforcement learning methods such as PPO (proximal policy optimization) are one method of utilizing this preference data to further align the model. RLHF can also be used to align diffusion models in a similar way.\nA drawback to RLHF is that it is complicated and expensive to implement: a separate “reward model” needs to be trained on human feedback data, and then used to align the base model by assessing its outputs and adjusting its weights accordingly. Libraries like Hugging Face’s transformer reinforcement learning (TRL) have been written to abstract away some of this complexity.\nDirect preference optimization (DPO) is a streamlined reformulation of RLHF that has become a widely used alternative for human-feedback alignment in models such as Llama 3. It dispenses with the need to train a reward model, instead allowing the model to be directly tuned on paired human preference data. Like RLHF, DPO (in a formulation termed diffusion-DPO) can also be used to align diffusion models.\nGetting human feedback data To use DPO, we need human feedback data in the form of preference pairs: a pair of samples ranked by a human on some criterion. Here, our samples are pairs of 128×128 pixel images generated by the model. Since we hope to improve glyph plausibility, the criterion is simply “which sample is a more plausible Chinese glyph?”\nAside: for more general image generation models, I’m not aware of any literature on aligning the unconditional base model. I suspect unconditional alignment wouldn’t be as helpful for these models, as the space of possible generations is much larger than the model we’re using here and so would require a much larger preference dataset to effectively align. In the diffusion-DPO paper for example, text-to-image prompt adherence is the alignment criterion. I also tested prompt adherence alignment with the text-to-glyph glyffuser model, which I may discuss in a later post.\nIn practical terms, I generated a 10,000 image set using the unconditional glyffuser model (repo, model weights) and vibe coded a simple GUI for saving preference data (repo):\nA simple GUI for generating a human preference dataset. For the example shown, those familiar with Chinese characters would almost certainly prefer the example on the right.\nThe key design choice here was to use keyboard shortcuts to improve ergonomics. I included a skip function as well, to drop one sample where it wasn’t clear which was better. Using the GUI, the process was fairly painless, taking me around 2 hours to label 10,000 pairs (while listening to an audiobook!)\nThe result was a json file with around 2,500 pairs (around 5,000 samples being dropped) structured in this way:\n1 2 3 4 [{ \"better\": \"image1.jpg\", \"worse\": \"image2.jpg\", }] Implementing the diffusion-DPO loss While the derivation of the diffusion-DPO loss is nontrivial, the implementation is fairly straightforward, as described in the pseudocode in section S9 of the diffusion-DPO paper:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def loss(model, ref_model, x_w, x_l, c, beta): \"\"\" # This is an example psuedo-code snippet for calculating the Diffusion-DPO loss # on a single image pair with corresponding caption model: Diffusion model that accepts prompt conditioning c and time conditioning t ref_model: Frozen initialization of model x_w: Preferred Image (latents in this work) x_l: Non-Preferred Image (latents in this work) c: Conditioning (text in this work) beta: Regularization Parameter returns: DPO loss value \"\"\" timestep = torch.randint(0, 1000) noise = torch.randn_like(x_w) noisy_x_w = add_noise(x_w, noise, t) noisy_x_l = add_noise(x_l, noise, t) model_w_pred = model(noisy_x_w, c, t) model_l_pred = model(noisy_x_l, c, t) ref_w_pred = ref_model(noisy_x_w, c, t) ref_l_pred = ref_model(noisy_x_l, c, t) model_w_err = (model_w_pred - noise).norm().pow(2) model_l_err = (model_l_pred - noise).norm().pow(2) ref_w_err = (ref_w_pred - noise).norm().pow(2) ref_l_err = (ref_l_pred - noise).norm().pow(2) w_diff = model_w_err - ref_w_err l_diff = model_l_err - ref_l_err inside_term = -1 * beta * (w_diff - l_diff) loss = -1 * log(sigmoid(inside_term)) return loss We can almost paste this code directly into python—see the “DPO Loss Function” section in the training notebook for the exact details. (Note: since we’re not using text conditioning, we can simply ignore c.)\nBelow I’ll discuss what’s happening in this loss function; if you’re mainly interested in the implementation, feel free to skip to the next section.\nTo interpret the DPO loss, we need to recall the standard diffusion training loop for the noise-prediction model (commonly a U-net), the core of a diffusion model. If you aren’t familiar with this, I’ve written a diffusion explainer that goes through this topic clearly with minimal assumed knowledge.\nTo summarize, in the diffusion training loop we:\nDefine a noising trajectory from pure data to pure (pixel-wise) Gaussian noise. Commonly the trajectory is divided into 1000 timesteps, where t = 0 is uncorrupted data and t = 1000 is pure noise. Take a random training example, and noise it to a random timestep t. We can do this efficiently as the Gaussian noise for any t can be obtained in closed form. Pass the noised example through the noise-prediction model to obtain predicted noise. Update the weights of the noise-prediction model by minimizing the mean squared error (MSE) loss between the predicted noise and actual noise, conditioned on the timestep. Repeat. Over many samples and timesteps, the model learns to predict noise for the training data distribution at any given point on the denoising trajectory. The diffusion-DPO loss has the following differences:\nWe start with a noise-prediction model already pre-trained using the method above. We load one copy of the pre-trained model as a frozen ref_model and another as a trainable model. Instead of a single sample, each training sample is a preference pair (x_w, x_l). We noise both samples with identical noise to obtain noisy_x_w and noisy_x_l. We then pass the noised pair through both models to obtain 4 noise predictions: model_w_pred, model_l_pred, ref_w_pred and ref_l_pred. From these, we obtain 4 MSEs between predicted and actual noise: model_w_err, model_l_err, ref_w_err, and ref_l_err. We construct the diffusion-DPO loss by combining these errors. The overall aim of the diffusion-DPO loss is to push model to generate samples more similar to x_w, and less similar to x_l. To do this, we define w_diff and l_diff as the differences between the model error and the reference error. Intuitively, we want to to minimize w_diff while maximizing l_diff. These values are combined in inside_term with a scaling term beta, then passed through a log-sigmoid to generate a log-likelihood loss. The figure belows visualizes this: to minimize the loss, we want to maximize inside_term, which means reducing w_diff and increasing l_diff.\nHow the DPO loss varies with inside_term\nTo paraphrase Linkin Park’s “Numb”:\nI’m becoming this, all I want to do\nIs be more like x_w and be less like x_l\nCreating the training script We can base the diffusion-DPO training script the standard diffusion training script for the glyffuser with relatively small changes:\nSingle image data were previously loaded with a standard pytorch Dataset. Now we create a PreferenceDataset subclass that reads the human feedback JSON created before, and returns an image preference pair. (“Preference Dataset” in the training notebook) The training loss was previously a single MSE; we substitute in the diffusion-DPO loss discussed above. Training the model and assessing the results We can start with the key training hyperparameters from the diffusion-DPO paper:\nA learning rate of 2000⁄β 2.048·10⁻⁸ is used with 25% linear warmup. The inverse scaling is motivated by the norm of the DPO objective gradient being proportional to β (the divergence penalty parameter) [33]. For both SD1.5 and SDXL, we find β ∈ [2000, 5000] to offer good performance (Supp. S5). We present main SD1.5 results with β = 2000 and SDXL results with β = 5000.\nS5 offers a little more information (recall that β is the coefficient for inside_term):\nFor β far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater, and the KL-divergence penalty greatly restricts any appreciable adaptation.\nSince our data distribution, preference dataset size and model complexity are all quite different from the paper, I started with runs around the same scale as the base model training—100 epochs of the 2,500 pair dataset. The maximum batch size is much smaller as the VRAM needs to hold an additional (frozen) model and 4 sets of activations rather than 1.\nAn initial run at β = 5000 shows very little change: Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a ‘spot the difference’ game.\nFurther testing shows the importance of the β value. The interactive figure below shows how the output of the model changes as it’s trained with β = 1500 over 200 epochs.\nPlay\rTraining epoch: 0\rWhile there currently don’t exist any scoring models that can assess ‘glyph plausibility’, the subjective quality change is very clear to me:\n12 or more of the 16 glyphs from the reference model (epoch 0) have problems By 100 epochs, less than half of the glyphs have clear problems and the problems are less pronounced At 200 epochs, we lose a lot of glyph complexity, likely due to overfitting to the small preference set. (I often favoured simpler glyphs when generating the preference set, as complex glyphs tended to contain more errors.) At 200 epochs, training metrics were still improving (see figure below) and importantly the preference gap l_diff - w_diff was still increasing. Looking at the progression above, however, we can’t rely on metrics and have to make a judgement on when to stop training. Here, I think that 130-150 epochs seems to strike a good balance of creativity and plausibility.\nTraining metrics for DPO alignment at β = 1500 over 200 epochs.\nFurther remarks I hope that if you’re interested using human feedback to align your models but weren’t sure where to start, this has given you some ideas!\nI was surprised at how effective the model alignment was with a relatively small dataset. With the increasing interest in diffusion models (and other generative models) in the sciences, this might be an interesting avenue of research. Preference sets might be generated automatically from literature using agentic methods. A brief search reveals only a few papers in this area (example).\nDPO alignment also worked for the text-conditioned glyffuser model, but I’ve left that out of this post as I think it complicates things without changing the basic idea.\n","wordCount":"2272","inLanguage":"en","datePublished":"2025-06-02T00:00:00Z","dateModified":"2025-06-02T00:00:00Z","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yue-here.com/posts/dpo/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"https://yue-here.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yue-here.com/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yue-here.com/index.html title=Home><span>Home</span></a></li><li><a href=https://yue-here.com/bio/ title=Bio><span>Bio</span></a></li><li><a href=https://yue-here.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yue-here.com/research/ title=Research><span>Research</span></a></li><li><a href=https://yue-here.com/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Using human feedback to align a diffusion model with DPO</h1><div class=post-meta><span title='2025-06-02 00:00:00 +0000 UTC'>June 2, 2025</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;2272 words&nbsp;·&nbsp;Yue Wu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#background>Background</a></li><li><a href=#getting-human-feedback-data>Getting human feedback data</a></li><li><a href=#implementing-the-diffusion-dpo-loss>Implementing the diffusion-DPO loss</a></li><li><a href=#creating-the-training-script>Creating the training script</a></li><li><a href=#training-the-model-and-assessing-the-results>Training the model and assessing the results</a></li><li><a href=#further-remarks>Further remarks</a></li></ul></nav></div></details></div><div class=post-content><script>document.addEventListener("DOMContentLoaded",e=>{function t(e,t,n,s,o){const i=document.getElementById(e),d=document.getElementById(t),u=document.getElementById(n),c=document.getElementById(s);i.min=0,i.max=199,i.step=1,i.value=0;let a=!1,l;function r(e){const t=String(e).padStart(4,"0");u.textContent=`Training epoch: ${e}`,d.src=`/${o}_slider/${t}.png`}i.addEventListener("input",e=>{r(+e.target.value)}),c.addEventListener("click",()=>{a=!a,c.textContent=a?"Pause":"Play",a?h():clearInterval(l)});function h(){const e=+i.max;l=setInterval(()=>{let t=+i.value;t=(t+1)%(e+1),i.value=t,r(t)},20)}r(0)}t("parameterSlider1","outputImage1","sliderValue1","autoplayButton1","DPO")})</script><style>html,body{margin:0;padding:0;height:100%;overflow-x:hidden;box-sizing:border-box}*,*::before,*::after{box-sizing:inherit}.slider-container{max-width:100%;width:100%;margin:20px auto;display:flex;align-items:center;flex-wrap:nowrap;overflow:hidden}.parameterSlider{flex:1;margin-left:10px;min-width:0}.outputImage{display:block;margin:20px auto;max-width:100%}.sliderValue{min-width:100px;text-align:right;white-space:nowrap}.autoplayButton{min-width:60px;margin-right:10px;flex-shrink:0;outline:1px solid #000;outline-offset:-2px;background:#bbb}</style><p>This article is aimed at those who want to do post-training alignment of generative models, but aren&rsquo;t sure where to start. I give a end-to-end example of using human feedback data to align a generative model with <a href=https://arxiv.org/abs/2305.18290>direct preference optimization</a> (DPO). I cover these steps:</p><ul><li>Creating a preference data set from the output of a previously trained model</li><li>Implementing the DPO loss in pytorch</li><li>DPO-training the model and assessing the outcome</li></ul><p>Approaching model alignment can initially seem daunting as there a lot of moving parts, but the implementation is surprisingly straightforward. If you&rsquo;d like to test it out or follow along with the code, have a look at the notebook in <a href=https://github.com/yue-here/glyffuser-dpo>this repo</a>.</p><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>This post builds on my previous <a href=/posts/glyffuser/>glyffuser</a> project, a diffusion model trained to generate novel Chinese-like glyphs. I encountered a common problem in generative AI: low consistency of good generations. While some samples are convincing, others look wrong to anyone who has some experience with Chinese characters. I&rsquo;ve highlighted some examples in the figure below:</p><figure class=align-center><img loading=lazy src=/real_fake_data_highlighted.png#center alt="Examples of glyffuser training data (left) and generated data (right); parts that &rsquo;look wrong&rsquo; are highlighted in red." width=80%><figcaption><p>Examples of glyffuser training data (left) and generated data (right); parts that &rsquo;look wrong&rsquo; are highlighted in red.</p></figcaption></figure><p>The problem of &lsquo;glyph plausibility&rsquo; is highly suited to post-training model alignment: it is difficult to specify as a training objective directly, but easy for a human to assess as Chinese characters tend to obey certain stroke order and structural rules.</p><p>I&rsquo;ll go through how to gather preference data from a human &ldquo;expert&rdquo; (me), then use DPO to align the model to only produce highly plausible glyphs. A preview of the results below—I think the improvement should be clear, even to the untrained eye!<figure class=align-center><img loading=lazy src=/ref_vs_dpo.png#center alt="Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)" width=100%><figcaption><p>Samples generated by the reference glyffuser model (top) and the DPO-aligned model (below)</p></figcaption></figure></p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>Broadly speaking, generative models trained on large amounts of data learn underlying patterns in that data, but don&rsquo;t necessarily behave in a useful way. For example, large language models will simply use the most probable patterns they have learnt to complete a given text sequence. OpenAI&rsquo;s <a href=https://openai.com/index/instruction-following/>InstructGPT</a> article gives this example output from GPT-3:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Prompt: Explain the moon landing to a 6 year old in a few sentences.
</span></span><span class=line><span class=cl>Completion: Explain the theory of gravity to a 6 year old.
</span></span></code></pre></td></tr></table></div></div><p>Using human feedback to align generative models is the key post-training &lsquo;secret sauce&rsquo; that turned text completion models into useful chatbots. OpenAI developed the general-purpose <a href=https://arxiv.org/pdf/1706.03741>reinforcement learning with human feedback (RLHF)</a> method and used it to create <a href=https://openai.com/index/instruction-following/>InstructGPT</a> from the base GPT-3 model, paving the way for ChatGPT and the subsequent deluge of LLM-based chatbots.</p><p>In a typical training sequence, models are first pre-trained on huge datasets. Then, a small amount of high-quality example data are used to warm-start the model alignment with supervised finetuning (SFT). This data might be explicit examples of what response would be expected from a model given a prompt, and is very expensive to obtain as it is human-generated. A larger amount of preference data can then be generated by asking humans to rank machine-generated examples, which is significantly cheaper but cannot be used for SFT directly. Reinforcement learning methods such as <a href=https://openai.com/index/openai-baselines-ppo/>PPO</a> (proximal policy optimization) are one method of utilizing this preference data to further align the model. RLHF can also be used to <a href=https://arxiv.org/pdf/2305.13301>align diffusion models</a> in a similar way.</p><p>A drawback to RLHF is that it is complicated and expensive to implement: a separate &ldquo;reward model&rdquo; needs to be trained on human feedback data, and then used to align the base model by assessing its outputs and adjusting its weights accordingly. Libraries like Hugging Face&rsquo;s <a href=https://huggingface.co/docs/trl/en/index>transformer reinforcement learning (TRL)</a> have been written to abstract away some of this complexity.</p><p><a href=https://arxiv.org/abs/2305.18290>Direct preference optimization (DPO)</a> is a streamlined reformulation of RLHF that has become a widely used alternative for human-feedback alignment in models such as Llama 3. It dispenses with the need to train a reward model, instead allowing the model to be directly tuned on paired human preference data. Like RLHF, DPO (in a formulation termed diffusion-DPO) can also be used to <a href=https://arxiv.org/pdf/2311.12908>align diffusion models</a>.</p><h2 id=getting-human-feedback-data>Getting human feedback data<a hidden class=anchor aria-hidden=true href=#getting-human-feedback-data>#</a></h2><p>To use DPO, we need human feedback data in the form of preference pairs: a pair of samples ranked by a human on some criterion. Here, our samples are pairs of 128×128 pixel images generated by the model. Since we hope to improve glyph plausibility, the criterion is simply &ldquo;which sample is a more plausible Chinese glyph?&rdquo;</p><p>Aside: for more general image generation models, I&rsquo;m not aware of any literature on aligning the unconditional base model. I suspect unconditional alignment wouldn&rsquo;t be as helpful for these models, as the space of possible generations is much larger than the model we&rsquo;re using here and so would require a much larger preference dataset to effectively align. In the diffusion-DPO paper for example, text-to-image prompt adherence is the alignment criterion. I also tested prompt adherence alignment with the text-to-glyph glyffuser model, which I may discuss in a later post.</p><p>In practical terms, I generated a 10,000 image set using the unconditional glyffuser model (<a href=https://github.com/yue-here/glyffuser>repo</a>, <a href=https://huggingface.co/yuewu/glyffuser-unconditional>model weights</a>) and vibe coded a simple GUI for saving preference data (<a href=https://github.com/yue-here/glyph-chooser>repo</a>):</p><figure class=align-center><img loading=lazy src=/glyph_chooser.png#center alt="A simple GUI for generating a human preference dataset. For the example shown, those familiar with Chinese characters would almost certainly prefer the example on the right." width=60%><figcaption><p>A simple GUI for generating a human preference dataset. For the example shown, those familiar with Chinese characters would almost certainly prefer the example on the right.</p></figcaption></figure><p>The key design choice here was to use keyboard shortcuts to improve ergonomics. I included a skip function as well, to drop one sample where it wasn&rsquo;t clear which was better. Using the GUI, the process was fairly painless, taking me around 2 hours to label 10,000 pairs (while listening to an audiobook!)</p><p>The result was a json file with around 2,500 pairs (around 5,000 samples being dropped) structured in this way:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>[{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;better&#34;</span><span class=p>:</span> <span class=s2>&#34;image1.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;worse&#34;</span><span class=p>:</span> <span class=s2>&#34;image2.jpg&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}]</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=implementing-the-diffusion-dpo-loss>Implementing the diffusion-DPO loss<a hidden class=anchor aria-hidden=true href=#implementing-the-diffusion-dpo-loss>#</a></h2><p>While the derivation of the diffusion-DPO loss is nontrivial, the implementation is fairly straightforward, as described in the pseudocode in section S9 of the <a href=https://arxiv.org/pdf/2311.12908>diffusion-DPO paper</a>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>loss</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>ref_model</span><span class=p>,</span> <span class=n>x_w</span><span class=p>,</span> <span class=n>x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>beta</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    # This is an example psuedo-code snippet for calculating the Diffusion-DPO loss
</span></span></span><span class=line><span class=cl><span class=s2>    # on a single image pair with corresponding caption
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    model: Diffusion model that accepts prompt conditioning c and time conditioning t
</span></span></span><span class=line><span class=cl><span class=s2>    ref_model: Frozen initialization of model
</span></span></span><span class=line><span class=cl><span class=s2>    x_w: Preferred Image (latents in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    x_l: Non-Preferred Image (latents in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    c: Conditioning (text in this work)
</span></span></span><span class=line><span class=cl><span class=s2>    beta: Regularization Parameter
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    returns: DPO loss value
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>timestep</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noise</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn_like</span><span class=p>(</span><span class=n>x_w</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noisy_x_w</span> <span class=o>=</span> <span class=n>add_noise</span><span class=p>(</span><span class=n>x_w</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>noisy_x_l</span> <span class=o>=</span> <span class=n>add_noise</span><span class=p>(</span><span class=n>x_l</span><span class=p>,</span> <span class=n>noise</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_w_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>noisy_x_w</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_l_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>noisy_x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ref_w_pred</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>noisy_x_w</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_l_pred</span> <span class=o>=</span> <span class=n>ref_model</span><span class=p>(</span><span class=n>noisy_x_l</span><span class=p>,</span> <span class=n>c</span><span class=p>,</span> <span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_w_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_w_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model_l_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>model_l_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>ref_w_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>ref_w_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ref_l_err</span> <span class=o>=</span> <span class=p>(</span><span class=n>ref_l_pred</span> <span class=o>-</span> <span class=n>noise</span><span class=p>)</span><span class=o>.</span><span class=n>norm</span><span class=p>()</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>w_diff</span> <span class=o>=</span> <span class=n>model_w_err</span> <span class=o>-</span> <span class=n>ref_w_err</span>
</span></span><span class=line><span class=cl>    <span class=n>l_diff</span> <span class=o>=</span> <span class=n>model_l_err</span> <span class=o>-</span> <span class=n>ref_l_err</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>inside_term</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>*</span> <span class=n>beta</span> <span class=o>*</span> <span class=p>(</span><span class=n>w_diff</span> <span class=o>-</span> <span class=n>l_diff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=o>*</span> <span class=n>log</span><span class=p>(</span><span class=n>sigmoid</span><span class=p>(</span><span class=n>inside_term</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span>
</span></span></code></pre></td></tr></table></div></div><p>We can almost paste this code directly into python—see the &ldquo;DPO Loss Function&rdquo; section in the <a href=https://github.com/yue-here/glyffuser-dpo/blob/main/glyffuser_dpo.ipynb>training notebook</a> for the exact details. (Note: since we&rsquo;re not using text conditioning, we can simply ignore <code>c</code>.)</p><p>Below I&rsquo;ll discuss what&rsquo;s happening in this loss function; if you&rsquo;re mainly interested in the implementation, feel free to skip to the next section.</p><p>To interpret the DPO loss, we need to recall the standard diffusion training loop for the noise-prediction model (commonly a U-net), the core of a diffusion model. If you aren&rsquo;t familiar with this, I&rsquo;ve written a <a href=/posts/diffusion/>diffusion explainer</a> that goes through this topic clearly with minimal assumed knowledge.</p><p>To summarize, in the diffusion training loop we:</p><ul><li>Define a noising trajectory from pure data to pure (pixel-wise) Gaussian noise. Commonly the trajectory is divided into 1000 timesteps, where <code>t = 0</code> is uncorrupted data and <code>t = 1000</code> is pure noise.</li><li>Take a random training example, and noise it to a random timestep <code>t</code>. We can do this efficiently as the Gaussian noise for any <code>t</code> can be obtained in closed form.</li><li>Pass the noised example through the noise-prediction model to obtain predicted noise.</li><li>Update the weights of the noise-prediction model by minimizing the mean squared error (MSE) loss between the predicted noise and actual noise, conditioned on the timestep.</li><li>Repeat. Over many samples and timesteps, the model learns to predict noise for the training data distribution at any given point on the denoising trajectory.</li></ul><p>The diffusion-DPO loss has the following differences:</p><ul><li>We start with a noise-prediction model already pre-trained using the method above.</li><li>We load one copy of the pre-trained model as a frozen <code>ref_model</code> and another as a trainable <code>model</code>.</li><li>Instead of a single sample, each training sample is a preference pair (<code>x_w</code>, <code>x_l</code>).</li><li>We noise both samples with identical noise to obtain <code>noisy_x_w</code> and <code>noisy_x_l</code>.</li><li>We then pass the noised pair through both models to obtain 4 noise predictions: <code>model_w_pred</code>, <code>model_l_pred</code>, <code>ref_w_pred</code> and <code>ref_l_pred</code>.</li><li>From these, we obtain 4 MSEs between predicted and actual noise: <code>model_w_err</code>, <code>model_l_err</code>, <code>ref_w_err</code>, and <code>ref_l_err</code>.</li><li>We construct the diffusion-DPO loss by combining these errors.</li></ul><p>The overall aim of the diffusion-DPO loss is to push <code>model</code> to generate samples more similar to <code>x_w</code>, and less similar to <code>x_l</code>. To do this, we define <code>w_diff</code> and <code>l_diff</code> as the differences between the model error and the reference error. Intuitively, we want to to minimize <code>w_diff</code> while maximizing <code>l_diff</code>. These values are combined in <code>inside_term</code> with a scaling term <code>beta</code>, then passed through a log-sigmoid to generate a log-likelihood loss. The figure belows visualizes this: to minimize the loss, we want to maximize <code>inside_term</code>, which means reducing <code>w_diff</code> and increasing <code>l_diff</code>.</p><figure class=align-center><img loading=lazy src=/dpo_loss.png#center alt="How the DPO loss varies with inside_term" width=60%><figcaption><p>How the DPO loss varies with <code>inside_term</code></p></figcaption></figure><p>To paraphrase Linkin Park&rsquo;s &ldquo;Numb&rdquo;:<br></p><blockquote><p>I&rsquo;m becoming this, all I want to do<br>Is be more like <code>x_w</code> and be less like <code>x_l</code><br></p></blockquote><h2 id=creating-the-training-script>Creating the training script<a hidden class=anchor aria-hidden=true href=#creating-the-training-script>#</a></h2><p>We can base the diffusion-DPO training script the standard diffusion training script for the glyffuser with relatively small changes:</p><ul><li>Single image data were previously loaded with a standard pytorch <code>Dataset</code>. Now we create a <code>PreferenceDataset</code> subclass that reads the human feedback JSON created before, and returns an image preference pair. (&ldquo;Preference Dataset&rdquo; in the <a href=https://github.com/yue-here/glyffuser-dpo/blob/main/glyffuser_dpo.ipynb>training notebook</a>)</li><li>The training loss was previously a single MSE; we substitute in the diffusion-DPO loss discussed above.</li></ul><h2 id=training-the-model-and-assessing-the-results>Training the model and assessing the results<a hidden class=anchor aria-hidden=true href=#training-the-model-and-assessing-the-results>#</a></h2><p>We can start with the key training hyperparameters from the <a href=https://arxiv.org/pdf/2311.12908>diffusion-DPO paper</a>:</p><blockquote><p>A learning rate of <sup>2000</sup>⁄<sub>β</sub> 2.048·10⁻⁸ is used with 25% linear warmup. The inverse scaling is motivated by the norm of the DPO objective gradient being proportional to β (the divergence penalty parameter) [33]. For both SD1.5 and SDXL, we find β ∈ [2000, 5000] to offer good performance (Supp. S5). We present main SD1.5 results with β = 2000 and SDXL results with β = 5000.</p></blockquote><p>S5 offers a little more information (recall that β is the coefficient for <code>inside_term</code>):</p><blockquote><p>For β far below the displayed regime, the diffusion model degenerates into a pure reward scoring model. Much greater, and the KL-divergence penalty greatly restricts any appreciable adaptation.</p></blockquote><p>Since our data distribution, preference dataset size and model complexity are all quite different from the paper, I started with runs around the same scale as the base model training—100 epochs of the 2,500 pair dataset. The maximum batch size is much smaller as the VRAM needs to hold an additional (frozen) model and 4 sets of activations rather than 1.</p><p>An initial run at β = 5000 shows very little change:<figure class=align-center><img loading=lazy src=/ref_vs_dpo_b5000.png#center alt="Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a &lsquo;spot the difference&rsquo; game." width=100%><figcaption><p>Samples generated by the reference glyffuser model (top) and a DPO-aligned model trained at β = 5000 (below). The differences are so small this resembles a &lsquo;spot the difference&rsquo; game.</p></figcaption></figure></p><p>Further testing shows the importance of the β value. The interactive figure below shows how the output of the model changes as it&rsquo;s trained with β = 1500 over 200 epochs.</p><div class="slider-container unique-slider-container1"><button id=autoplayButton1 class=autoplayButton>Play</button>
<span id=sliderValue1 class=sliderValue>Training epoch: 0</span>
<input type=range id=parameterSlider1 class=parameterSlider min=0 max=20 step=1 value=0></div><center><img id=outputImage1 class=outputImage src=/DPO_slider/epoch_0000.png alt="DPO Model Output"></center><p>While there currently don&rsquo;t exist any scoring models that can assess &lsquo;glyph plausibility&rsquo;, the subjective quality change is very clear to me:</p><ul><li>12 or more of the 16 glyphs from the reference model (epoch 0) have problems</li><li>By 100 epochs, less than half of the glyphs have clear problems and the problems are less pronounced</li><li>At 200 epochs, we lose a lot of glyph complexity, likely due to overfitting to the small preference set. (I often favoured simpler glyphs when generating the preference set, as complex glyphs tended to contain more errors.)</li></ul><p>At 200 epochs, training metrics were still improving (see figure below) and importantly the preference gap <code>l_diff - w_diff</code> was still increasing. Looking at the progression above, however, we can&rsquo;t rely on metrics and have to make a judgement on when to stop training. Here, I think that 130-150 epochs seems to strike a good balance of creativity and plausibility.</p><figure class=align-center><img loading=lazy src=/dpo_training_metrics.png#center alt="Training metrics for DPO alignment at β = 1500 over 200 epochs." width=100%><figcaption><p>Training metrics for DPO alignment at β = 1500 over 200 epochs.</p></figcaption></figure><h2 id=further-remarks>Further remarks<a hidden class=anchor aria-hidden=true href=#further-remarks>#</a></h2><p>I hope that if you&rsquo;re interested using human feedback to align your models but weren&rsquo;t sure where to start, this has given you some ideas!</p><p>I was surprised at how effective the model alignment was with a relatively small dataset. With the increasing interest in diffusion models (and other generative models) in the sciences, this might be an interesting avenue of research. Preference sets might be generated automatically from literature using agentic methods. A brief search reveals only a few papers in this area (<a href=https://arxiv.org/pdf/2310.12304>example</a>).</p><p>DPO alignment also worked for the text-conditioned glyffuser model, but I&rsquo;ve left that out of this post as I think it complicates things without changing the basic idea.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://yue-here.com/posts/diffusion/><span class=title>Next »</span><br><span>A visual guide to how diffusion models work</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yue-here.com/>Yue Wu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>