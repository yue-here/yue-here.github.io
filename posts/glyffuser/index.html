<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Teaching an AI to invent new Chinese characters | Yue Wu</title>
<meta name=keywords content><meta name=description content="


For associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign."><meta name=author content="Yue Wu"><link rel=canonical href=https://yue-here.com/posts/glyffuser/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://yue-here.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yue-here.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yue-here.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yue-here.com/apple-touch-icon.png><link rel=mask-icon href=https://yue-here.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yue-here.com/posts/glyffuser/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css integrity=sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js integrity=sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:url" content="https://yue-here.com/posts/glyffuser/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="Teaching an AI to invent new Chinese characters"><meta property="og:description" content="For associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-26T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-26T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Teaching an AI to invent new Chinese characters"><meta name=twitter:description content="


For associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yue-here.com/posts/"},{"@type":"ListItem","position":2,"name":"Teaching an AI to invent new Chinese characters","item":"https://yue-here.com/posts/glyffuser/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Teaching an AI to invent new Chinese characters","name":"Teaching an AI to invent new Chinese characters","description":"\rFor associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign.\n","keywords":[],"articleBody":"\rFor associated code, please see the github repo. Huge shoutout to my old friend Daniel Tse, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign.\nIn this article, we build a text-to-image AI that learns Chinese characters in the same way humans do - by understanding what their components mean. It can then invent new characters based on the meaning of an English prompt.\nFor a well-illustrated guide to how diffusion models work using examples from this project, see my companion piece to this post.\nIntro Chinese characters are pretty cool, and there are a lot of them; around 21,000 are represented in unicode alone. Interestingly, they encode meaning in multiple ways. Some are simple pictures of things - 山 (shān) means, as you might be able to guess, ‘mountain’. Most characters are compounds, however: 好 (hǎo), meaning ‘good’, is constructed from the pictograms for 女 (nǚ), ‘woman’ and 子 (zǐ), child. Read into that what you will. Compounds often contain a semantic and a phonetic component; these subcomponents are known as radicals. For example, the names of most metallic chemical elements such as lithium, 锂 (lǐ) are composed of a semantic radical (⻐, the condensed, simplified form of 金 (jīn) meaning ‘gold/metal’) and a phonetic component 里 (lǐ, of unrelated meaning) which approximates the ’li’ sound of lithium.\nI’ve been interested for a while in how Chinese characters might be explored using machine learning. Can we teach a model to understand how they are structured from pictures and definitions alone? Can it then invent new characters? (Spoiler: yes) Let’s talk about a couple of ways of engaging with this, starting with relatively simple ML models and moving towards state-of-the-art technologies.\nDataset First, we need a dataset consisting of images of as many known Chinese characters as possible. The unicode standard is a convenient method of indexing a large proportion of known Chinese characters. Fonts provide a specific way of representing characters, in the form of distinct glyphs. Fonts have different levels of completeness, so choosing one that contained as many characters as possible for a large and diverse dataset was important.\nThe main resources used for generating the dataset:\nThe unicode CJK Unified Ideographs block (4E00–9FFF) of 20,992 Chinese characters Google fonts (Noto Sans and others) English definitions from the unihan database (kDefinition.txt) - this becomes useful later Let’s make the dataset by saving the glyphs as pictures in the square 2n×2n format favoured by ML convention. We want to keep detail while minimizing size - I found that 64×64 and 128×128 worked well. Since we don’t need colour, we can get away with a single channel - otherwise we’d use a 3 channel RGB image of size 3×128×128.\nVisualise the character set in 2D (Jupyter notebook) First let’s map the space with a dimensional reduction. UMAP works well here. Distinct clusters emerge - the largest cluster on the left represents the “lefty-righty” characters (to use the technical term), while the second largest cluster on the right represents “uppy-downy” characters. Tighter clusters/subclusters tend to correspond to characters that share the same radical - the small streak at the very top right is an almost literal leper colony of characters with the 疒 radical, indicating sickness. (Hover over/tap points to see the characters)\nVariational autoencoder (Jupyter notebook) Let’s try using an older generative ML technology, the variational autoencoder (VAE), to model the underlying distribution of Chinese characters. The dimensionality of the original input boils down to a vector of length 64×64=4096, i.e. one value for each pixel. The VAE architecture works by using convolutional layers to reduce the dimensionality of the input over several steps (the ’encoder’), then using a reverse convolution to increase the dimensionality back to the original (the ‘decoder’). The network is trained on its ability to output a match to the original.\nThe key intuition is that not all of the pixels in the images contain valuable information, so they can somehow be compressed to a smaller (lower-dimensional) vector. This is known as the ’latent space’. By passing our training images through a bottleneck the size of the latent space and then training the model to reconstruct the full original image, we create a ‘decoder’ that can translate any vector in the latent space to a full sized image. In this case, we go from 64×64=4096 to a latent space of size 256.\nBy passing images from our original dataset though the VAE again, we can see that the VAE has learnt a good latent representation of our glyphs:\nAside: the VAE is somewhat different compared to a vanilla autoencoder which is only graded on reconstruction. The VAE learns its latent space as a probability distribution rather than deterministically. To make it possible to train the VAE via backpropagation, the “reparameterization trick” is used which essentially separates out the parameters of the distribution into learnable NN parameters. Our VAE loss function consists of not just reconstruction loss, but also the Kullback–Leibler divergence, which is a measure of distance between two probability distributions. We want to minimize the distance between the learned distribution of Chinese glyphs and the normal distribution N(0,I). This pushes the latent distribution from ‘whatever it wants to be’ towards a normal distribution, which makes sampling and traversing the latent space easier.\nWe can compare the latent space learnt by the VAE by passing the dataset through the VAE encoder only, generating a dataset of vectors in the latent space. (Note: the latent space is a probability distribution, and so our vectors are non-deterministic samples) We can then apply the same UMAP visualisation as before. While the lefty-righty and uppy-downy clusters from before still exist, you can see that the points have all been pushed closer to a (2D) normal distribution. Intuitively, adjacent points in this distribution are more likely to be similar - it’s ‘smoother’.\nWe can interact with the latent space via the encoder and decoder components of our trained VAE. For example, we can perform bilinear interpolation between 4 characters. We obtain the latent vector for each one by using the VAE encoder as before, then interpolate the intermediate vectors and decode them to images with VAE decoder.\nThis allows us to do some interesting things like morph smoothly between characters by interpolating between vectors in latent space:\nYour browser does not support the video tag.\rFor some types of data such as faces or organic molecules, VAEs can be used to generate convincing new samples simply by moving around in the latent space. However, this is a dead end for us - the intermediate steps are clearly not valid Chinese characters, and there is no clear way of accessing the underlying structure of the distribution do so. This gives us a not-so-elegant segue to the generative modality du jour: diffusion models.\nUnconditional diffusion model Initially models designed to remove noise from images, it was found that when applied to pure noise, denoising diffusion models could generate entirely new images of types they were trained on. Even more excitingly, when trained with text conditioning, they could then generate images related to a provided text prompt. This led the proliferation of text-to-image models starting around late 2022 like Dall-E, Midjourney, Imagen, and Stable Diffusion. These models tend to be trained on a huge number of text-image pairs harvested en masse from the internet. The training cost of these models is generally on the order of 6 figures (USD) or more.\nI was curious if the same type of architecture could be used in a simplified form, with a smaller dataset, to achieve an interesting result like generating convincing Chinese characters. Of course, I didn’t have 6 figures (USD) to spend on this - everything was done on my home rig with a used 3090 bought for ~$700 (the best VRAM Gb/$ value in 2023/4!).\nThe first step was to implement an unconditional model trained on our existing dataset, to see if convincing generations could be achieved at all. Unconditional simply means that the model is trained to generate similar things to the data it sees, without any other guidance. Those playing the home game can follow along with the associated notebook here\nMost image diffusion models use some variant of the U-net architecture, initially developed for segmenting biomedical images. This network turns out to be a good choice for diffusion models, where we need to train a model that can predict noise that can then be subtracted to generate recognizable images. We also need a noise scheduler that controls the mathematics of how noise is added during the training process. To save time in implementing the whole model from scratch, we can use Huggingface’s handy diffusers library with the UNet2DModel for the Unet and the DDPMScheduler as the noise scheduler. This makes the core of the code relatively straightforward:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from diffusers import UNet2DModel, DDPMScheduler # Define the UNet model model = UNet2DModel( sample_size=128, # the target image resolution in_channels=1, # the number of input channels out_channels=1, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(128, 128, 256, 256, 512, 512), # the number of output channels for each UNet block # Define the UNet architecture down_block_types=( \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\", ), up_block_types=( \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", ), ) # Initialize the noise scheduler noise_scheduler = DDPMScheduler(num_train_timesteps=1000) As the process is relatively expensive, the large models mentioned above generally train on some kind of lower dimensional space, coupled with a method to later obtain high resolution images. For example, Google’s Imagen uses a sequence of conditioned superresolution networks, while Stable Diffusion is built on the idea of latent diffusion, where the final images are decoded from a lower-dimensional latent space using a VAE. This step is omitted here as we have enough compute to train the model in the relatively small native size (128×128) of the data. The remainder of the code is mostly plumbing to get the data to the model. I did run into a couple of traps here though:\nThe scheduler for running inference on a diffusion model need not be the same as the one used for training, and many good schedulers have been developed by the community that can give excellent results in 20-50 steps rather than the 100s that might be needed with the standard DDPMScheduler(). For our purposes, “DPM++ 2M” or DPMSolverMultistepScheduler() in the Diffusers library worked very well.\nDataloader shuffling is essential with such small datasets. I ran into a bug that took weeks to diagnose: sampling every epoch led to overfitting (lower training loss) with much poorer model performance as assessed by eye. The same model sampled every 10 epochs had no such problems. Test showed model parameters before and after sampling were identical. After much suffering, the problem was found to be caused by this inference call to the Diffusers pipeline I’d cribbed from Huggingface:\n1 2 3 4 images = pipeline(batch_size=batch_size, generator=torch.manual_seed(config.seed), num_inference_steps=num_steps, ).images Setting generator=torch.manual_seed(config.seed) resets the seed for the whole training loop, meaning the model will see training samples in the same order every reset. This allowed the model to learn the sample order as an unintended side effect, leading to overfitting and degraded performance. Setting a generator on the CPU instead allows the training to continue unmolested: generator=torch.Generator(device='cpu').manual_seed(config.seed)\nBelow is a video of characters generated during each training epoch from a model trained for 100 epochs on the same Chinese glyph dataset used before. Compared to the VAE model discussed in the last part, we can see the unconditional diffusion model is much better at capturing the hierarchical structure of characters at different levels, and learns this very early on in the training process.\nYour browser does not support the video tag.\rThe model also works on other writing styles such as ancient Chinese seal script!\nConditional diffusion model Now that we have confirmed a diffusion model can generate convincing Chinese characters, let’s train a text-to-image model conditioned on the English definitions of each character. If the model correctly learns how the English definition relates to the Chinese character, it should be able to ‘understand’ the rules for how characters are constructed and generate characters that ‘give the right vibe’ for a given english prompt.\nTip The concept of ‘conditioning’ may seem mysterious but here it boils down to working out a way to represent the English text as a vector, then adding that vector to another vector representing the image during the training process. In a previous blog post, I discussed finetuning Stable Diffusion. However that seemed like the wrong approach here - the pretraining of SD wouldn’t do much for us since the types of images we want to generate are unlikely to be well represented in their training set. So which framework to use?\nIn a misguided attempt to save effort, I first tried Assembly AI’s minimagen implementation, as an ostensibly simple conditional diffusion framework. It rapidly (but not rapidly enough) became clear that even after extensive debugging this was not fully functional demo code. I moved on to lucidrains’ much cleaner imagen implementation and trained some variants in unconditional mode, matching architectures as best as I could with my previous model (within the constraints of the framework), but I couldn’t never replicate the same quality - I suspect this was due to some differences in the layer architecture I couldn’t identify.\n“You could not live with your own failure, where did that bring you? Back to me.”\nIn the end I decided I had to implement the model myself, something I’d been studiously avoiding. While huggingface’s UNet2DConditionModel() provides a framework for this, I was unable to find good documentation and so ended up having to scour the codebase directly. My observations below on what is needed, in case you dear reader want to take up this foolhardy task. Follow along with the notebook here.\nThe UNet2DConditionModel() is able to be conditioned on either discrete classes (dog, car, house, etc.), or embeddings (vectors from a latent space). For text-to-image, we are using embeddings. I decided to take a page out of Google’s book and use LLM embeddings directly for conditioning, as Imagen does. What Google found with Imagen was that conditioning on text embeddings from an LLM such as their T5 model worked just as well or even better than the CLIP model trained on image-caption pairs that Dall-E uses. This is very handy for us, since pretrained CLIP-type embeddings would likely be inappropriate for our use case reasons discussed above. In practice, we can call down existing T5 methods from Huggingface’s transformers library and pass it some text; it will give us back a fixed-length vector embedding, imbued with LLM magic.\nHere’s the core of the conditional UNet model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from diffusers import UNet2DConditionModel # Define the conditional UNet model model = UNet2DConditionModel( sample_size=128, in_channels=1, out_channels=1, layers_per_block=2, block_out_channels=(128, 256, 512, 512), addition_embed_type=\"text\", # Make it conditional cross_attention_dim=512, encoder_hid_dim=512, # the hidden dimension of the encoder encoder_hid_dim_type=\"text_proj\", down_block_types=( \"DownBlock2D\", \"DownBlock2D\", \"CrossAttnDownBlock2D\", \"DownBlock2D\" ), up_block_types=( \"UpBlock2D\", \"CrossAttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\" ), ) We specify the nature of the conditioning vector being passed to the model via some extra arguments. We also need to introduce some extra plumbing. First, we now need text captions for each training image, which we obtained from the unihan definitions database (way back at the top of this article). We then need a collator function that helps the dataloader return LLM embeddings from the text caption, as well as an attention mask for the embedding. Finally, we write our own pipeline, GlyffuserPipeline() (a subclass of DiffusionPipeline()) that can correctly pass the text embeddings and masks during inference. From there, we can train it in the same way as the previous model.\nLearning the structure of Chinese characters Now that we’ve trained the conditional model, how can we know what the model has learnt about how Chinese characters are constructed? One way is to probe it with different prompts. In the intro I mentioned that most characters contain semantic or phonetic radicals, and the UMAP shows the distribution of major character types.\nIf we obtain a list of most common Chinese radicals from, say, an 18th century Chinese dictionary we can probe the model with prompts corresponding to the English meaning of each radical and see what is activated. Below we see what happens using different numbers of sampling steps to approximate the denoising trajectory (NB: in the animation in the previous section, we show training steps but each frame is sampled for 50 steps. Don’t get these confused!) for English text prompts corresponding to the most frequently appearing 36 Chinese radicals. (The radicals are not used in the prompt but shown in parentheses for reference):\nVery interesting! We see in most cases the corresponding radical is directly activated in the part of the character it usually resides, even though the rest of the character is very noisy. We can infer that the english prompt is strongly tied semantically, i.e. in meaning, to characters that contain this radical.\nThe 虫 radical means “bug” and is quite strongly activated by the prompt ‘insect’ as it is used in bug-related characters (fly: 蝇, butterfly: 蝴蝶, earthworm: 蚯蚓). However, the 忄(heart) radical is not activated by “heart” (its literal meaning) as its semantic meaning is linked to the historical belief that thoughts and emotions arise from the physical heart (memory: 忆, afraid: 怕, fast: 快). Can we activate this radical by changing our prompt?\nThe picture shows increasing the number of sampling steps on the same prompt. With 1 step, we see the activation of 忄 but as we increase the resolution of the sampling trajectory the preferred radical resolves to 心 at the bottom. If you can read Chinese, you’ll already know why this is: 忄 is actually an alternative form of 心, which is standalone character for heart and can also be used as a bottom radical. Characters with this radical are also linked to thoughts and feelings, e.g. 念 (to miss) and 想 (to think).\nNotably, we can’t access the phonetic component in this model as we haven’t included any information on Chinese pronunciation in the training process. Can we, however, generate new characters using semantic components? Let’s try:\nWe can! We have both the components for ‘sickness’ and ’thoughts/feelings’ in this character. The overall structure of this character likely draws from characters such as 痣 (zhì, a mole/mark on the skin) in the training set, but in 痣 the 志, also pronounced zhì, is a phonetic component and has no semantic significance. Of course, this is a cherry-picked example. What happens if we try to activate two radicals that prefer to live in the same place? If we provoke a conflict by prompting “a sickness of fire”, for example, we get this:\nYou can see that the 1 step denoising tries to place us in the 火 radical part of the data distribution, suggesting that might be the strongest semantic adherence in the prompt. With 3 steps, we also see the presence of a partial sickness radical 疒. The fact that the fire and sickness radicals share a large stroke in the training font suggest they are close in the data distribution and the low resolution sampling has landed somewhere in the middle. With more extensive sampling, however, neither concept is strong enough to dominate so the model navigates toward enforcing some local structure out of the noise, with the overall character looking not at all convincing.\nNow that we understand better how the model works, let’s take another look at previous grid and take it all the way up to 50 sampling steps. You may wonder why we never generate the base character, always the radical. The nature of our training set means that there are many more semantically-related training samples containing the radical compared to the one original character, so it makes sense that model would prefer to place the radical in the appropriate position. Sample weighting by corpus frequency would be one way of getting around this! We also see the the rest of the character is filled with what appears to be random stuff. I suspect this is again the model enforcing local consistency without particularly strong semantic guidance.\nI tested an implementation including phonetic radical embeddings but there are some structural reason it did not work well - I discuss this here.\nClassifier-free guidance (CFG) for the glyffuser Classifier-free guidance is an elegant and powerful technique that is now used in basically all production conditional diffusion models to improve adherence to prompts. (For an excellent treatment, see here)\nEssentially, this method allows the strength of any given prompt to be varied without needing to perform any additional training. Moreover, the strength of the prompt can be increased far above that for standard conditional training. Can we use this method get better generations from the glyffuser?\nTo implement this method, we simply add random dropout of the text conditioning tokens during training (10-20% has been found to work well). This effectively trains an unconditional model at the same time. During sampling steps, we then perform the noise prediction twice, once normally and once with a blank conditioning tensor. We then combine them as follows:\nnoise_prediction = noise_prediction_unconditional + guidance_scale * (noise_prediction - noise_prediction_unconditional)\nTip At guidance_scale = 0, the model acts as an unconditional model while at guidance_scale = 1, the model acts as the standard conditional model Generally, increasing guidance_scale in text-to-image models decreases variety while increasing adherence to the prompt. Let’s try probing the model by varying the number of sampling steps and guidance scale for the prompt “bird” corresponding to a very common radical (鳥/鸟):\nTip Unusually, the “bird” radical can occur on either the left (“鸵”, ostrich) or right (“鸡”, chicken) sides of characters. I’ve also include interactive explorers for the prompts ‘fire’ and ‘hair’ in the appendices. I highly recommend taking a look, they are very funny!\nCompared to the base conditional model, we see that as we increase the guidance scale, the ‘bird’ radical becomes increasingly activated with even very few sampling steps. Interestingly, while the traditional form of the bird character “鳥” dominates (it is more prevalent in the training set), the simplified form “鸟” also makes a single appearance (10 steps, scale=50), making it a ’transition state’ during the denoising process. The explorer below shows CFG scales of 0 to 100 for different random seeds - higher CFG scales do indeed reduce sample variety. Compared to general-purpose text-to-image models however, we can tolerate higher CFG scales as they tend to give more convincing characters. If you follow any individual character, you’ll see that it tends to start with one ‘bird’ radical, then as CFG scale increases, at some point the other side will also collapse to a ‘bird’ radical.\nPause\rCFG scale 1\rLooking at the a picture of varying CFG scale generations for prompts for each common radical below, we see that for scale values of around 5-15, the generated characters are much more consistently convincing than the those made by the base conditional or unconditional models. (I think that anyone who grew up with Chinese or has studied it for a long time gains a strong instinctive sense of whether a character could be real, even if they don’t recognise it.)\nOutro So can we teach a model to understand how Chinese characters and then invent new ones? It looks the answer is a resounding yes! We find that the final conditioned diffusion model (the Glyffuser™) has a strong conception of how the components of a Chinese character relate to its meaning, in much the same way a human would guess at the meaning of an unknown character. Moreover, by strengthening the adherence to known concepts using classifier-free guidance, we can generate very convincing characters that follow the rules of Chinese glyph construction.\nTest the models here If you want to try out the conditional glyffuser, the easiest way is to get the demo repo with git clone https://github.com/yue-here/glyffuser, make a python environment with requirements.txt (you may need to set it up for your GPU and jupyter) then run the glyffuser inference.ipynb notebook, which will summon the trained model from my huggingface repo.\nFailing that, I’ve made a gradio applet on Hugging Face that will run the inference, but please be patient as it’s quite slow (around a minute per step).\nAppendices Appendix 1: other writing styles Here’s a training video from a version of the glyffuser trained on the ancient Chinese writing known as seal script:\nYour browser does not support the video tag.\rAppendix 2: phonetic radicals To include the phonetic radicals, we need a representation. The standard romanization is known as pinyin - for example, the phrase 人工智能 (artificial intelligence) would be “rén gōng zhì néng”. The diacritics on the vowels are the tones of which there are 4 in standard mandarin. These can also be represented numerically, e.g. “ren2 gong1 zhi4 neng2”.\nSimply adding the pinyin to the english definition prior to training is unlikely to work as we are using a frozen tokenizer and text model which does not recognize the pinyin syllables. Instead, we can create separate embeddings of the same length as the image vector for the pinyin and add them directly, the same as the text conditioning is added. Specifically, based on how Chinese syllables are structured, I added 3 trainable embeddings for the initial, final, and tone (for example, ren2 would become ‘r’, ’en’ and ‘2’). To train the model, I added extra inputs for the pinyin and tone.\nAfter training the model in the same way as before, I tried to ‘summon’ the phonetic components. However, the results only changed slightly with different pinyin prompts. I suspect this is because of the excessive homophony in Chinese. For example, 羊 (yang2, ‘goat/sheep’) is a common phonetic radical. But for this exact syllable, wiktionary gives 47 different characters. Have a look at characters containing the top two phonetic radicals for this pronunciation, 羊 and 昜:\n羊佯垟徉样洋烊珜眻蛘𨦡羏劷\n昜崵揚暘楊湯敭瑒煬禓瘍諹輰鍚鐊陽霷颺鰑鸉\nIt’s likely the same situation as the ‘clashing radicals’ case where we can’t activate the ‘fire’ and ‘sickness’ radicals at same time - when neither phonetic radical dominates the distribution, we end up sampling garbage.\nAppendix 3: CFG variations for “fire” The Chinese character for fire “火” has a particularly varied set of possible locations. These are showcased in the characters “炎” and “焱”. Another form is the bottom radical “灬”, a kind of deconstructed version of “火”. As such, greater variety is possible and this shows:\nPause\rCFG scale 1\rAppendix 4: CFG variations for “hair” I’m mostly including this because the characters look very funny.\nPause\rCFG scale 1\r","wordCount":"4514","inLanguage":"en","datePublished":"2024-05-26T00:00:00Z","dateModified":"2024-05-26T00:00:00Z","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yue-here.com/posts/glyffuser/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"https://yue-here.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yue-here.com/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yue-here.com/index.html title=Home><span>Home</span></a></li><li><a href=https://yue-here.com/bio/ title=Bio><span>Bio</span></a></li><li><a href=https://yue-here.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yue-here.com/research/ title=Research><span>Research</span></a></li><li><a href=https://yue-here.com/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Teaching an AI to invent new Chinese characters</h1><div class=post-meta><span title='2024-05-26 00:00:00 +0000 UTC'>May 26, 2024</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;4514 words&nbsp;·&nbsp;Yue Wu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#visualise-the-character-set-in-2d>Visualise the character set in 2D</a></li><li><a href=#variational-autoencoder>Variational autoencoder</a></li><li><a href=#unconditional-diffusion-model>Unconditional diffusion model</a></li><li><a href=#conditional-diffusion-model>Conditional diffusion model</a></li><li><a href=#learning-the-structure-of-chinese-characters>Learning the structure of Chinese characters</a></li><li><a href=#classifier-free-guidance-cfg-for-the-glyffuser>Classifier-free guidance (CFG) for the glyffuser</a></li><li><a href=#outro>Outro</a></li><li><a href=#test-the-models-here>Test the models here</a></li><li><a href=#appendices>Appendices</a><ul><li><a href=#appendix-1-other-writing-styles>Appendix 1: other writing styles</a></li><li><a href=#appendix-2-phonetic-radicals>Appendix 2: phonetic radicals</a></li><li><a href=#appendix-3-cfg-variations-for-fire>Appendix 3: CFG variations for &ldquo;fire&rdquo;</a></li><li><a href=#appendix-4-cfg-variations-for-hair>Appendix 4: CFG variations for &ldquo;hair&rdquo;</a></li></ul></li></ul></nav></div></details></div><div class=post-content><script>document.addEventListener("DOMContentLoaded",e=>{function t(e,t,n,s,o){const a=document.getElementById(e),u=document.getElementById(t),h=document.getElementById(n),r=document.getElementById(s);let i=!0,c;function l(e){h.textContent=`CFG scale ${e}`,u.src=`/${o}_slider/${o}-CFG${e}_grid.png`}a.addEventListener("input",e=>{const t=e.target.value;l(t)}),r.addEventListener("click",()=>{i=!i,r.textContent=i?"Pause":"Play",i?d():clearInterval(c)});function d(){c=setInterval(()=>{let e=parseInt(a.value,10);e=(e+1)%100,a.value=e,l(e)},100)}d()}t("parameterSlider1","outputImage1","sliderValue1","autoplayButton1","bird"),t("parameterSlider2","outputImage2","sliderValue2","autoplayButton2","fire"),t("parameterSlider3","outputImage3","sliderValue3","autoplayButton3","hair")})</script><style>html,body{margin:0;padding:0;height:100%;overflow-x:hidden;box-sizing:border-box}*,*::before,*::after{box-sizing:inherit}.slider-container{max-width:100%;width:100%;margin:20px auto;display:flex;align-items:center;flex-wrap:nowrap;overflow:hidden}.parameterSlider{flex:1;margin-left:10px;min-width:0}.outputImage{display:block;margin:20px auto;max-width:100%}.sliderValue{min-width:100px;text-align:right;white-space:nowrap}.autoplayButton{min-width:60px;margin-right:10px;flex-shrink:0}</style><p><em>For associated code, please see the <a href=https://github.com/yue-here/glyffuser>github repo</a>. Huge shoutout to my old friend <a href=http://overpunch.com/>Daniel Tse</a>, linguist and ML expert extraordinaire for invaluable help and ideas on both fronts throughout this campaign.</em></p><center><figure><img loading=lazy src=/glyffuser%20grabber.png></figure><p>In this article, we build a text-to-image AI that learns Chinese characters in the same way humans do - by understanding what their components mean. It can then invent new characters based on the meaning of an English prompt.</p><p>For a well-illustrated guide to how diffusion models work using examples from this project, see my <a href=/posts/diffusion/>companion piece</a> to this post.</p></center><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>Chinese characters are pretty cool, and there are a lot of them; around 21,000 are represented in unicode alone. Interestingly, they encode meaning in multiple ways. Some are simple pictures of things - 山 (shān) means, as you might be able to guess, &lsquo;mountain&rsquo;. Most characters are compounds, however: 好 (hǎo), meaning &lsquo;good&rsquo;, is constructed from the pictograms for 女 (nǚ), &lsquo;woman&rsquo; and 子 (zǐ), child. Read into that what you will. Compounds often contain a semantic and a phonetic component; these subcomponents are known as radicals. For example, the names of most metallic chemical elements such as lithium, 锂 (lǐ) are composed of a semantic radical (⻐, the condensed, simplified form of 金 (jīn) meaning &lsquo;gold/metal&rsquo;) and a phonetic component 里 (lǐ, of unrelated meaning) which approximates the &rsquo;li&rsquo; sound of lithium.</p><p>I&rsquo;ve been interested for a while in how Chinese characters might be explored using machine learning. Can we teach a model to understand how they are structured from pictures and definitions alone? Can it then invent new characters? (Spoiler: yes) Let&rsquo;s talk about a couple of ways of engaging with this, starting with relatively simple ML models and moving towards state-of-the-art technologies.</p><h2 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h2><p>First, we need a dataset consisting of images of as many known Chinese characters as possible. The unicode standard is a convenient method of indexing a large proportion of known Chinese characters. Fonts provide a specific way of representing characters, in the form of distinct glyphs. Fonts have different levels of completeness, so choosing one that contained as many characters as possible for a large and diverse dataset was important.</p><p>The main resources used for generating the dataset:</p><ul><li>The unicode CJK Unified Ideographs block (4E00–9FFF) of 20,992 Chinese characters</li><li>Google fonts (<a href=https://fonts.google.com/noto/specimen/Noto+Sans>Noto Sans</a> and others)</li><li>English definitions from the unihan database (<a href=https://github.com/unicode-org/unihan-database/blob/main/kDefinition.txt>kDefinition.txt</a>) - this becomes useful later</li></ul><p>Let&rsquo;s <a href=https://github.com/yue-here/glyffuser/blob/main/Unihan%20glyph%20generator.ipynb>make the dataset</a> by saving the glyphs as pictures in the square 2<sup>n</sup>×2<sup>n</sup> format favoured by ML convention. We want to keep detail while minimizing size - I found that 64×64 and 128×128 worked well. Since we don&rsquo;t need colour, we can get away with a single channel - otherwise we&rsquo;d use a 3 channel RGB image of size 3×128×128.</p><h2 id=visualise-the-character-set-in-2d>Visualise the character set in 2D<a hidden class=anchor aria-hidden=true href=#visualise-the-character-set-in-2d>#</a></h2><p>(<a href=https://github.com/yue-here/glyffuser/blob/main/glyph%20explorer.ipynb>Jupyter notebook</a>) First let&rsquo;s map the space with a dimensional reduction. UMAP works well here. Distinct clusters emerge - the largest cluster on the left represents the &ldquo;lefty-righty&rdquo; characters (to use the technical term), while the second largest cluster on the right represents &ldquo;uppy-downy&rdquo; characters. Tighter clusters/subclusters tend to correspond to characters that share the same radical - the small streak at the very top right is an almost literal leper colony of characters with the 疒 radical, indicating sickness. (<strong>Hover over/tap points to see the characters</strong>)</p><p><script src=https://cdn.plot.ly/plotly-latest.min.js></script><div id=/umap.json class=plotly style=width:100%></div><script>Plotly.d3.json("/umap.json",function(e,t){Plotly.plot("/umap.json",t.data,t.layout,{responsive:!0})})</script></p><h2 id=variational-autoencoder>Variational autoencoder<a hidden class=anchor aria-hidden=true href=#variational-autoencoder>#</a></h2><p>(<a href=https://github.com/yue-here/glyffuser/blob/main/glyph%20explorer.ipynb>Jupyter notebook</a>) Let&rsquo;s try using an older generative ML technology, the variational autoencoder (VAE), to model the underlying distribution of Chinese characters. The dimensionality of the original input boils down to a vector of length 64×64=4096, i.e. one value for each pixel. The VAE architecture works by using convolutional layers to reduce the dimensionality of the input over several steps (the &rsquo;encoder&rsquo;), then using a reverse convolution to increase the dimensionality back to the original (the &lsquo;decoder&rsquo;). The network is trained on its ability to output a match to the original.</p><p>The key intuition is that not all of the pixels in the images contain valuable information, so they can <em>somehow</em> be compressed to a smaller (lower-dimensional) vector. This is known as the &rsquo;latent space&rsquo;. By passing our training images through a bottleneck the size of the latent space and then training the model to reconstruct the full original image, we create a &lsquo;decoder&rsquo; that can translate any vector in the latent space to a full sized image. In this case, we go from 64×64=4096 to a latent space of size 256.</p><p>By passing images from our original dataset though the VAE again, we can see that the VAE has learnt a good latent representation of our glyphs:</p><center><figure><img loading=lazy src=/VAE_reconstruction.png></figure></center><p><em>Aside: the VAE is somewhat different compared to a vanilla autoencoder which is only graded on reconstruction. The VAE learns its latent space as a probability distribution rather than deterministically. To make it possible to train the VAE via backpropagation, the &ldquo;reparameterization trick&rdquo; is used which essentially separates out the parameters of the distribution into learnable NN parameters. Our VAE loss function consists of not just reconstruction loss, but also the Kullback–Leibler divergence, which is a measure of distance between two probability distributions. We want to minimize the distance between the learned distribution of Chinese glyphs and the normal distribution N(0,I). This pushes the latent distribution from &lsquo;whatever it wants to be&rsquo; towards a normal distribution, which makes sampling and traversing the latent space easier.</em></p><p>We can compare the latent space learnt by the VAE by passing the dataset through the VAE encoder only, generating a dataset of vectors in the latent space. (Note: the latent space is a probability distribution, and so our vectors are non-deterministic samples) We can then apply the same UMAP visualisation as before. While the lefty-righty and uppy-downy clusters from before still exist, you can see that the points have all been pushed closer to a (2D) normal distribution. Intuitively, adjacent points in this distribution are more likely to be similar - it&rsquo;s &lsquo;smoother&rsquo;.</p><p><div id=/umap_vae.json class=plotly style=width:100%></div><script>Plotly.d3.json("/umap_vae.json",function(e,t){Plotly.plot("/umap_vae.json",t.data,t.layout,{responsive:!0})})</script></p><p>We can interact with the latent space via the encoder and decoder components of our trained VAE. For example, we can perform bilinear interpolation between 4 characters. We obtain the latent vector for each one by using the VAE encoder as before, then interpolate the intermediate vectors and decode them to images with VAE decoder.</p><center><figure><img loading=lazy src=/VAE_grid_interpolation.png></figure></center><p>This allows us to do some interesting things like morph smoothly between characters by interpolating between vectors in latent space:</p><center><video width=128 height=128 autoplay loop muted playsinline>
<source src=/VAE_interpolation_small.mp4 type=video/mp4>Your browser does not support the video tag.</video></center><p>For some types of data such as faces or organic molecules, VAEs can be used to generate convincing new samples simply by moving around in the latent space. However, this is a dead end for us - the intermediate steps are clearly not valid Chinese characters, and there is no clear way of accessing the underlying structure of the distribution do so. This gives us a not-so-elegant segue to the generative modality <em>du jour</em>: diffusion models.</p><h2 id=unconditional-diffusion-model>Unconditional diffusion model<a hidden class=anchor aria-hidden=true href=#unconditional-diffusion-model>#</a></h2><p>Initially models designed to remove noise from images, it was found that when applied to pure noise, <a href=https://arxiv.org/abs/2006.11239>denoising diffusion models</a> could generate entirely new images of types they were trained on. Even more excitingly, when trained with text conditioning, they could then generate images related to a provided text prompt. This led the proliferation of text-to-image models starting around late 2022 like Dall-E, Midjourney, Imagen, and Stable Diffusion. These models tend to be trained on a huge number of text-image pairs harvested <em>en masse</em> from the internet. The training cost of these models is generally on the order of 6 figures (USD) or more.</p><p>I was curious if the same type of architecture could be used in a simplified form, with a smaller dataset, to achieve an interesting result like generating convincing Chinese characters. Of course, I didn&rsquo;t have 6 figures (USD) to spend on this - everything was done on my home rig with a used 3090 bought for ~$700 (the best VRAM Gb/$ value in 2023/4!).</p><p>The first step was to implement an unconditional model trained on our existing dataset, to see if convincing generations could be achieved at all. Unconditional simply means that the model is trained to generate similar things to the data it sees, without any other guidance. Those playing the home game can follow along with the <a href=https://github.com/yue-here/glyffuser/blob/main/glyffuser%20unconditional.ipynb>associated notebook here</a></p><p>Most image diffusion models use some variant of the <a href=https://arxiv.org/abs/1505.04597>U-net</a> architecture, initially developed for segmenting biomedical images. This network turns out to be a good choice for diffusion models, where we need to train a model that can predict noise that can then be subtracted to generate recognizable images. We also need a noise scheduler that controls the mathematics of how noise is added during the training process. To save time in implementing the whole model from scratch, we can use Huggingface&rsquo;s handy <code>diffusers</code> library with the <code>UNet2DModel</code> for the Unet and the <code>DDPMScheduler</code> as the noise scheduler. This makes the core of the code relatively straightforward:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>UNet2DModel</span><span class=p>,</span> <span class=n>DDPMScheduler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the UNet model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>UNet2DModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>sample_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>  <span class=c1># the target image resolution</span>
</span></span><span class=line><span class=cl>    <span class=n>in_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>  <span class=c1># the number of input channels</span>
</span></span><span class=line><span class=cl>    <span class=n>out_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>  <span class=c1># the number of output channels</span>
</span></span><span class=line><span class=cl>    <span class=n>layers_per_block</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>  <span class=c1># how many ResNet layers to use per UNet block</span>
</span></span><span class=line><span class=cl>    <span class=n>block_out_channels</span><span class=o>=</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>  <span class=c1># the number of output channels for each UNet block</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Define the UNet architecture</span>
</span></span><span class=line><span class=cl>    <span class=n>down_block_types</span><span class=o>=</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;AttnDownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>up_block_types</span><span class=o>=</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;AttnUpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize the noise scheduler</span>
</span></span><span class=line><span class=cl><span class=n>noise_scheduler</span> <span class=o>=</span> <span class=n>DDPMScheduler</span><span class=p>(</span><span class=n>num_train_timesteps</span><span class=o>=</span><span class=mi>1000</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>As the process is relatively expensive, the large models mentioned above generally train on some kind of lower dimensional space, coupled with a method to later obtain high resolution images. For example, Google&rsquo;s Imagen uses a sequence of conditioned superresolution networks, while Stable Diffusion is built on the idea of latent diffusion, where the final images are decoded from a lower-dimensional latent space using a VAE. This step is omitted here as we have enough compute to train the model in the relatively small native size (128×128) of the data. The remainder of the code is mostly plumbing to get the data to the model. I did run into a couple of traps here though:</p><ol><li><p>The scheduler for running inference on a diffusion model need not be the same as the one used for training, and many good schedulers have been developed by the community that can give excellent results in 20-50 steps rather than the 100s that might be needed with the standard <code>DDPMScheduler()</code>. For our purposes, &ldquo;DPM++ 2M&rdquo; or <code>DPMSolverMultistepScheduler()</code> in the Diffusers library worked very well.</p></li><li><p>Dataloader shuffling is essential with such small datasets. I ran into a bug that took weeks to diagnose: sampling every epoch led to overfitting (lower training loss) with much poorer model performance as assessed by eye. The same model sampled every 10 epochs had no such problems. Test showed model parameters before and after sampling were identical. After much suffering, the problem was found to be caused by this inference call to the Diffusers pipeline I&rsquo;d cribbed from Huggingface:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>images</span> <span class=o>=</span> <span class=n>pipeline</span><span class=p>(</span><span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                  <span class=n>generator</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>seed</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                  <span class=n>num_inference_steps</span><span class=o>=</span><span class=n>num_steps</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                  <span class=p>)</span><span class=o>.</span><span class=n>images</span>
</span></span></code></pre></td></tr></table></div></div><p>Setting <code>generator=torch.manual_seed(config.seed)</code> resets the seed for the whole training loop, meaning the model will see training samples in the same order every reset. This allowed the model to learn the sample order as an unintended side effect, leading to overfitting and degraded performance. Setting a generator on the CPU instead allows the training to continue unmolested: <code>generator=torch.Generator(device='cpu').manual_seed(config.seed)</code></p></li></ol><p>Below is a video of characters generated during each training epoch from a model trained for 100 epochs on the same Chinese glyph dataset used before. Compared to the VAE model discussed in the last part, we can see the unconditional diffusion model is much better at capturing the hierarchical structure of characters at different levels, and learns this very early on in the training process.</p><center><video width=100% autoplay loop muted playsinline>
<source src=/unconditional_glyffuser_training.mp4 type=video/mp4>Your browser does not support the video tag.</video></center><p>The model also works on other writing styles such as <a href=#appendix-1-other-writing-styles>ancient Chinese seal script</a>!</p><h2 id=conditional-diffusion-model>Conditional diffusion model<a hidden class=anchor aria-hidden=true href=#conditional-diffusion-model>#</a></h2><p>Now that we have confirmed a diffusion model can generate convincing Chinese characters, let&rsquo;s train a text-to-image model conditioned on the English definitions of each character. If the model correctly learns how the English definition relates to the Chinese character, it should be able to &lsquo;understand&rsquo; the rules for how characters are constructed and generate characters that &lsquo;give the right vibe&rsquo; for a given english prompt.</p><style>.admonition{border-radius:5px;padding:0;border-left:5px solid #00bcf6;box-shadow:0 0 .5rem .2rem #00000025}.admonition-title-container{background-color:#00bcf6;border-top-right-radius:5px}.admonition-title{font-weight:bolder;font-size:large;backdrop-filter:grayscale(50%)brightness(150%);-webkit-backdrop-filter:grayscale(50%)brightness(150%);padding:5px 0 5px 30px;border-top-right-radius:5px}@media(prefers-color-scheme:dark){.admonition-title{backdrop-filter:grayscale(40%)brightness(40%);-webkit-backdrop-filter:grayscale(40%)brightness(40%)}}.admonition-content{padding:10px 0 10px 15px}</style><div class=admonition><div class=admonition-title-container><div class=admonition-title>Tip</div></div><div class=admonition-content>The concept of &lsquo;conditioning&rsquo; may seem mysterious but here it boils down to working out a way to represent the English text as a vector, then adding that vector to another vector representing the image during the training process.</div></div><p>In a previous blog post, I discussed finetuning Stable Diffusion. However that seemed like the wrong approach here - the pretraining of SD wouldn&rsquo;t do much for us since the types of images we want to generate are unlikely to be well represented in their training set. So which framework to use?</p><p>In a misguided attempt to save effort, I first tried Assembly AI&rsquo;s <a href=https://github.com/AssemblyAI-Examples/MinImagen>minimagen</a> implementation, as an ostensibly simple conditional diffusion framework. It rapidly (but not rapidly enough) became clear that even after extensive debugging this was not fully functional demo code. I moved on to lucidrains&rsquo; much cleaner <a href=https://github.com/lucidrains/imagen-pytorch>imagen</a> implementation and trained some variants in unconditional mode, matching architectures as best as I could with my previous model (within the constraints of the framework), but I couldn&rsquo;t never replicate the same quality - I suspect this was due to some differences in the layer architecture I couldn&rsquo;t identify.</p><p><em>&ldquo;You could not live with your own failure, where did that bring you? Back to me.&rdquo;</em></p><p>In the end I decided I had to implement the model myself, something I&rsquo;d been studiously avoiding. While huggingface&rsquo;s <code>UNet2DConditionModel()</code> provides a framework for this, I was unable to find good documentation and so ended up having to scour the codebase directly. My observations below on what is needed, in case you dear reader want to take up this foolhardy task. Follow along with the <a href=https://github.com/yue-here/glyffuser/blob/main/glyffuser%20conditional.ipynb>notebook here</a>.</p><p>The <code>UNet2DConditionModel()</code> is able to be conditioned on either discrete classes (dog, car, house, etc.), or embeddings (vectors from a latent space). For text-to-image, we are using embeddings. I decided to take a page out of Google&rsquo;s book and use LLM embeddings directly for conditioning, as Imagen does. What Google found with Imagen was that conditioning on text embeddings from an LLM such as their T5 model worked just as well or even better than the CLIP model trained on image-caption pairs that Dall-E uses. This is very handy for us, since pretrained CLIP-type embeddings would likely be inappropriate for our use case reasons discussed above. In practice, we can call down existing T5 methods from Huggingface&rsquo;s <code>transformers</code> library and pass it some text; it will give us back a fixed-length vector embedding, imbued with LLM magic.</p><p>Here&rsquo;s the core of the conditional UNet model:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>diffusers</span> <span class=kn>import</span> <span class=n>UNet2DConditionModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the conditional UNet model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>UNet2DConditionModel</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>sample_size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>in_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>out_channels</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>layers_per_block</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>block_out_channels</span><span class=o>=</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=mi>256</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=mi>512</span><span class=p>),</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>addition_embed_type</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=c1># Make it conditional</span>
</span></span><span class=line><span class=cl>    <span class=n>cross_attention_dim</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_hid_dim</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>  <span class=c1># the hidden dimension of the encoder</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder_hid_dim_type</span><span class=o>=</span><span class=s2>&#34;text_proj&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>down_block_types</span><span class=o>=</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;CrossAttnDownBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;DownBlock2D&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>up_block_types</span><span class=o>=</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;CrossAttnUpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;UpBlock2D&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>We specify the nature of the conditioning vector being passed to the model via some extra arguments. We also need to introduce some extra plumbing. First, we now need text captions for each training image, which we obtained from the unihan definitions database (way back at the <a href=#dataset>top of this article</a>). We then need a collator function that helps the dataloader return LLM embeddings from the text caption, as well as an attention mask for the embedding. Finally, we write our own pipeline, <code>GlyffuserPipeline()</code> (a subclass of <code>DiffusionPipeline()</code>) that can correctly pass the text embeddings and masks during inference. From there, we can train it in the same way as the previous model.</p><h2 id=learning-the-structure-of-chinese-characters>Learning the structure of Chinese characters<a hidden class=anchor aria-hidden=true href=#learning-the-structure-of-chinese-characters>#</a></h2><p>Now that we&rsquo;ve trained the conditional model, how can we know what the model has learnt about how Chinese characters are constructed? One way is to probe it with different prompts. In the <a href=#intro>intro</a> I mentioned that most characters contain semantic or phonetic radicals, and the <a href=#visualise-the-character-set-in-2d>UMAP</a> shows the distribution of major character types.</p><p>If we obtain a list of most common Chinese radicals from, say, an <a href=https://en.wikipedia.org/wiki/Kangxi_radical>18th century Chinese dictionary</a> we can probe the model with prompts corresponding to the English meaning of each radical and see what is activated. Below we see what happens using different numbers of sampling steps to approximate the denoising trajectory (NB: in the animation in the previous section, we show <em>training</em> steps but each frame is <em>sampled</em> for 50 steps. Don&rsquo;t get these confused!) for English text prompts corresponding to the most frequently appearing 36 Chinese radicals. (The radicals are not used in the prompt but shown in parentheses for reference):</p><center><figure><img loading=lazy src=/36%20common%20radicals%201%20step.png></figure></center><p>Very interesting! We see in most cases the corresponding radical is directly activated in the part of the character it usually resides, even though the rest of the character is very noisy. We can infer that the english prompt is strongly tied <em>semantically</em>, i.e. in meaning, to characters that contain this radical.</p><p>The 虫 radical means &ldquo;bug&rdquo; and is quite strongly activated by the prompt &lsquo;insect&rsquo; as it is used in bug-related characters (fly: 蝇, butterfly: 蝴蝶, earthworm: 蚯蚓). However, the 忄(heart) radical is not activated by &ldquo;heart&rdquo; (its literal meaning) as its <em>semantic</em> meaning is linked to the historical belief that thoughts and emotions arise from the physical heart (memory: 忆, afraid: 怕, fast: 快). Can we activate this radical by changing our prompt?</p><center><figure><img loading=lazy src=/steps%20probe.png></figure></center><p>The picture shows increasing the number of sampling steps on the same prompt. With 1 step, we see the activation of 忄 but as we increase the resolution of the sampling trajectory the preferred radical resolves to 心 at the bottom. If you can read Chinese, you&rsquo;ll already know why this is: 忄 is actually an alternative form of 心, which is standalone character for heart and can also be used as a bottom radical. Characters with this radical are also linked to thoughts and feelings, e.g. 念 (to miss) and 想 (to think).</p><p>Notably, we can&rsquo;t access the phonetic component in this model as we haven&rsquo;t included any information on Chinese pronunciation in the training process. Can we, however, generate new characters using semantic components? Let&rsquo;s try:</p><center><figure><img loading=lazy src=/a%20mental%20illness.png></figure></center><p>We can! We have both the components for &lsquo;sickness&rsquo; and &rsquo;thoughts/feelings&rsquo; in this character. The overall structure of this character likely draws from characters such as 痣 (zhì, a mole/mark on the skin) in the training set, but in 痣 the 志, also pronounced zhì, is a phonetic component and has no semantic significance. Of course, this is a cherry-picked example. What happens if we try to activate two radicals that prefer to live in the same place? If we provoke a conflict by prompting &ldquo;a sickness of fire&rdquo;, for example, we get this:</p><center><figure><img loading=lazy src=/a%20sickness%20of%20fire.png></figure></center><p>You can see that the 1 step denoising tries to place us in the 火 radical part of the data distribution, suggesting that might be the strongest semantic adherence in the prompt. With 3 steps, we also see the presence of a partial sickness radical 疒. The fact that the fire and sickness radicals share a large stroke in the training font suggest they are close in the data distribution and the low resolution sampling has landed somewhere in the middle. With more extensive sampling, however, neither concept is strong enough to dominate so the model navigates toward enforcing some local structure out of the noise, with the overall character looking not at all convincing.</p><p>Now that we understand better how the model works, let&rsquo;s take another look at previous grid and take it all the way up to 50 sampling steps. You may wonder why we never generate the base character, always the radical. The nature of our training set means that there are many more semantically-related training samples containing the radical compared to the one original character, so it makes sense that model would prefer to place the radical in the appropriate position. Sample weighting by corpus frequency would be one way of getting around this! We also see the the rest of the character is filled with what appears to be random stuff. I suspect this is again the model enforcing local consistency without particularly strong semantic guidance.</p><center><figure><img loading=lazy src=/36%20common%20radicals%2050%20step.png></figure></center><p>I tested an implementation including phonetic radical embeddings but there are some structural reason it did not work well - I discuss this <a href=#appendix-2-phonetic-radicals>here</a>.</p><h2 id=classifier-free-guidance-cfg-for-the-glyffuser>Classifier-free guidance (CFG) for the glyffuser<a hidden class=anchor aria-hidden=true href=#classifier-free-guidance-cfg-for-the-glyffuser>#</a></h2><p><a href=https://arxiv.org/abs/2207.12598>Classifier-free guidance</a> is an elegant and powerful technique that is now used in basically all production conditional diffusion models to improve adherence to prompts. (For an excellent treatment, see <a href=https://sander.ai/2022/05/26/guidance.html>here</a>)</p><p>Essentially, this method allows the strength of any given prompt to be varied without needing to perform any additional training. Moreover, the strength of the prompt can be increased far above that for standard conditional training. Can we use this method get better generations from the glyffuser?</p><p>To implement this method, we simply add random dropout of the text conditioning tokens during training (10-20% has been found to work well). This effectively trains an unconditional model at the same time. During sampling steps, we then perform the noise prediction twice, once normally and once with a blank conditioning tensor. We then combine them as follows:</p><p><code>noise_prediction = noise_prediction_unconditional + guidance_scale * (noise_prediction - noise_prediction_unconditional)</code></p><div class=admonition><div class=admonition-title-container><div class=admonition-title>Tip</div></div><div class=admonition-content>At <code>guidance_scale = 0</code>, the model acts as an unconditional model while at <code>guidance_scale = 1</code>, the model acts as the standard conditional model</div></div><p>Generally, increasing <code>guidance_scale</code> in text-to-image models decreases variety while increasing adherence to the prompt. Let&rsquo;s try probing the model by varying the number of sampling steps and guidance scale for the prompt &ldquo;bird&rdquo; corresponding to a very common radical (鳥/鸟):</p><center><figure><img loading=lazy src=/cfg-steps%20grid.png></figure></center><div class=admonition><div class=admonition-title-container><div class=admonition-title>Tip</div></div><div class=admonition-content>Unusually, the &ldquo;bird&rdquo; radical can occur on either the left (&ldquo;鸵&rdquo;, ostrich) or right (&ldquo;鸡&rdquo;, chicken) sides of characters.</div></div><p>I&rsquo;ve also include interactive explorers for the prompts <a href=#appendix-3-cfg-variations-for-fire>&lsquo;fire&rsquo;</a> and <a href=#appendix-4-cfg-variations-for-hair>&lsquo;hair&rsquo;</a> in the appendices. I highly recommend taking a look, they are very funny!</p><p>Compared to the base conditional model, we see that as we increase the guidance scale, the &lsquo;bird&rsquo; radical becomes increasingly activated with even very few sampling steps. Interestingly, while the traditional form of the bird character &ldquo;鳥&rdquo; dominates (it is more prevalent in the training set), the simplified form &ldquo;鸟&rdquo; also makes a single appearance (10 steps, scale=50), making it a &rsquo;transition state&rsquo; during the denoising process. The explorer below shows CFG scales of 0 to 100 for different random seeds - higher CFG scales do indeed reduce sample variety. Compared to general-purpose text-to-image models however, we can tolerate higher CFG scales as they tend to give more convincing characters. If you follow any individual character, you&rsquo;ll see that it tends to start with one &lsquo;bird&rsquo; radical, then as CFG scale increases, at some point the other side will also collapse to a &lsquo;bird&rsquo; radical.</p><div class="slider-container unique-slider-container1"><button id=autoplayButton1 class=autoplayButton>Pause</button>
<span id=sliderValue1 class=sliderValue>CFG scale 1</span>
<input type=range min=0 max=99 step=1 value=1 id=parameterSlider1 class=parameterSlider></div><img id=outputImage1 class=outputImage src=/fire_slider/fire-CFG0_grid.png alt="Model Output"><p>Looking at the a picture of varying CFG scale generations for prompts for each common radical below, we see that for scale values of around 5-15, the generated characters are much more consistently convincing than the those made by the base conditional or unconditional models. (I think that anyone who grew up with Chinese or has studied it for a long time gains a strong instinctive sense of whether a character could be real, even if they don&rsquo;t recognise it.)</p><center><figure><img loading=lazy src=/guidance_scale_grid.png></figure></center><h2 id=outro>Outro<a hidden class=anchor aria-hidden=true href=#outro>#</a></h2><p>So can we teach a model to understand how Chinese characters and then invent new ones? It looks the answer is a resounding yes! We find that the final conditioned diffusion model (the Glyffuser™) has a strong conception of how the components of a Chinese character relate to its meaning, in much the same way a human would guess at the meaning of an unknown character. Moreover, by strengthening the adherence to known concepts using classifier-free guidance, we can generate very convincing characters that follow the rules of Chinese glyph construction.</p><br><br><br><br><h2 id=test-the-models-here>Test the models here<a hidden class=anchor aria-hidden=true href=#test-the-models-here>#</a></h2><p>If you want to try out the conditional glyffuser, the easiest way is to get the demo repo with <code>git clone https://github.com/yue-here/glyffuser</code>, make a python environment with <code>requirements.txt</code> (you may need to set it up for your GPU and jupyter) then run the <code>glyffuser inference.ipynb</code> notebook, which will summon the trained model from my huggingface repo.</p><p>Failing that, I&rsquo;ve made a gradio <a href=https://huggingface.co/spaces/yuewu/glyffuser>applet</a> on Hugging Face that will run the inference, but please be patient as it&rsquo;s quite slow (around a minute per step).</p><h2 id=appendices>Appendices<a hidden class=anchor aria-hidden=true href=#appendices>#</a></h2><h3 id=appendix-1-other-writing-styles>Appendix 1: other writing styles<a hidden class=anchor aria-hidden=true href=#appendix-1-other-writing-styles>#</a></h3><p>Here&rsquo;s a training video from a version of the glyffuser trained on the ancient Chinese writing known as <a href=https://en.wikipedia.org/wiki/Seal_script>seal script</a>:</p><center><video width=100% autoplay loop muted playsinline>
<source src=/sealscript_training.mp4 type=video/mp4>Your browser does not support the video tag.</video></center><h3 id=appendix-2-phonetic-radicals>Appendix 2: phonetic radicals<a hidden class=anchor aria-hidden=true href=#appendix-2-phonetic-radicals>#</a></h3><p>To include the phonetic radicals, we need a representation. The standard romanization is known as <a href=https://en.wikipedia.org/wiki/Pinyin>pinyin</a> - for example, the phrase 人工智能 (artificial intelligence) would be &ldquo;rén gōng zhì néng&rdquo;. The diacritics on the vowels are the tones of which there are 4 in standard mandarin. These can also be represented numerically, e.g. &ldquo;ren2 gong1 zhi4 neng2&rdquo;.</p><p>Simply adding the pinyin to the english definition prior to training is unlikely to work as we are using a frozen tokenizer and text model which does not recognize the pinyin syllables. Instead, we can create separate embeddings of the same length as the image vector for the pinyin and add them directly, the same as the text conditioning is added. Specifically, based on how Chinese syllables are <a href=https://en.wikipedia.org/wiki/Pinyin_table>structured</a>, I added 3 trainable embeddings for the initial, final, and tone (for example, ren2 would become &lsquo;r&rsquo;, &rsquo;en&rsquo; and &lsquo;2&rsquo;). To train the model, I added extra inputs for the pinyin and tone.</p><p>After training the model in the same way as before, I tried to &lsquo;summon&rsquo; the phonetic components. However, the results only changed slightly with different pinyin prompts. I suspect this is because of the excessive homophony in Chinese. For example, 羊 (yang2, &lsquo;goat/sheep&rsquo;) is a common phonetic radical. But for this exact syllable, wiktionary gives <a href=https://en.wiktionary.org/wiki/y%C3%A1ng>47 different characters</a>. Have a look at characters containing the top two phonetic radicals for this pronunciation, 羊 and 昜:</p><p>羊佯垟徉样洋烊珜眻蛘𨦡羏劷<br>昜崵揚暘楊湯敭瑒煬禓瘍諹輰鍚鐊陽霷颺鰑鸉</p><p>It&rsquo;s likely the same situation as the <a href=#conditional-diffusion-model>&lsquo;clashing radicals&rsquo; case</a> where we can&rsquo;t activate the &lsquo;fire&rsquo; and &lsquo;sickness&rsquo; radicals at same time - when neither phonetic radical dominates the distribution, we end up sampling garbage.</p><h3 id=appendix-3-cfg-variations-for-fire>Appendix 3: CFG variations for &ldquo;fire&rdquo;<a hidden class=anchor aria-hidden=true href=#appendix-3-cfg-variations-for-fire>#</a></h3><p>The Chinese character for fire &ldquo;火&rdquo; has a particularly varied set of possible locations. These are showcased in the characters &ldquo;炎&rdquo; and &ldquo;焱&rdquo;. Another form is the bottom radical &ldquo;灬&rdquo;, a kind of deconstructed version of &ldquo;火&rdquo;. As such, greater variety is possible and this shows:</p><div class="slider-container unique-slider-container2"><button id=autoplayButton2 class=autoplayButton>Pause</button>
<span id=sliderValue2 class=sliderValue>CFG scale 1</span>
<input type=range min=0 max=99 step=1 value=1 id=parameterSlider2 class=parameterSlider></div><img id=outputImage2 class=outputImage src=/bird_slider/bird-CFG0_grid.png alt="Model Output"><h3 id=appendix-4-cfg-variations-for-hair>Appendix 4: CFG variations for &ldquo;hair&rdquo;<a hidden class=anchor aria-hidden=true href=#appendix-4-cfg-variations-for-hair>#</a></h3><p>I&rsquo;m mostly including this because the characters look very funny.</p><div class="slider-container unique-slider-container3"><button id=autoplayButton3 class=autoplayButton>Pause</button>
<span id=sliderValue3 class=sliderValue>CFG scale 1</span>
<input type=range min=0 max=99 step=1 value=1 id=parameterSlider3 class=parameterSlider></div><img id=outputImage3 class=outputImage src=/hair_slider/hair-CFG0_grid.png alt="Model Output"></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yue-here.com/posts/diffusion/><span class=title>« Prev</span><br><span>A visual guide to how diffusion models work</span>
</a><a class=next href=https://yue-here.com/posts/opticalflow/><span class=title>Next »</span><br><span>Optical flow timelapse stabiliser</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yue-here.com/>Yue Wu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>