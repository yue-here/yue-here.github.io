<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Optical flow timelapse stabiliser | Yue Wu</title>
<meta name=keywords content><meta name=description content="For associated code, please see the Jupyter notebook in the github repository
While machine learning has been very successful in image-processing applications, there&rsquo;s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I&rsquo;ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn&rsquo;t rely on specific feature tracking."><meta name=author content="Yue Wu"><link rel=canonical href=https://yue-here.github.io/posts/opticalflow/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://yue-here.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yue-here.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yue-here.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://yue-here.github.io/apple-touch-icon.png><link rel=mask-icon href=https://yue-here.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yue-here.github.io/posts/opticalflow/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yue-here.github.io/posts/opticalflow/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="Optical flow timelapse stabiliser"><meta property="og:description" content="For associated code, please see the Jupyter notebook in the github repository
While machine learning has been very successful in image-processing applications, there’s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I’ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn’t rely on specific feature tracking."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-14T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Optical flow timelapse stabiliser"><meta name=twitter:description content="For associated code, please see the Jupyter notebook in the github repository
While machine learning has been very successful in image-processing applications, there&rsquo;s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I&rsquo;ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn&rsquo;t rely on specific feature tracking."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yue-here.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Optical flow timelapse stabiliser","item":"https://yue-here.github.io/posts/opticalflow/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Optical flow timelapse stabiliser","name":"Optical flow timelapse stabiliser","description":"For associated code, please see the Jupyter notebook in the github repository\nWhile machine learning has been very successful in image-processing applications, there\u0026rsquo;s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I\u0026rsquo;ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn\u0026rsquo;t rely on specific feature tracking.\n","keywords":[],"articleBody":"For associated code, please see the Jupyter notebook in the github repository\nWhile machine learning has been very successful in image-processing applications, there’s still a place for traditional computer vision techniques. One of my hobbies is making timelapse videos by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I’ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn’t rely on specific feature tracking.\nA short clip of unstabilized footage Background I had a vague conception that tracking a similarity metric between two frames would furnish the shift between then, perhaps using least-squares peak fitting in a similar way to the fitting of X-ray diffraction data (once you have a hammer, everything looks like a nail!) by performing gradient descent with small shifts to the two frames.\nTo my great relief I didn’t have to write this code as further investigation revealed OpenCV contains a feature called optical flow which essentially performs this calculation, outputting vectors for the movement of every pixel between two frames. This could form the basis of stabilisation code.\nExecution: optical flow interpretation My exploratory calculations showed that the optical flow vectors did indeed provide a clear picture of shift direction and magnitude.\nOptical flow analysis of a frame transition with a small shift downward You can see in the maps above the algorithm detects pixels that can be mapped to a clear shift (furniture etc.) while uniform areas like wallpaper aren’t picked up. The regions of uniform grey in the “flow magnitude” plot indicate a consistent shift, and the sharp peak in the histogram of shifts confirms this - the camera view has tilted by around 16 pixels. Likewise for the “flow angle plot”, the peak in the histogram at ~90° indicates a shift downwards.\nThe next step was to work out how to use these results. It was clear that this was already a relatively expensive calculation and it would have to be performed on ~18,000 pairs of frames for my test timelapse dataset.\n(Aside: luckily, OpenCV has easily accessible GPU acceleration with OpenCL. Simply replacing standard matrices with UMat objects speeds up the calculations by an order of magnitude. I looked into using the CUDA implementation which would likely be even faster, but it seemed more trouble than it was worth to install.)\nWhile peak fitting the histograms would yield the best result in principle, I opted for an economical approach to save compute: filter out magnitudes that were the first entry in the array (i.e. the top of the decay curve, not a peak), only include angles within 5 of 90/270 degrees (i.e. vertical shifts), then take the max values and round the angles to 90 or 270.\nThis part was done as a single script outputting a CSV file of the 4 relevant parameters extracted from histograms for each frame - vector magnitude, vector angle, and the frequencies of each . (In principle the full optical flow vector set could be saved but file would be huge. The 17K frame dataset took around 90 min on my system which was an acceptable tradeoff for repeats.)\nExecution: stabilisation This part was simple in principle but somewhat difficult in practice. The idea was very simple - by cropping the top and bottom of each frame by the maximum total shift across the dataset, the relative shift of each frame could be controlled by varying the shifts above and below.\nThe execution: Calculate the relative shift from the starting position at for each frame of the dataset by summing the up and down shifts. The overall window size across the set can be extrapolated by the distance between the maximum and minimum shift. Then for the starting frames, crop down to the window size. Afterwards, perform the first crop plus the relative shift calculated previously to shift the visible window to a consistent location. It’s difficult to explain clearly but the code and demonstration make it clearer.\nThe unstabilized footage from above\rThe optical flow stabilized footage\r","wordCount":"737","inLanguage":"en","datePublished":"2024-02-14T00:00:00Z","dateModified":"2024-02-14T00:00:00Z","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yue-here.github.io/posts/opticalflow/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"https://yue-here.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yue-here.github.io/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yue-here.github.io/index.html title=Home><span>Home</span></a></li><li><a href=https://yue-here.github.io/bio/ title=Bio><span>Bio</span></a></li><li><a href=https://yue-here.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yue-here.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://yue-here.github.io/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Optical flow timelapse stabiliser</h1><div class=post-meta><span title='2024-02-14 00:00:00 +0000 UTC'>February 14, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;737 words&nbsp;·&nbsp;Yue Wu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#execution-optical-flow-interpretation>Execution: optical flow interpretation</a></li><li><a href=#execution-stabilisation>Execution: stabilisation</a></li></ul></nav></div></details></div><div class=post-content><p>For associated code, please see the <a href=https://github.com/yue-here/optical-flow-stabilizer/blob/main/optical%20flow%20timelapse%20stabilizer%20tutorial.ipynb>Jupyter notebook</a> in the <a href=https://github.com/yue-here/optical-flow-stabilizer>github repository</a></p><p>While machine learning has been very successful in image-processing applications, there&rsquo;s still a place for traditional computer vision techniques. One of my hobbies is making <a href=https://youtu.be/SA69YDp-wbg>timelapse videos</a> by taking photos every 10 minutes for many days, weeks or even months. Over this time scale, environmental effects such as thermal expansion from the day-night cycle can introduce period offsets into the footage. I have a backlog of timelapse footage that is unwatchable due to excessive shifts of this nature. In the past I&rsquo;ve used the Blender VFX stabilization tool to stabilize on a feature, but this breaks in e.g. day-night cycles or subjects moving in front of the feature. I finally got round to writing my code to do this that doesn&rsquo;t rely on specific feature tracking.</p><center><figure><img loading=lazy src=/unstabilized.gif><figcaption>A short clip of unstabilized footage</figcaption></figure></center><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>I had a vague conception that tracking a similarity metric between two frames would furnish the shift between then, perhaps using least-squares peak fitting in a similar way to the fitting of X-ray diffraction data (once you have a hammer, everything looks like a nail!) by performing gradient descent with small shifts to the two frames.</p><p>To my great relief I didn&rsquo;t have to write this code as further investigation revealed OpenCV contains a feature called <a href=https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html>optical flow</a> which essentially performs this calculation, outputting vectors for the movement of every pixel between two frames. This could form the basis of stabilisation code.</p><h2 id=execution-optical-flow-interpretation>Execution: optical flow interpretation<a hidden class=anchor aria-hidden=true href=#execution-optical-flow-interpretation>#</a></h2><p>My exploratory calculations showed that the optical flow vectors did indeed provide a clear picture of shift direction and magnitude.</p><center><figure><img loading=lazy src=/optical_flow_exploration.png><figcaption>Optical flow analysis of a frame transition with a small shift downward</figcaption></figure></center><p>You can see in the maps above the algorithm detects pixels that can be mapped to a clear shift (furniture etc.) while uniform areas like wallpaper aren&rsquo;t picked up. The regions of uniform grey in the &ldquo;flow magnitude&rdquo; plot indicate a consistent shift, and the sharp peak in the histogram of shifts confirms this - the camera view has tilted by around 16 pixels. Likewise for the &ldquo;flow angle plot&rdquo;, the peak in the histogram at ~90° indicates a shift downwards.</p><p>The next step was to work out how to use these results. It was clear that this was already a relatively expensive calculation and it would have to be performed on ~18,000 pairs of frames for my test timelapse dataset.</p><p>(Aside: luckily, OpenCV has easily accessible GPU acceleration with <a href=https://opencv.org/opencl/>OpenCL</a>. Simply replacing standard matrices with UMat objects speeds up the calculations by an order of magnitude. I looked into using the CUDA implementation which would likely be even faster, but it seemed more trouble than it was worth to install.)</p><p>While peak fitting the histograms would yield the best result in principle, I opted for an economical approach to save compute: filter out magnitudes that were the first entry in the array (i.e. the top of the decay curve, not a peak), only include angles within 5 of 90/270 degrees (i.e. vertical shifts), then take the max values and round the angles to 90 or 270.</p><p>This part was done as a single script outputting a CSV file of the 4 relevant parameters extracted from histograms for each frame - vector magnitude, vector angle, and the frequencies of each . (In principle the full optical flow vector set could be saved but file would be huge. The 17K frame dataset took around 90 min on my system which was an acceptable tradeoff for repeats.)</p><h2 id=execution-stabilisation>Execution: stabilisation<a hidden class=anchor aria-hidden=true href=#execution-stabilisation>#</a></h2><p>This part was simple in principle but somewhat difficult in practice. The idea was very simple - by cropping the top and bottom of each frame by the maximum total shift across the dataset, the relative shift of each frame could be controlled by varying the shifts above and below.</p><p>The execution: Calculate the relative shift from the starting position at for each frame of the dataset by summing the up and down shifts. The overall window size across the set can be extrapolated by the distance between the maximum and minimum shift. Then for the starting frames, crop down to the window size. Afterwards, perform the first crop plus the relative shift calculated previously to shift the visible window to a consistent location. It&rsquo;s difficult to explain clearly but the code and demonstration make it clearer.</p><div style=display:flex;justify-content:space-around;align-items:center><div><figure><img src=/unstabilized.gif alt="A short clip of unstabilized footage"><figcaption>The unstabilized footage from above</figcaption></figure></div><div><figure><img src=/stabilized.gif alt="The stabilized footage"><figcaption>The optical flow stabilized footage</figcaption></figure></div></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yue-here.github.io/posts/glyffuser/><span class=title>« Prev</span><br><span>Teaching an AI to invent new Chinese characters</span>
</a><a class=next href=https://yue-here.github.io/posts/chemicaldiffusion/><span class=title>Next »</span><br><span>Chemical Diffusion</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://yue-here.github.io/>Yue Wu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>