<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Training T5 models and generating text | Yue Wu</title>
<meta name=keywords content><meta name=description content="For the language-to-language components of this JACS does not exist, I chose Google&rsquo;s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model.
I had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE.
As abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5&rsquo;s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here&rsquo;s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer."><meta name=author content="Yue Wu"><link rel=canonical href=https://yue-here.com/posts/t5/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://yue-here.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yue-here.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yue-here.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yue-here.com/apple-touch-icon.png><link rel=mask-icon href=https://yue-here.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yue-here.com/posts/t5/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yue-here.com/posts/t5/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="Training T5 models and generating text"><meta property="og:description" content="For the language-to-language components of this JACS does not exist, I chose Google’s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model.
I had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE.
As abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5’s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here’s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-25T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-25T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Training T5 models and generating text"><meta name=twitter:description content="For the language-to-language components of this JACS does not exist, I chose Google&rsquo;s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model.
I had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE.
As abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5&rsquo;s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here&rsquo;s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yue-here.com/posts/"},{"@type":"ListItem","position":2,"name":"Training T5 models and generating text","item":"https://yue-here.com/posts/t5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training T5 models and generating text","name":"Training T5 models and generating text","description":"For the language-to-language components of this JACS does not exist, I chose Google\u0026rsquo;s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model.\nI had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE.\nAs abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5\u0026rsquo;s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here\u0026rsquo;s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer.\n","keywords":[],"articleBody":"For the language-to-language components of this JACS does not exist, I chose Google’s T5 (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model.\nI had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE.\nAs abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5’s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here’s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer.\n1 2 3 tokenizer = AutoTokenizer.from_pretrained('t5-base') abs_lengths = df['Abstract'].map(lambda x: len(tokenizer(x)['input_ids'])) abs_lengths.hist(bins = 100) It seems that the vast majority of abstracts are within the 512 token limit, so I didn’t do any further preprocessing. Note - unlike the GPT-2 tokenizer discussed in a previous post, the T5 tokenizer has separate tokens for padding and end-of-sequence, so we don’t need change anything.\nTo fine-tune the base T5 model, I used a standard Huggingface sequence-to-sequence trainer structure with a pytorch dataset, similar to the one I used for the vision encoder-decoder model discussed in a previous post.\nFor my previous text generators toc2title a simple model.generate() call was sufficient while title2abstract worked well after adding no_repeat_ngram_size=2.\nHowever, for abstract2title, I wanted to generate multiple distinct but convincing title suggestions from an abstract - essentially a summarization task. After finetuning, the simplest generation methods (greedy and beam search) resulted in very similar suggestions each time. Instead, I used top-K and top-p sampling to generate more distinct and surprising text. This is easily implemented with the top_k and top_p arguments to the generate() method:\n1 2 3 4 5 6 7 8 generated_ids = model.generate( input_ids, max_length=128, num_return_sequences=3, do_sample=True, top_k=5, top_p=0.95, early_stopping=True) If we take the example of my own first paper, you can see that beam search generates very similar results while even with a very limited amount of text, the sampling methods give much more diversity.\nBeam search Top-K + top-p sampling 1. Negative Thermal Expansion in a Metal–Organic Framework 1. Negative Thermal Expansion in a Metal–Organic Framework 2. Negative Thermal Expansion in a Metal–Organic Framework Lattice 2. Dynamics of Metal–Organic Frameworks 3. Negative Thermal Expansion in Metal–Organic Frameworks 3. Effect of Metal–Organic Framework Contraction on Negative Thermal Expansion (Abstract: The action behind contraction: The metal–organic framework [Cu 3 (btc) 2] displays negative thermal expansion (NTE) over a broad temperature range. This property arises from two coincident mechanisms, each of which are unique for NTE systems: the concerted transverse vibration of triangular organic linkers, and the local dynamic distortion of dinuclear metal centers within the framework lattice.)\nThe real title? Negative Thermal Expansion in the Metal-Organic Framework Material Cu3(1,3,5-benzenetricarboxylate)2\n","wordCount":"465","inLanguage":"en","datePublished":"2022-08-25T00:00:00Z","dateModified":"2022-08-25T00:00:00Z","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yue-here.com/posts/t5/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"https://yue-here.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yue-here.com/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yue-here.com/index.html title=Home><span>Home</span></a></li><li><a href=https://yue-here.com/bio/ title=Bio><span>Bio</span></a></li><li><a href=https://yue-here.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yue-here.com/research/ title=Research><span>Research</span></a></li><li><a href=https://yue-here.com/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Training T5 models and generating text</h1><div class=post-meta><span title='2022-08-25 00:00:00 +0000 UTC'>August 25, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;465 words&nbsp;·&nbsp;Yue Wu</div></header><div class=post-content><p>For the language-to-language components of <a href=https://thisjacsdoesnotexist.com>this JACS does not exist</a>, I chose Google&rsquo;s <a href=https://huggingface.co/docs/transformers/v4.21.2/en/model_doc/t5>T5</a> (text-to-text transfer transformer) as a recent cutting-edge text sequence to sequence model.</p><p>I had already scraped all the JACS titles and abstracts, so training data was readily available. The first task was to generate somewhat-convincing abstracts from titles to increase the entertainment value of TJDNE.</p><p>As abstracts have a maximum length, I wanted to make sure that a whole abstract would be included in T5&rsquo;s maximum input length of 512 tokens so that end-of-sequence locations could be determined. Here&rsquo;s a histogram of the length distribution of the abstracts tokenized with the T5 tokenizer.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;t5-base&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>abs_lengths</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=s1>&#39;Abstract&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokenizer</span><span class=p>(</span><span class=n>x</span><span class=p>)[</span><span class=s1>&#39;input_ids&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl><span class=n>abs_lengths</span><span class=o>.</span><span class=n>hist</span><span class=p>(</span><span class=n>bins</span> <span class=o>=</span> <span class=mi>100</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><center><p><img alt="Abstract token length" loading=lazy src=/abs_lengths.png></p></center><p>It seems that the vast majority of abstracts are within the 512 token limit, so I didn&rsquo;t do any further preprocessing. Note - unlike the GPT-2 tokenizer discussed in a previous post, the T5 tokenizer has separate tokens for padding and end-of-sequence, so we don&rsquo;t need change anything.</p><p>To fine-tune the base T5 model, I used a standard Huggingface sequence-to-sequence trainer structure with a pytorch dataset, similar to the one I used for the vision encoder-decoder model discussed in a previous post.</p><p>For my previous text generators <a href=/posts/thisjacsdoesnotexist/#toc2title>toc2title</a> a simple <code>model.generate()</code> call was sufficient while <a href=/posts/thisjacsdoesnotexist/#title2abstract>title2abstract</a> worked well after adding <code>no_repeat_ngram_size=2</code>.</p><p>However, for <a href=/posts/abstract2title/>abstract2title</a>, I wanted to generate multiple distinct but convincing title suggestions from an abstract - essentially a summarization task. After finetuning, the simplest generation methods (greedy and beam search) resulted in very similar suggestions each time. Instead, I used <a href=https://huggingface.co/blog/how-to-generate>top-K and top-p sampling</a> to generate more distinct and surprising text. This is easily implemented with the <code>top_k</code> and <code>top_p</code> arguments to the <code>generate()</code> method:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>input_ids</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>max_length</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>num_return_sequences</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>do_sample</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_k</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>top_p</span><span class=o>=</span><span class=mf>0.95</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>early_stopping</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>If we take the example of my own first paper, you can see that beam search generates very similar results while even with a very limited amount of text, the sampling methods give much more diversity.</p><table><thead><tr><th>Beam search</th><th>Top-K + top-p sampling</th></tr></thead><tbody><tr><td>1. Negative Thermal Expansion in a Metal–Organic Framework</td><td>1. Negative Thermal Expansion in a Metal–Organic Framework</td></tr><tr><td>2. Negative Thermal Expansion in a Metal–Organic Framework Lattice</td><td>2. Dynamics of Metal–Organic Frameworks</td></tr><tr><td>3. Negative Thermal Expansion in Metal–Organic Frameworks</td><td>3. Effect of Metal–Organic Framework Contraction on Negative Thermal Expansion</td></tr></tbody></table><p>(Abstract: <em>The action behind contraction: The metal–organic framework [Cu 3 (btc) 2] displays negative thermal expansion (NTE) over a broad temperature range. This property arises from two coincident mechanisms, each of which are unique for NTE systems: the concerted transverse vibration of triangular organic linkers, and the local dynamic distortion of dinuclear metal centers within the framework lattice.</em>)</p><p>The real title? <em>Negative Thermal Expansion in the Metal-Organic Framework Material Cu3(1,3,5-benzenetricarboxylate)2</em></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yue-here.com/posts/chemicaldiffusion/><span class=title>« Prev</span><br><span>Chemical Diffusion</span>
</a><a class=next href=https://yue-here.com/posts/abstract2title/><span class=title>Next »</span><br><span>Abstract2title</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://yue-here.com/>Yue Wu</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>