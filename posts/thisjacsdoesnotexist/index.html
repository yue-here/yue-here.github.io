<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>This JACS does not exist | Yue Wu</title>
<meta name=keywords content><meta name=description content="

     
            An imaginary abstract generated at thisJACSdoesnotexist.com
        



In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee)
In general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train:"><meta name=author content="Yue Wu"><link rel=canonical href=https://yue-here.com/posts/thisjacsdoesnotexist/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=https://yue-here.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yue-here.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yue-here.com/favicon-32x32.png><link rel=apple-touch-icon href=https://yue-here.com/apple-touch-icon.png><link rel=mask-icon href=https://yue-here.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yue-here.com/posts/thisjacsdoesnotexist/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://yue-here.com/posts/thisjacsdoesnotexist/"><meta property="og:site_name" content="Yue Wu"><meta property="og:title" content="This JACS does not exist"><meta property="og:description" content=" An imaginary abstract generated at thisJACSdoesnotexist.com In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee)
In general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-01T00:56:40-07:00"><meta property="article:modified_time" content="2022-08-01T00:56:40-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="This JACS does not exist"><meta name=twitter:description content="

     
            An imaginary abstract generated at thisJACSdoesnotexist.com
        



In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee)
In general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yue-here.com/posts/"},{"@type":"ListItem","position":2,"name":"This JACS does not exist","item":"https://yue-here.com/posts/thisjacsdoesnotexist/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"This JACS does not exist","name":"This JACS does not exist","description":" An imaginary abstract generated at thisJACSdoesnotexist.com In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee)\nIn general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train:\n","keywords":[],"articleBody":" An imaginary abstract generated at thisJACSdoesnotexist.com In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. TOC ROFL, of which I am a multiple inductee)\nIn general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (JACS) as a dataset to train:\nA generative adversarial network (StyleGAN3) - a model which learns to generate images similar to those in its training set. A finetuned version of GPT-2 that generates chemistry-paper-title-like lists of words. A vision encoder-decoder model (test it out here) that converts images to the appropriate text - here, the above GPT-2 model acts as the text generator. A T5 sequence-to-sequence model (test it out here) that generates a text sequence from a prompt. For those who are interested in the inner workings or may want to do something similar, I‚Äôve included a writeup of the project below. Examples of the code are available on my GitHub. As you‚Äôll see, a lot of my choices were fairly abitrary, arising more-or-less from the availability heuristic.\nTraining the GAN Getting the dataset ready My initial idea was simply to see how well a GAN could generate ToC images, and so I needed a whole bunch of ToC images. I used the popular python package beautifulsoup with a headless browser and some simple loops to scrape the JACS website for the ToC images, titles and abstracts. The full dataset from around 60,000 papers could be collected overnight. Note: this was relatively easy with JACS (and ACS journals in general) as the website organisation follows a logical structure. YMMV for other publishers‚Äô platforms.\nGenerally, ML training inputs for image models should be the same size, often with powers of 2 as values - StyleGAN3 uses square images of size 128, 256, 512, and 1024 pixels. Luckily, JACS ToC images are fixed to 500 pixels in width and maximum height. I padded the figures to 512x512 using Irfanview‚Äôs batch processing tool, then resized those images to generate smaller datasets at 128 and 256 px as well.\nRunning StyleGAN3 I chose StyleGAN3 as I had heard of it before and 3 was the latest version number. At the time of writing it appears to be one of the best \u0026 most mature GANs for image generation. The implementation is fairly user-friendly and includes various data preparation and training scripts; the documentation is excellent and deserves commendation.\nThe first step was to use the dataset_tool.py script to prepare a dataset in a form the model could use - something like this:\n1 python dataset_tool.py --source=\"C:\\...\\512px\" --dest=\"C:\\...\\512px.zip\" After this, it‚Äôs straightforward to train the model using the following script:\n1 python train.py --outdir=\"C:\\...\\training_runs\" --cfg=stylegan3-t --data=\"C:\\...\\512px.zip\" --gpus=1 --batch=32 --gamma=8 --batch-gpu=8 --snap=10 --cbase=16384 The key parameters that need to be set are batch size and gamma, which is the R1 regularization parameter. ‚Äòcbase=16384‚Äô speeds up training by sacrificing network capacity, but at low resolutions I didn‚Äôt find it to be a problem. Having only a humble RTX 3060 Ti (the best I could get during the GPU apocalypse of 2020-2021), the meagre 8 GB of VRAM was a limiting factor for batch sizes. I also found useful the ‚Äìresume flag, which allows you to resume training from a previous network snapshot in case of crashes, although I found I also had to edit the resume_kimgs variable in the training script directly to get it to work properly. The tensorbard output is also very good, and can be run like so:\n1 tensorboard --logdir \"C:\\...\\\" I considered training on cloud compute but seemed to be more expensive than training locally, and also required learning a new thing. In retrospect this might have been a poor life choice. My wife was still annoyed that our power bill doubled during the month I was training though! Also that the computer was essentially a fan heater running in the bedroom during the height of summer. After bluescreening a couple of times I removed the computer side panel which dropped temps by a couple of degrees.\nFakes generated from a model trained on 128 px images\nI ran a first test with a 128 px as proof of concept, which was convincing enough that I scaled up to 256 px. In principle resolutions of up to 1024 px are available, but the raw data resolution mean that going above 512 was meaningless, and compute requirements made that unfeasible on my home system. After training continuously for a few weeks, the 256 px model started to converge. In this case I used a common evaluation metric for GANs called the Fr√©chet Inception Distance (FID), a measure of similarity between image distributions. Below is a video of images generated from the model as it trains - each successive frame, the models has trained on an additional 40,000 images. Overall, the model saw images from the training set around 5 million times.\nGAN training progress video Generating fake ToC images With the GAN model trained, it was time to generate the fake ToC images. This is done very simply with this StyleGAN script:\n1 python gen_images.py --outdir=\"C:\\...\\output\" --trunc=0.8 --seeds=0-50000 --network=\"C:\\...\\network-snapshot-....pkl\" The key parameters here are œà (‚Äútrunc‚Äù), which controls the tradeoff between image quality and weirdness (higher values are weirder). Some examples below:\nœà = 0.2 œà = 0.8 œà = 1.1 Assembling a ToC to title model With generated ToC images in hand, I wondered if it was possible to generate relevant paper titles. Having labels in the form of titles for all the ToCs in the training set, the challenge was finding an appropriate model. After trying a couple of things, I settled on using ü§ó Hugging Face‚Äôs transformers library, mostly due to its good documentation and straightforward code structuring.\nI used the ü§ó vision encoder decoder model as the basis for the toc-to-title model. This model can easily be warm-started with a pretrained vision transformer plus a pretrained language transformer model. This is very convenient as big tech companies have already spent millions of dollars and vast amounts of compute training models which then can be easily fine-tuned or even just used out-of-the-box.\nFine-tuning GPT-2 to generate paper titles I chose GPT-2 as the base for the language model. Once considered too dangerous to fully release, GPT-2 is now the most popular model hosted on Hugging Face. Much more powerful (and presumably commensurately dangerous) models such as the 20 billion parameter GPT-NeoX are now available (cf. the mere 1.5B parameters of GPT-2), however for the purposes of this project the relative computational tractability of tuning GPT-2 makes it an excellent choice.\nGPT-2 is based on what is known as a decoder-only model (at this point, what I‚Äôll dubb ‚ÄúGodwin‚Äôs law but for AI‚Äù comes into effect - as the length of this ML article increases, the probability that I‚Äôll direct you to the original transformers paper, ‚ÄúAttention Is All You Need‚Äù , approaches 1). These models are good for text generation, but the base model is designed to generate text of an arbitrary length, so I had to make a couple of tweaks to get the model to generate paper-title-style sentences.\nLanguage models such a GPT-2 first tokenize their inputs - that is, they break human-readable language into a set of tokens that may have some relationship to the underlying structure of the language, such as words, subwords, or in the case of GPT-2 byte-pair encoding. Additionally, tokens for special occurences such as beginning/end of sequence, or padding, can be used. As GPT-2 only uses end-of-sequence tokens by default, I added a padding token ‚Äú\u003c|pad|\u003e‚Äù to distinguish between the padding required for setting all training inputs to the same size, and the actual end of a title. As a rank amateur, this took quite a long time to work out so I share it here in the hope that it may be helpful to someone. I may do a full writeup in a separate post. Sample code below:\n1 2 3 from transformers import GPT2Tokenizer text_processor = GPT2Tokenizer.from_pretrained(\"gpt2\", pad_token=\"\u003c|pad|\u003e\") I then appended end-of-sequence tokens to each of the training outputs. Finally, it‚Äôs imporant to resize the token embeddings of the model so that the extra token is accounted for:\n1 2 3 4 5 6 from transformers import AutoModelForCausalLM model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # To account for extra padding token model.resize_token_embeddings(len(text_processor)) Having done that, the model can then be trained and produces convincing results. Here‚Äôs a sample of real titles compared to GPT-2-JACS completions using the first 4 words of the real title as a prompt:\nReal GPT-2-JACS completion Origin of Dark-Channel X-ray Fluorescence from Transition-Metal Ions in Water Origin of Dark-Channel X-ray Absorption Fine Structure of (NH3)4Ca(OH)5 in Supercritical Carbon Dioxide Comprehensive Thermochemistry of W‚ÄìH Bonding in the Metal Hydrides CpW(CO)2(IMes)H, [CpW(CO)2(IMes)H]‚Ä¢+, and [CpW(CO)2(IMes)(H)2]+. Influence of an N-Heterocyclic Carbene Ligand on Metal Hydride Bond Energies Comprehensive Thermochemistry of W‚ÄìH and H‚ÄìH Bonds in the Lanthanide Phosphate (Ln5Me4) System Fragmentation Energetics of Clusters Relevant to Atmospheric New Particle Formation Fragmentation Energetics of Clusters Based on Cluster Modification: Assignment of the Concentration-Dependent Rate Constant Transient Photoconductivity of Acceptor-Substituted Poly(3-butylthiophene) Transient Photoconductivity of Acceptor-Substituted Layered Zirconium Oxides Palladium-Catalyzed Aerobic Oxidative Cyclization of N-Aryl Imines: Indole Synthesis from Anilines and Ketones Palladium-Catalyzed Aerobic Oxidative Cyclization of Unactivated Alkenes Mild Aerobic Oxidative Palladium (II) Catalyzed C‚àíH Bond Functionalization:‚ÄâRegioselective and Switchable C‚àíH Alkenylation and Annulation of Pyrroles Mild Aerobic Oxidative Palladium(II)-Catalyzed Arylation of Indoles: Access to Chiral Olefins A Pentacoordinate Boron-Containing œÄ-Electron System with Cl‚ÄìB‚ÄìCl Three-Center Four-Electron Bonds A Pentacoordinate Boron-Containing œÄ-Electron System for High-Performance Polymer Solar Cells Ferroelectric Alkylamide-Substituted Helicene Derivative with Two-Dimensional Hydrogen-Bonding Lamellar Phase Ferroelectric Alkylamide-Substituted Helicene Derivative: Synthesis, Characterization, and Redox Properties Tandem Cyclopropanation/Ring-Closing Metathesis of Dienynes Tandem Cyclopropanation/Ring-Closing Metathesis of Cyclohexadienes: Convergent Access to Optically Active Œ±-Hydroxy Esters Cyclic Penta-Twinned Rhodium Nanobranches as Superior Catalysts for Ethanol Electro-oxidation Cyclic Penta-Twinned Rhodium Nanobranches: Isolation, Structural Characterization, and Catalytic Activity Choosing a vision model Vision transformer models seem to have superceded CNNs as cutting edge vision models. Initially (and perhaps still) I thought that fine-tuning a vision transformer such as ViT or Swin would be the way to go. I tried pre-training both of these with masked image modelling (helpful repo here), which uses a partially masked version of original images as training data and the unmasked version as the ground truth. However in the end the vanilla BEiT proved to work better out of the box. A reminder to not reinvent wheels.\nPutting it all together Once the vision and language models are selected, its a simple matter of combining them then training the model via the standard transformers api. (Full writeup in subsequent post)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from transformers import BeitConfig, GPT2Config # Load pretrained components config_encoder = BeitConfig.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k-ft22k\") config_decoder = GPT2Config.from_pretrained(\"local model folder/\") # set decoder config to causal lm config_decoder.is_decoder = True config_decoder.add_cross_attention = True config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder) # Initializing the model model = VisionEncoderDecoderModel(config=config) Training a title-to-abstract model Section to be completed.\nTest the models here I‚Äôve generated huggingface spaces with gradio apps - they are embedded below if you want to test the models.\nImage to title generator Title to abstract generator ","wordCount":"1939","inLanguage":"en","datePublished":"2022-08-01T00:56:40-07:00","dateModified":"2022-08-01T00:56:40-07:00","author":{"@type":"Person","name":"Yue Wu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yue-here.com/posts/thisjacsdoesnotexist/"},"publisher":{"@type":"Organization","name":"Yue Wu","logo":{"@type":"ImageObject","url":"https://yue-here.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yue-here.com/ accesskey=h title="Yue Wu (Alt + H)">Yue Wu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yue-here.com/index.html title=Home><span>Home</span></a></li><li><a href=https://yue-here.com/bio/ title=Bio><span>Bio</span></a></li><li><a href=https://yue-here.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://yue-here.com/research/ title=Research><span>Research</span></a></li><li><a href=https://yue-here.com/personal/ title=Personal><span>Personal</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">This JACS does not exist</h1><div class=post-meta><span title='2022-08-01 00:56:40 -0700 PDT'>August 1, 2022</span>&nbsp;¬∑&nbsp;10 min&nbsp;¬∑&nbsp;1939 words&nbsp;¬∑&nbsp;Yue Wu</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#training-the-gan>Training the GAN</a><ul><li><a href=#getting-the-dataset-ready>Getting the dataset ready</a></li><li><a href=#running-stylegan3>Running StyleGAN3</a></li><li><a href=#generating-fake-toc-images>Generating fake ToC images</a></li></ul></li><li><a href=#assembling-a-toc-to-title-model>Assembling a ToC to title model</a><ul><li><a href=#fine-tuning-gpt-2-to-generate-paper-titles>Fine-tuning GPT-2 to generate paper titles</a></li><li><a href=#choosing-a-vision-model>Choosing a vision model</a></li><li><a href=#putting-it-all-together>Putting it all together</a></li></ul></li><li><a href=#training-a-title-to-abstract-model>Training a title-to-abstract model</a></li><li><a href=#test-the-models-here>Test the models here</a><ul><li><a href=#toc2title>Image to title generator</a></li><li><a href=#title2abstract>Title to abstract generator</a></li></ul></li></ul></nav></div></details></div><div class=post-content><center><p><a href=http://thisjacsdoesnotexist.com/><figure><img loading=lazy src=/TJDNE_website_3.png><figcaption>An imaginary abstract generated at thisJACSdoesnotexist.com</figcaption></figure></a></p></center><p>In academic chemistry, authors submit a promotional table-of-contents (ToC) image when publishing research papers in most journals. These fascinate me as they are one of the few places where unfettered self expression is tolerated, if not condoned. (See e.g. <a href=https://tocrofl.tumblr.com/>TOC ROFL</a>, of which I am a multiple <a href=https://pubs.acs.org/cms/10.1021/acs.chemmater.5b03085/asset/images/medium/cm-2015-030857_0005.gif>inductee</a>)</p><p>In general, though, ToC images follow a fairly consistent visual language, with distinct conventions followed in different subfields. This presents an vaguely plausible excuse to train some machine learning models to generate ToCs and their accompanying paraphenalia. In this project, I use ToC images, titles and abstracts from one of the most longest running and well-known chemistry journals, the Journal of the American Chemical Society (<a href=https://pubs.acs.org/journal/jacsat>JACS</a>) as a dataset to train:</p><ol><li>A generative adversarial network (<a href=https://github.com/NVlabs/stylegan3>StyleGAN3</a>) - a model which learns to generate images similar to those in its training set.</li><li>A finetuned version of <a href=https://openai.com/blog/better-language-models/>GPT-2</a> that generates chemistry-paper-title-like lists of words.</li><li>A <a href=https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder>vision encoder-decoder model</a> (<a href=#toc2title>test it out here</a>) that converts images to the appropriate text - here, the above GPT-2 model acts as the text generator.</li><li>A <a href=https://huggingface.co/docs/transformers/model_doc/t5>T5 sequence-to-sequence model</a> (<a href=#title2abstract>test it out here</a>) that generates a text sequence from a prompt.</li></ol><p>For those who are interested in the inner workings or may want to do something similar, I&rsquo;ve included a writeup of the project below. Examples of the code are available on my <a href=https://github.com/yue-here/tocgan>GitHub</a>. As you&rsquo;ll see, a lot of my choices were fairly abitrary, arising more-or-less from the availability heuristic.</p><h2 id=training-the-gan>Training the GAN<a hidden class=anchor aria-hidden=true href=#training-the-gan>#</a></h2><h3 id=getting-the-dataset-ready>Getting the dataset ready<a hidden class=anchor aria-hidden=true href=#getting-the-dataset-ready>#</a></h3><p>My initial idea was simply to see how well a GAN could generate ToC images, and so I needed a whole bunch of ToC images. I used the popular python package beautifulsoup with a headless browser and some simple loops to scrape the JACS website for the ToC images, titles and abstracts. The full dataset from around 60,000 papers could be collected overnight. Note: this was relatively easy with JACS (and ACS journals in general) as the website organisation follows a logical structure. YMMV for other publishers&rsquo; platforms.</p><p>Generally, ML training inputs for image models should be the same size, often with powers of 2 as values - StyleGAN3 uses square images of size 128, 256, 512, and 1024 pixels. Luckily, JACS ToC images are fixed to 500 pixels in width and maximum height. I padded the figures to 512x512 using Irfanview&rsquo;s batch processing tool, then resized those images to generate smaller datasets at 128 and 256 px as well.</p><h3 id=running-stylegan3>Running StyleGAN3<a hidden class=anchor aria-hidden=true href=#running-stylegan3>#</a></h3><p>I chose StyleGAN3 as I had heard of it before and 3 was the latest version number. At the time of writing it appears to be one of the best & most mature GANs for image generation. The implementation is fairly user-friendly and includes various data preparation and training scripts; the documentation is excellent and deserves commendation.</p><p>The first step was to use the dataset_tool.py script to prepare a dataset in a form the model could use - something like this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Shell data-lang=Shell><span class=line><span class=cl>python dataset_tool.py --source<span class=o>=</span><span class=s2>&#34;C:\...\512px&#34;</span> --dest<span class=o>=</span><span class=s2>&#34;C:\...\512px.zip&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>After this, it&rsquo;s straightforward to train the model using the following script:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Shell data-lang=Shell><span class=line><span class=cl>python train.py --outdir<span class=o>=</span><span class=s2>&#34;C:\...\training_runs&#34;</span> --cfg<span class=o>=</span>stylegan3-t --data<span class=o>=</span><span class=s2>&#34;C:\...\512px.zip&#34;</span> --gpus<span class=o>=</span><span class=m>1</span> --batch<span class=o>=</span><span class=m>32</span> --gamma<span class=o>=</span><span class=m>8</span> --batch-gpu<span class=o>=</span><span class=m>8</span> --snap<span class=o>=</span><span class=m>10</span> --cbase<span class=o>=</span><span class=m>16384</span>
</span></span></code></pre></td></tr></table></div></div><p>The key parameters that need to be set are batch size and gamma, which is the R1 regularization parameter. &lsquo;cbase=16384&rsquo; speeds up training by sacrificing network capacity, but at low resolutions I didn&rsquo;t find it to be a problem. Having only a humble RTX 3060 Ti (the best I could get during the GPU apocalypse of 2020-2021), the meagre 8 GB of VRAM was a limiting factor for batch sizes. I also found useful the &ndash;resume flag, which allows you to resume training from a previous network snapshot in case of crashes, although I found I also had to edit the resume_kimgs variable in the training script directly to get it to work properly. The tensorbard output is also very good, and can be run like so:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Shell data-lang=Shell><span class=line><span class=cl>tensorboard --logdir <span class=s2>&#34;C:\...\&lt;training run directory&gt;&#34;</span> 
</span></span></code></pre></td></tr></table></div></div><p>I considered training on cloud compute but seemed to be more expensive than training locally, and also required learning a new thing. In retrospect this might have been a poor life choice. My wife was still annoyed that our power bill doubled during the month I was training though! Also that the computer was essentially a fan heater running in the bedroom during the height of summer. After bluescreening a couple of times I removed the computer side panel which dropped temps by a couple of degrees.</p><center><p><img alt=128px loading=lazy src=/fakes_128px.png></p><p>Fakes generated from a model trained on 128 px images</p></center>I ran a first test with a 128 px as proof of concept, which was convincing enough that I scaled up to 256 px.
In principle resolutions of up to 1024 px are available, but the raw data resolution mean that going above 512 was meaningless, and compute requirements made that unfeasible on my home system.<p>After training continuously for a few weeks, the 256 px model started to converge. In this case I used a common evaluation metric for GANs called the Fr√©chet Inception Distance (FID), a measure of similarity between image distributions. Below is a video of images generated from the model as it trains - each successive frame, the models has trained on an additional 40,000 images. Overall, the model saw images from the training set around 5 million times.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/K4JdoLxgoUQ?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><center>GAN training progress video</center><h3 id=generating-fake-toc-images>Generating fake ToC images<a hidden class=anchor aria-hidden=true href=#generating-fake-toc-images>#</a></h3><p>With the GAN model trained, it was time to generate the fake ToC images. This is done very simply with this StyleGAN script:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Shell data-lang=Shell><span class=line><span class=cl>python gen_images.py --outdir<span class=o>=</span><span class=s2>&#34;C:\...\output&#34;</span> --trunc<span class=o>=</span>0.8 --seeds<span class=o>=</span>0-50000 --network<span class=o>=</span><span class=s2>&#34;C:\...\network-snapshot-....pkl&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>The key parameters here are œà (&ldquo;trunc&rdquo;), which controls the tradeoff between image quality and weirdness (higher values are weirder). Some examples below:</p><center><p><img alt=psi02 loading=lazy src=/psi02_1.png><img alt=psi02 loading=lazy src=/psi02_2.png><img alt=psi02 loading=lazy src=/psi02_3.png><img alt=psi02 loading=lazy src=/psi02_4.png><br>œà = 0.2<br><br></p><p><img alt=psi08 loading=lazy src=/psi08_1.png><img alt=psi08 loading=lazy src=/psi08_2.png><img alt=psi08 loading=lazy src=/psi08_3.png><img alt=psi08 loading=lazy src=/psi08_4.png><br>œà = 0.8<br><br></p><p><img alt=psi11 loading=lazy src=/psi11_1.png><img alt=psi11 loading=lazy src=/psi11_2.png><img alt=psi11 loading=lazy src=/psi11_3.png><img alt=psi11 loading=lazy src=/psi11_4.png><br>œà = 1.1<br><br></p></center><h2 id=assembling-a-toc-to-title-model>Assembling a ToC to title model<a hidden class=anchor aria-hidden=true href=#assembling-a-toc-to-title-model>#</a></h2><p>With generated ToC images in hand, I wondered if it was possible to generate relevant paper titles. Having labels in the form of titles for all the ToCs in the training set, the challenge was finding an appropriate model. After trying a couple of things, I settled on using ü§ó Hugging Face&rsquo;s transformers library, mostly due to its good documentation and straightforward code structuring.</p><p>I used the <a href=https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder>ü§ó vision encoder decoder model</a> as the basis for the toc-to-title model. This model can easily be warm-started with a pretrained vision transformer plus a pretrained language transformer model. This is very convenient as big tech companies have already spent millions of dollars and vast amounts of compute training models which then can be easily fine-tuned or even just used out-of-the-box.</p><h3 id=fine-tuning-gpt-2-to-generate-paper-titles>Fine-tuning GPT-2 to generate paper titles<a hidden class=anchor aria-hidden=true href=#fine-tuning-gpt-2-to-generate-paper-titles>#</a></h3><p>I chose GPT-2 as the base for the language model. Once considered too <a href=https://openai.com/blog/better-language-models/>dangerous</a> to fully release, GPT-2 is now the most popular model hosted on Hugging Face. Much more powerful (and presumably commensurately dangerous) models such as the 20 billion parameter GPT-NeoX are now available (cf. the mere 1.5B parameters of GPT-2), however for the purposes of this project the relative computational tractability of tuning GPT-2 makes it an excellent choice.</p><p>GPT-2 is based on what is known as a decoder-only model (at this point, what I&rsquo;ll dubb &ldquo;Godwin&rsquo;s law but for AI&rdquo; comes into effect - as the length of this ML article increases, the probability that I&rsquo;ll direct you to the original transformers paper, <a href=https://arxiv.org/abs/1706.03762>&ldquo;Attention Is All You Need&rdquo; </a>, approaches 1). These models are good for text generation, but the base model is designed to generate text of an arbitrary length, so I had to make a couple of tweaks to get the model to generate paper-title-style sentences.</p><p>Language models such a GPT-2 first tokenize their inputs - that is, they break human-readable language into a set of tokens that may have some relationship to the underlying structure of the language, such as words, subwords, or in the case of GPT-2 <a href=https://huggingface.co/course/chapter6/5>byte-pair</a> encoding. Additionally, tokens for special occurences such as beginning/end of sequence, or padding, can be used. As GPT-2 only uses end-of-sequence tokens by default, I added a padding token &ldquo;&lt;|pad|>&rdquo; to distinguish between the padding required for setting all training inputs to the same size, and the actual end of a title. As a rank amateur, this took quite a long time to work out so I share it here in the hope that it may be helpful to someone. I may do a full writeup in a separate post. Sample code below:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>GPT2Tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>text_processor</span> <span class=o>=</span> <span class=n>GPT2Tokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>,</span> <span class=n>pad_token</span><span class=o>=</span><span class=s2>&#34;&lt;|pad|&gt;&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>I then appended end-of-sequence tokens to each of the training outputs. Finally, it&rsquo;s imporant to resize the token embeddings of the model so that the extra token is accounted for:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># To account for extra padding token</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>resize_token_embeddings</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>text_processor</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>Having done that, the model can then be trained and produces convincing results. Here&rsquo;s a sample of real titles compared to GPT-2-JACS completions using the first 4 words of the real title as a prompt:</p><table><thead><tr><th>Real</th><th>GPT-2-JACS completion</th></tr></thead><tbody><tr><td>Origin of Dark-Channel X-ray Fluorescence from Transition-Metal Ions in Water</td><td>Origin of Dark-Channel X-ray Absorption Fine Structure of (NH3)4Ca(OH)5 in Supercritical Carbon Dioxide</td></tr><tr><td>Comprehensive Thermochemistry of W‚ÄìH Bonding in the Metal Hydrides CpW(CO)2(IMes)H, [CpW(CO)2(IMes)H]‚Ä¢+, and [CpW(CO)2(IMes)(H)2]+. Influence of an N-Heterocyclic Carbene Ligand on Metal Hydride Bond Energies</td><td>Comprehensive Thermochemistry of W‚ÄìH and H‚ÄìH Bonds in the Lanthanide Phosphate (Ln5Me4) System</td></tr><tr><td>Fragmentation Energetics of Clusters Relevant to Atmospheric New Particle Formation</td><td>Fragmentation Energetics of Clusters Based on Cluster Modification: Assignment of the Concentration-Dependent Rate Constant</td></tr><tr><td>Transient Photoconductivity of Acceptor-Substituted Poly(3-butylthiophene)</td><td>Transient Photoconductivity of Acceptor-Substituted Layered Zirconium Oxides</td></tr><tr><td>Palladium-Catalyzed Aerobic Oxidative Cyclization of N-Aryl Imines: Indole Synthesis from Anilines and Ketones</td><td>Palladium-Catalyzed Aerobic Oxidative Cyclization of Unactivated Alkenes</td></tr><tr><td>Mild Aerobic Oxidative Palladium (II) Catalyzed C‚àíH Bond Functionalization:‚Äâ Regioselective and Switchable C‚àíH Alkenylation and Annulation of Pyrroles</td><td>Mild Aerobic Oxidative Palladium(II)-Catalyzed Arylation of Indoles: Access to Chiral Olefins</td></tr><tr><td>A Pentacoordinate Boron-Containing œÄ-Electron System with Cl‚ÄìB‚ÄìCl Three-Center Four-Electron Bonds</td><td>A Pentacoordinate Boron-Containing œÄ-Electron System for High-Performance Polymer Solar Cells</td></tr><tr><td>Ferroelectric Alkylamide-Substituted Helicene Derivative with Two-Dimensional Hydrogen-Bonding Lamellar Phase</td><td>Ferroelectric Alkylamide-Substituted Helicene Derivative: Synthesis, Characterization, and Redox Properties</td></tr><tr><td>Tandem Cyclopropanation/Ring-Closing Metathesis of Dienynes</td><td>Tandem Cyclopropanation/Ring-Closing Metathesis of Cyclohexadienes: Convergent Access to Optically Active Œ±-Hydroxy Esters</td></tr><tr><td>Cyclic Penta-Twinned Rhodium Nanobranches as Superior Catalysts for Ethanol Electro-oxidation</td><td>Cyclic Penta-Twinned Rhodium Nanobranches: Isolation, Structural Characterization, and Catalytic Activity</td></tr></tbody></table><h3 id=choosing-a-vision-model>Choosing a vision model<a hidden class=anchor aria-hidden=true href=#choosing-a-vision-model>#</a></h3><p>Vision transformer models seem to have superceded CNNs as cutting edge vision models. Initially (and perhaps still) I thought that fine-tuning a vision transformer such as <a href=https://huggingface.co/docs/transformers/model_doc/vit>ViT</a> or <a href=https://huggingface.co/docs/transformers/model_doc/swin>Swin</a> would be the way to go. I tried pre-training both of these with masked image modelling (helpful repo <a href=https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining>here</a>), which uses a partially masked version of original images as training data and the unmasked version as the ground truth. However in the end the vanilla <a href=https://huggingface.co/docs/transformers/model_doc/beit>BEiT</a> proved to work better out of the box. A reminder to not reinvent wheels.</p><h3 id=putting-it-all-together>Putting it all together<a hidden class=anchor aria-hidden=true href=#putting-it-all-together>#</a></h3><p>Once the vision and language models are selected, its a simple matter of combining them then training the model via the standard transformers api. (Full writeup in subsequent post)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-Python data-lang=Python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>BeitConfig</span><span class=p>,</span> <span class=n>GPT2Config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load pretrained components</span>
</span></span><span class=line><span class=cl><span class=n>config_encoder</span> <span class=o>=</span> <span class=n>BeitConfig</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;microsoft/beit-base-patch16-224-pt22k-ft22k&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>config_decoder</span> <span class=o>=</span> <span class=n>GPT2Config</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;local model folder/&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># set decoder config to causal lm</span>
</span></span><span class=line><span class=cl><span class=n>config_decoder</span><span class=o>.</span><span class=n>is_decoder</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=n>config_decoder</span><span class=o>.</span><span class=n>add_cross_attention</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=n>VisionEncoderDecoderConfig</span><span class=o>.</span><span class=n>from_encoder_decoder_configs</span><span class=p>(</span><span class=n>config_encoder</span><span class=p>,</span> <span class=n>config_decoder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initializing the model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>VisionEncoderDecoderModel</span><span class=p>(</span><span class=n>config</span><span class=o>=</span><span class=n>config</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=training-a-title-to-abstract-model>Training a title-to-abstract model<a hidden class=anchor aria-hidden=true href=#training-a-title-to-abstract-model>#</a></h2><p>Section to be completed.</p><h2 id=test-the-models-here>Test the models here<a hidden class=anchor aria-hidden=true href=#test-the-models-here>#</a></h2><p>I&rsquo;ve generated huggingface spaces with gradio apps - they are embedded below if you want to test the models.</p><h3 id=toc2title>Image to title generator<a hidden class=anchor aria-hidden=true href=#toc2title>#</a></h3><script type=module src=https://gradio.s3-us-west-2.amazonaws.com/3.1.4/gradio.js>
</script><gradio-app space=yuewu/tocgen></gradio-app><h3 id=title2abstract>Title to abstract generator<a hidden class=anchor aria-hidden=true href=#title2abstract>#</a></h3><script type=module src=https://gradio.s3-us-west-2.amazonaws.com/3.1.4/gradio.js>
</script><gradio-app space=yuewu/title2abstract></gradio-app></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yue-here.com/posts/abstract2title/><span class=title>¬´ Prev</span><br><span>Abstract2title</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://yue-here.com/>Yue Wu</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>